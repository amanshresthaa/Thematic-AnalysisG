

# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/metrics.py
# File: /Users/amankumarshrestha/Downloads/Example/src/metrics.py
# metrics.py
import logging
from typing import List, Dict, Any, Callable
import dspy
from utils.utils import check_answer_length
from utils.logger import setup_logging

# Initialize logger
logger = logging.getLogger(__name__)

class BaseAssessment(dspy.Signature):
    """
    Base class for all assessment signatures.
    """
    context: str = dspy.InputField(
        desc=(
            "The contextual information provided to generate the answer. This includes all relevant "
            "documents, data chunks, or information sources that the answer is based upon."
        )
    )
    question: str = dspy.InputField(
        desc=(
            "The original question that was posed. This is used to understand the intent and scope "
            "of the answer in relation to the provided context."
        )
    )
    answer: str = dspy.InputField(
        desc=(
            "The answer generated by the system that needs to be evaluated for factual correctness "
            "based on the provided context."
        )
    )

    def generate_prompt(self, context: str, question: str, answer: str, task: str) -> str:
        return (
            f"Context: {context}\n"
            f"Question: {question}\n"
            f"Answer: {answer}\n\n"
            f"{task}"
        )

class Assess(BaseAssessment):
    """
    Assess the factual correctness of an answer based on the provided context.

    This signature evaluates whether the generated answer accurately reflects the information
    present in the given context. It leverages a language model to perform a nuanced analysis
    beyond simple keyword matching, ensuring a thorough assessment of factual accuracy.
    """
    factually_correct: str = dspy.OutputField(
        desc=(
            "Indicator of whether the answer is factually correct based on the context. "
            "Should be 'Yes' if the answer accurately reflects the information in the context, "
            "and 'No' otherwise."
        )
    )

    def forward(self, context: str, question: str, answer: str) -> Dict[str, str]:
        try:
            logger.debug(f"Assessing factual correctness for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Based on the context provided, evaluate whether the answer is factually correct.\n"
                "Respond with 'Yes' if the answer accurately reflects the information in the context.\n"
                "Respond with 'No' if the answer contains factual inaccuracies or is not supported by the context.\n"
                "Please respond with 'Yes' or 'No' only."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=3,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Factuality assessment response: '{response}'")
            if response.lower() in ['yes', 'no']:
                result = response.capitalize()
            else:
                logger.warning(f"Unexpected response from factuality assessment: '{response}'. Defaulting to 'No'.")
                result = 'No'
            logger.info(f"Factuality assessment result: {result} for question: '{question}'")
            return {"factually_correct": result}
        except Exception as e:
            logger.error(f"Error in Assess.forward: {e}", exc_info=True)
            return {"factually_correct": "No"}


class AssessRelevance(BaseAssessment):
    """
    Assess the relevance of an answer to the given question and context.
    
    This signature evaluates whether the generated answer directly and comprehensively 
    addresses the user's query, considering the provided context.
    """
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug(f"Assessing relevance for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the relevance of the answer to the question based on the context.\n"
                "Provide a relevance score between 1 (not relevant) and 5 (highly relevant)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Relevance assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected relevance score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid relevance score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Relevance assessment result: {result} for question: '{question}'")
            return {"relevance_score": result}
        except Exception as e:
            logger.error(f"Error in AssessRelevance.forward: {e}", exc_info=True)
            return {"relevance_score": 1}


class AssessCoherence(BaseAssessment):
    """
    Assess the coherence of an answer.
    
    This signature evaluates whether the generated answer is well-structured and logically consistent.
    """
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing coherence of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the coherence of the above answer.\n"
                "Provide a coherence score between 1 (not coherent) and 5 (highly coherent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Coherence assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected coherence score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid coherence score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Coherence assessment result: {result}")
            return {"coherence_score": result}
        except Exception as e:
            logger.error(f"Error in AssessCoherence.forward: {e}", exc_info=True)
            return {"coherence_score": 1}


class AssessConciseness(BaseAssessment):
    """
    Assess the conciseness of an answer.
    
    This signature evaluates whether the generated answer is succinct and free from unnecessary verbosity.
    """
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing conciseness of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the conciseness of the above answer.\n"
                "Provide a conciseness score between 1 (not concise) and 5 (highly concise)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Conciseness assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected conciseness score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid conciseness score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Conciseness assessment result: {result}")
            return {"conciseness_score": result}
        except Exception as e:
            logger.error(f"Error in AssessConciseness.forward: {e}", exc_info=True)
            return {"conciseness_score": 1}


class AssessFluency(BaseAssessment):
    """
    Assess the fluency of an answer.
    
    This signature evaluates whether the generated answer exhibits natural language flow and is free from grammatical errors.
    """
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing fluency of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the fluency of the above answer.\n"
                "Provide a fluency score between 1 (not fluent) and 5 (highly fluent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Fluency assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected fluency score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid fluency score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Fluency assessment result: {result}")
            return {"fluency_score": result}
        except Exception as e:
            logger.error(f"Error in AssessFluency.forward: {e}", exc_info=True)
            return {"fluency_score": 1}


class ComprehensiveAssessment(BaseAssessment):
    """
    Comprehensive assessment of generated answers across multiple dimensions.
    """
    factually_correct: str = dspy.OutputField(desc="Whether the answer is factually correct ('Yes'/'No').")
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, Any]:
        try:
            logger.debug(f"Performing comprehensive assessment for question: '{question}'")
            
            # Assess factual correctness
            factuality = Assess()(context=context, question=question, answer=answer)['factually_correct']
            
            # Assess relevance
            relevance = AssessRelevance()(context=context, question=question, answer=answer)['relevance_score']
            
            # Assess coherence
            coherence = AssessCoherence()(context=context, question=question, answer=answer)['coherence_score']
            
            # Assess conciseness
            conciseness = AssessConciseness()(context=context, question=question, answer=answer)['conciseness_score']
            
            # Assess fluency
            fluency = AssessFluency()(context=context, question=question, answer=answer)['fluency_score']
            
            # Aggregate scores with adjusted weights
            composite_score = {
                "factually_correct": factuality,
                "relevance_score": relevance,
                "coherence_score": coherence,
                "conciseness_score": conciseness,
                "fluency_score": fluency
            }
            logger.info(f"Comprehensive assessment result: {composite_score}")
            return composite_score
        except Exception as e:
            logger.error(f"Error in ComprehensiveAssessment.forward: {e}", exc_info=True)
            return {
                "factually_correct": "No",
                "relevance_score": 1,
                "coherence_score": 1,
                "conciseness_score": 1,
                "fluency_score": 1
            }


def comprehensive_metric(example: Dict[str, Any], pred: Dict[str, Any], trace: Any = None) -> float:
    """
    Comprehensive metric to evaluate the answer across multiple dimensions.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
        trace (Any, optional): Trace information for optimization (unused here).
    
    Returns:
        float: A combined score representing the quality of the answer.
    """
    try:
        logger.debug(f"Evaluating comprehensive metrics for question: '{example.get('question', '')}'")
        assessment = comprehensive_assessment_module(
            context=example.get('context', ''),
            question=example.get('question', ''),
            answer=pred.get('answer', '')
        )
        logger.debug(f"Comprehensive assessment: {assessment}")
        
        # Convert 'factually_correct' to binary
        factually_correct = 1 if assessment.get("factually_correct") == "Yes" else 0
        
        # Normalize scores between 0 and 1
        relevance = assessment.get("relevance_score", 1) / 5
        coherence = assessment.get("coherence_score", 1) / 5
        conciseness = assessment.get("conciseness_score", 1) / 5
        fluency = assessment.get("fluency_score", 1) / 5
        
        # Define adjusted weights for each metric
        weights = {
            "factually_correct": 0.5,
            "relevance": 0.2,
            "coherence": 0.1,
            "conciseness": 0.1,
            "fluency": 0.1
        }
        
        # Calculate the composite score
        composite_score = (
            weights["factually_correct"] * factually_correct +
            weights["relevance"] * relevance +
            weights["coherence"] * coherence +
            weights["conciseness"] * conciseness +
            weights["fluency"] * fluency
        )
        
        logger.info(f"Comprehensive metric score: {composite_score}")
        return composite_score
    except Exception as e:
        logger.error(f"Error in comprehensive_metric: {e}", exc_info=True)
        return 0.0


def is_answer_fully_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    """
    Determines if the answer meets all quality metrics.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
    
    Returns:
        bool: True if all metrics meet the desired thresholds, False otherwise.
    """
    scores = comprehensive_metric(example, pred)
    # Define threshold (e.g., composite score should be at least 0.8)
    is_factual = scores >= 0.8
    logger.debug(f"Is answer fully correct (scores >= 0.8): {is_factual}")
    return is_factual


def factuality_metric(example: Dict[str, Any], pred: Dict[str, Any]) -> int:
    """
    Metric to evaluate factual correctness of the answer.

    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.

    Returns:
        int: 1 if factually correct, 0 otherwise.
    """
    try:
        assess = Assess()
        result = assess(context=example.get('context', ''), question=example.get('question', ''), answer=pred.get('answer', ''))
        factually_correct = result.get('factually_correct', 'No')
        logger.debug(f"Factuality metric result: {factually_correct}")
        return 1 if factually_correct == 'Yes' else 0
    except Exception as e:
        logger.error(f"Error in factuality_metric: {e}", exc_info=True)
        return 0 


# Initialize the assessment modules
try:
    # Use the unoptimized module directly without caching
    comprehensive_assessment_module = dspy.ChainOfThought(ComprehensiveAssessment)
    logger.info("Comprehensive Assessment DSPy module initialized successfully.")
except Exception as e:
    logger.error(f"Error initializing Comprehensive Assessment DSPy module: {e}", exc_info=True)
    raise

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/select_quotation.py
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/select_quotation.py

import logging
from typing import List, Dict, Any
import dspy
from utils.validation_functions import validate_relevance, validate_quality, validate_context_clarity

logger = logging.getLogger(__name__)

class SelectQuotationSignature(dspy.Signature):
    """
    Select relevant quotations from transcript chunks based on research objectives.
    """
    research_objectives: str = dspy.InputField(
        desc="The research objectives guiding the selection of quotations."
    )
    transcript_chunks: List[str] = dspy.InputField(
        desc="Chunks of transcript from which quotations are to be selected."
    )
    quotations: List[str] = dspy.OutputField(
        desc="List of selected quotations relevant to the research objectives."
    )
    types_and_functions: List[str] = dspy.OutputField(
        desc="Types and functions of the selected quotations."
    )
    purpose: str = dspy.OutputField(
        desc="Purpose of selecting the quotations."
    )

    def forward(self, research_objectives: str, transcript_chunks: List[str]) -> Dict[str, Any]:
        try:
            logger.debug("Starting quotation selection process.")
            prompt = (
                f"You are an expert in qualitative research and thematic analysis.\n\n"
                f"**Research Objectives**:\n{research_objectives}\n\n"
                f"**Transcript Chunks**:\n" +
                "\n".join([f"Chunk {i+1}: {chunk}" for i, chunk in enumerate(transcript_chunks)]) +
                "\n\n"
                f"**Task:** Extract **3-5** relevant quotations from the transcript chunks that align with the research objectives provided. "
                f"Provide each quotation in the following JSON format within a list:\n\n"
                f"```json\n"
                f"[\n"
                f"    {{\"QUOTE\": \"This is the first quotation.\"}},\n"
                f"    {{\"QUOTE\": \"This is the second quotation.\"}},\n"
                f"    {{\"QUOTE\": \"This is the third quotation.\"}}\n"
                f"]\n"
                f"```\n"
                f"Additionally, categorize each quotation under appropriate types and functions, and state the purpose of selecting these quotations.\n"
                f"Ensure that the response is a valid JSON object containing the quotations, their types and functions, and the overall purpose. "
                f"If no quotations are available, respond with empty lists and an empty string."
            )

            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=500,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=None
            ).strip()

            logger.info("Quotations selected successfully.")
            parsed_response = self._parse_json_response(response)
            quotations = parsed_response.get("quotations", [])
            types_and_functions = parsed_response.get("types_and_functions", [])
            purpose = parsed_response.get("purpose", "")

            # Assertions
            # a. Relevance to Research Objectives
            dspy.Assert(
                validate_relevance(quotations, research_objectives),
                msg="Quotations are not sufficiently relevant to the research objectives.",
                backtrack=self
            )

            # b. Quality and Representation
            dspy.Assert(
                validate_quality(quotations),
                msg="One or more quotations do not meet the quality standards.",
                backtrack=self
            )

            # f. Context and Clarity
            context = "\n".join(transcript_chunks)
            dspy.Assert(
                validate_context_clarity(quotations, context),
                msg="Quotations lack sufficient context or clarity.",
                backtrack=self
            )

            return {
                "quotations": quotations,
                "types_and_functions": types_and_functions,
                "purpose": purpose
            }
        except Exception as e:
            logger.error(f"Error in SelectQuotationSignature.forward: {e}", exc_info=True)
            return {
                "quotations": [],
                "types_and_functions": [],
                "purpose": ""
            }

    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """
        Parses the JSON response to extract quotations, types/functions, and purpose.
        """
        import json
        try:
            parsed = json.loads(response)
            return parsed
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON response. Returning empty fields.")
            return {
                "quotations": [],
                "types_and_functions": [],
                "purpose": ""
            }

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/reranker.py
# File: /Users/amankumarshrestha/Downloads/Example/src/reranker.py

import logging
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util
import torch

from utils.logger import setup_logging

# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class SentenceTransformerReRanker:
    """
    Re-ranker using Sentence Transformers for semantic similarity.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        """
        Initializes the Sentence Transformer re-ranker.
        
        Args:
            model_name (str): Pre-trained Sentence Transformer model name.
            device (str, optional): Device to run the model on ('cuda', 'mps', or 'cpu'). Defaults to automatic selection.
        """
        self.model = SentenceTransformer(model_name)
        
        # Set device
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'.")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Re-ranks documents based on semantic similarity to the query.
        
        Args:
            query (str): The search query.
            documents (List[str]): List of document contents to re-rank.
            top_k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
        
        Returns:
            List[Dict[str, Any]]: List of re-ranked documents with similarity scores.
        """
        if not documents:
            logger.warning("No documents provided for re-ranking.")
            return []
        
        try:
            logger.debug("Encoding query and documents.")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities.")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)  # Adjust k to the number of available documents
            if actual_k == 0:
                logger.warning("No documents available for re-ranking.")
                return []
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}.")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            logger.info(f"Re-ranked {actual_k} documents based on semantic similarity.")
            return re_ranked_docs
        except Exception as e:
            logger.error(f"Error during re-ranking with Sentence Transformers: {e}", exc_info=True)
            return []

def rerank_documents_sentence_transformer(query: str, retrieved_docs: List[Dict[str, Any]], k: int = 20) -> List[Dict[str, Any]]:
    """
    Re-ranks the retrieved documents using Sentence Transformers.
    
    Args:
        query (str): The search query.
        retrieved_docs (List[Dict[str, Any]]): List of retrieved documents.
        k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.info(f"Starting re-ranking of {len(retrieved_docs)} documents for query: '{query}' using Sentence Transformers.")
    if not retrieved_docs:
        logger.warning(f"No documents to re-rank for query: '{query}'")
        return []
    try:
        # Initialize the re-ranker
        reranker = SentenceTransformerReRanker()
        
        # Extract document contents
        documents = [doc['chunk']['original_content'] for doc in retrieved_docs]
        
        # Perform re-ranking
        re_ranked = reranker.rerank(query, documents, top_k=k)
        
        # Attach scores back to the documents
        re_ranked_docs = []
        for r, original_doc in zip(re_ranked, retrieved_docs[:len(re_ranked)]):  # Ensure matching length
            re_ranked_docs.append({
                "chunk": original_doc['chunk'],
                "score": r['score']
            })
        
        logger.info(f"Re-ranking completed using Sentence Transformers. Top {k} documents selected.")
        return re_ranked_docs
    except Exception as e:
        logger.error(f"Error during document re-ranking with Sentence Transformers: {e}", exc_info=True)
        return retrieved_docs[:k]  # Fallback to original ranking if re-ranking fails

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/elasticsearch_bm25.py
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk, scan
from elasticsearch.exceptions import NotFoundError, RequestError
import json
import os
from utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)

es_host = "http://localhost:9200"
index_name = "contextual_bm25_index"
es_client = Elasticsearch(es_host)

if es_client.indices.exists(index=index_name):
    es_client.indices.delete(index=index_name)
    print(f"Index '{index_name}' deleted.")

class ElasticsearchBM25:
    
    def __init__(
        self, 
        index_name: str = "contextual_bm25_index",
        es_host: str = "http://localhost:9200",
        logger: Optional[logging.Logger] = None
    ):
        self.logger = logger or logging.getLogger(__name__)
        self.index_name = index_name
        self.es_client = Elasticsearch(es_host)
        
        if not self.es_client.ping():
            raise ConnectionError(f"Failed to connect to Elasticsearch at {es_host}")
            
        self._create_index()
        
    def _create_index(self) -> None:
        index_settings = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "english"
                        },
                        "custom_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "porter_stem"]
                        }
                    }
                },
                "similarity": {
                    "custom_bm25": {
                        "type": "BM25",
                        "k1": 1.2,
                        "b": 0.75,
                    }
                },
                "index": {
                    "refresh_interval": "1s",
                    "number_of_shards": 1,
                    "number_of_replicas": 0
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "contextualized_content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "doc_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "original_index": {"type": "integer"},
                    "metadata": {"type": "object", "enabled": True}
                }
            }
        }
        
        index_data_dir = f"./data/{self.index_name}"
        os.makedirs(index_data_dir, exist_ok=True)
        self.logger.debug(f"Ensured existence of index data directory: {index_data_dir}")
        
        try:
            if self.es_client.indices.exists(index=self.index_name):
                self.es_client.indices.close(index=self.index_name)
                self.es_client.indices.put_settings(
                    index=self.index_name,
                    body=index_settings["settings"]
                )
                self.es_client.indices.open(index=self.index_name)
            else:
                self.es_client.indices.create(
                    index=self.index_name,
                    body=index_settings
                )
            self.logger.info(f"Successfully configured index: {self.index_name}")
        except Exception as e:
            self.logger.error(f"Failed to create/update index: {str(e)}")
            raise

    def index_documents(
        self,
        documents: List[Dict[str, Any]],
        batch_size: int = 500
    ) -> Tuple[int, List[Dict[str, Any]]]:
        if not documents:
            self.logger.warning("No documents provided for indexing")
            return 0, []

        failed_docs = []
        success_count = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            actions = [{
                "_index": self.index_name,
                "_source": {
                    "content": doc.get("original_content", ""),
                    "contextualized_content": doc.get("contextualized_content", ""),
                    "doc_id": doc.get("doc_id", ""),
                    "chunk_id": doc.get("chunk_id", ""),
                    "original_index": doc.get("original_index", 0),
                    "metadata": doc.get("metadata", {})
                }
            } for doc in batch]
            
            try:
                success, failed = bulk(
                    self.es_client,
                    actions,
                    raise_on_error=False,
                    raise_on_exception=False
                )
                success_count += success
                if failed:
                    for fail in failed:
                        failed_doc = batch[fail.get('index', 0)]
                        failed_docs.append(failed_doc)
                    self.logger.warning(f"Failed to index {len(failed)} documents in batch")
                    
            except Exception as e:
                self.logger.error(f"Batch indexing error: {str(e)}")
                failed_docs.extend(batch)
                
        self.es_client.indices.refresh(index=self.index_name)
        self.logger.info(f"Indexed {success_count}/{len(documents)} documents successfully")
        return success_count, failed_docs

    def search(
        self,
        query: str,
        k: int = 20,
        min_score: float = 0.1,
        fields: List[str] = None,
        operator: str = "or",
        minimum_should_match: str = "30%"
    ) -> List[Dict[str, Any]]:
        if not fields:
            fields = ["content^1", "contextualized_content^1.5"]
            
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "operator": operator,
                    "minimum_should_match": minimum_should_match,
                    "type": "best_fields",
                    "tie_breaker": 0.3
                }
            },
            "min_score": min_score,
            "size": k,
            "_source": True
        }

        try:
            search_body_json = json.dumps(search_body, indent=2)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")
        except Exception as e:
            self.logger.error(f"Error converting search body to JSON: {e}")
            search_body_json = str(search_body)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")

        try:
            response = self.es_client.search(
                index=self.index_name,
                body=search_body
            )
            
            hits = [{
                "doc_id": hit["_source"]["doc_id"],
                "chunk_id": hit["_source"]["chunk_id"],
                "content": hit["_source"]["content"],
                "contextualized_content": hit["_source"].get("contextualized_content"),
                "score": hit["_score"],
                "metadata": hit["_source"].get("metadata", {})
            } for hit in response["hits"]["hits"]]
            
            self.logger.debug(
                f"Search for '{query}' returned {len(hits)} results "
                f"(max_score: {response['hits'].get('max_score', 0)})"
            )
            return hits
            
        except Exception as e:
            self.logger.error(f"Search error: {str(e)}")
            raise

    def search_content(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["content^1"],
            operator="or",
            minimum_should_match="30%"
        )

    def search_contextualized(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["contextualized_content^1.5"],
            operator="or",
            minimum_should_match="30%"
        )

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/query_generator.py
# File: /Users/amankumarshrestha/Downloads/Example/src/query_generator.py
# query_generator.py

import dspy
import logging
from typing import Dict

logger = logging.getLogger(__name__)

class QueryGeneratorSignature(dspy.Signature):
    question: str = dspy.InputField(desc="The original user question.")
    context: str = dspy.InputField(desc="The accumulated context from previous retrievals.")
    new_query: str = dspy.OutputField(desc="The generated query for the next retrieval hop.")

    def forward(self, question: str, context: str) -> Dict[str, str]:
        try:
            prompt = (
                f"Given the question: '{question}'\n"
                f"and the context retrieved so far:\n{context}\n"
                "Generate a search query that will help find additional information needed to answer the question."
            )
            new_query = self.language_model.generate(
                prompt=prompt,
                max_tokens=50,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=["\n"]
            ).strip()
            logger.info(f"Generated new query: '{new_query}'")
            return {"new_query": new_query}
        except Exception as e:
            logger.error(f"Error in QueryGeneratorSignature.forward: {e}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/evaluation.py
import logging
import time
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from contextual_vector_db import ContextualVectorDB
from elasticsearch_bm25 import ElasticsearchBM25
from utils.logger import setup_logging
from metrics import comprehensive_metric, is_answer_fully_correct

setup_logging()
logger = logging.getLogger(__name__)


def evaluate_pipeline(queries: List[Dict[str, Any]], retrieval_function: Callable, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int = 20) -> Dict[str, float]:
    total_score = 0
    total_queries = len(queries)
    queries_with_golden = 0
    queries_without_golden = 0

    semantic_success = 0
    bm25_contextual_success = 0
    total_semantic_hits = 0
    total_bm25_contextual_hits = 0

    logger.info(f"Starting evaluation of {total_queries} queries.")

    for query_item in tqdm(queries, desc="Evaluating retrieval"):
        query = query_item.get('query', '').strip()

        has_golden_data = all([
            'golden_doc_uuids' in query_item,
            'golden_chunk_uuids' in query_item,
            'golden_documents' in query_item
        ])

        if has_golden_data:
            queries_with_golden += 1
            golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
            golden_contents = []

            for doc_uuid, chunk_index in golden_chunk_uuids:
                golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                if not golden_doc:
                    logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                    continue
                golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                if not golden_chunk:
                    logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                    continue
                golden_contents.append(golden_chunk.get('content', '').strip())

            if not golden_contents:
                logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                continue

            retrieved_docs = retrieval_function(query, db, es_bm25, k)

            chunks_found = 0
            semantic_hits = 0
            bm25_contextual_hits = 0

            for golden_content in golden_contents:
                for doc in retrieved_docs[:k]:
                    retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                    contextualized_content = doc.get('chunk', {}).get('contextualized_content', '').strip()
                    if retrieved_content == golden_content:
                        chunks_found += 1
                        semantic_hits += 1
                        break
                    elif contextualized_content == golden_content:
                        chunks_found += 1
                        bm25_contextual_hits += 1
                        break

            query_score = chunks_found / len(golden_contents)
            total_score += query_score
            logger.debug(f"Query '{query}' score: {query_score}")

            semantic_success += semantic_hits
            bm25_contextual_success += bm25_contextual_hits
            total_semantic_hits += semantic_hits
            total_bm25_contextual_hits += bm25_contextual_hits
        else:
            queries_without_golden += 1
            logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
            continue

    average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
    pass_at_n = average_score * 100

    semantic_percentage = (semantic_success / total_semantic_hits) * 100 if total_semantic_hits > 0 else 0
    bm25_contextual_percentage = (bm25_contextual_success / total_bm25_contextual_hits) * 100 if total_bm25_contextual_hits > 0 else 0

    logger.info(f"Evaluation completed.")
    logger.info(f"Total Queries: {total_queries}")
    logger.info(f"Queries with Golden Data: {queries_with_golden}")
    logger.info(f"Queries without Golden Data: {queries_without_golden}")
    logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")
    logger.info(f"Semantic Hits: {semantic_percentage:.2f}%")
    logger.info(f"BM25 Contextual Hits: {bm25_contextual_percentage:.2f}%")

    return {
        "pass_at_n": pass_at_n,
        "average_score": average_score,
        "semantic_hit_percentage": semantic_percentage,
        "bm25_contextual_hit_percentage": bm25_contextual_percentage,
        "total_queries": total_queries,
        "queries_with_golden": queries_with_golden,
        "queries_without_golden": queries_without_golden
    }


def evaluate_complete_pipeline(db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k_values: List[int], evaluation_set: List[Dict[str, Any]], retrieval_function: Callable):
    for k in k_values:
        logger.info(f"Starting evaluation for Pass@{k}")
        results = evaluate_pipeline(evaluation_set, retrieval_function, db, es_bm25, k)
        logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
        logger.info(f"Average Score: {results['average_score']:.4f}")
        logger.info(f"Semantic Hit Percentage: {results['semantic_hit_percentage']:.2f}%")
        logger.info(f"BM25 Contextual Hit Percentage: {results['bm25_contextual_hit_percentage']:.2f}%")
        logger.info(f"Total Queries: {results['total_queries']}")
        logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
        logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/data_loader.py
# File: /Users/amankumarshrestha/Downloads/Example/src/data_loader.py
import json
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def load_json(file_path: str) -> List[Dict[str, Any]]:
    data = []
    try:
        if file_path.endswith('.jsonl'):
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_number, line in enumerate(f, start=1):
                    if line.strip():
                        try:
                            obj = json.loads(line)
                            data.append(obj)
                        except json.JSONDecodeError as e:
                            logger.error(f"JSON parsing error in file '{file_path}' at line {line_number}: {e}")
            logger.info(f"Loaded JSONL file '{file_path}' with {len(data)} entries successfully.")
        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"Loaded JSON file '{file_path}' successfully with {len(data)} entries.")
        return data
    except FileNotFoundError:
        logger.error(f"Error loading JSON file '{file_path}': File does not exist.")
        return []
    except json.JSONDecodeError as e:
        logger.error(f"Error loading JSON file '{file_path}': {e}")
        return []
    except Exception as e:
        logger.error(f"Unexpected error loading JSON file '{file_path}': {e}")
        return []

def load_codebase_chunks(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading codebase chunks from '{file_path}'.")
    return load_json(file_path)

def load_queries(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading queries from '{file_path}'.")
    return load_json(file_path)

################################################################################


################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/__init__.py

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/reranking.py
# File: /Users/amankumarshrestha/Downloads/Example/src/reranking.py

import logging
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util
import torch
import time

# Add missing import for ContextualVectorDB and ElasticsearchBM25
from contextual_vector_db import ContextualVectorDB
from elasticsearch_bm25 import ElasticsearchBM25

from utils.logger import setup_logging

# Import hybrid_retrieval from retrieval.py
from retrieval import hybrid_retrieval

# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class SentenceTransformerReRanker:
    """
    Re-ranker using Sentence Transformers for semantic similarity.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        """
        Initializes the Sentence Transformer re-ranker.
        
        Args:
            model_name (str): Pre-trained Sentence Transformer model name.
            device (str, optional): Device to run the model on ('cuda', 'mps', or 'cpu'). Defaults to automatic selection.
        """
        self.model = SentenceTransformer(model_name)
        
        # Set device
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'.")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Re-ranks documents based on semantic similarity to the query.
        
        Args:
            query (str): The search query.
            documents (List[str]): List of document contents to re-rank.
            top_k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
        
        Returns:
            List[Dict[str, Any]]: List of re-ranked documents with similarity scores.
        """
        if not documents:
            logger.warning("No documents provided for re-ranking.")
            return []
        
        try:
            logger.debug("Encoding query and documents.")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities.")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)  # Adjust k to the number of available documents
            if actual_k == 0:
                logger.warning("No documents available for re-ranking.")
                return []
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}.")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            logger.info(f"Re-ranked {actual_k} documents based on semantic similarity.")
            return re_ranked_docs
        except Exception as e:
            logger.error(f"Error during re-ranking with Sentence Transformers: {e}", exc_info=True)
            return []

def rerank_results(query: str, retrieved_docs: List[Dict[str, Any]], k: int = 20) -> List[Dict[str, Any]]:
    """
    Re-ranks the retrieved documents using Sentence Transformers.
    
    Args:
        query (str): The search query.
        retrieved_docs (List[Dict[str, Any]]): List of retrieved documents.
        k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.info(f"Starting re-ranking of {len(retrieved_docs)} documents for query: '{query}' using Sentence Transformers.")
    if not retrieved_docs:
        logger.warning(f"No documents to re-rank for query: '{query}'")
        return []
    try:
        # Initialize the re-ranker
        reranker = SentenceTransformerReRanker()
        
        # Extract document contents
        documents = [doc['chunk']['original_content'] for doc in retrieved_docs]
        
        # Perform re-ranking
        re_ranked = reranker.rerank(query, documents, top_k=k)
        
        # Attach scores back to the documents
        re_ranked_docs = []
        for r, original_doc in zip(re_ranked, retrieved_docs[:len(re_ranked)]):  # Ensure matching length
            re_ranked_docs.append({
                "chunk": original_doc['chunk'],
                "score": r['score']
            })
        
        logger.info(f"Re-ranking completed using Sentence Transformers. Top {k} documents selected.")
        return re_ranked_docs
    except Exception as e:
        logger.error(f"Error during document re-ranking with Sentence Transformers: {e}", exc_info=True)
        return retrieved_docs[:k]  # Fallback to original ranking if re-ranking fails

def retrieve_with_reranking(query: str, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int) -> List[Dict[str, Any]]:
    logger.debug(f"Entering retrieve_with_reranking method with query='{query}' and k={k}.")
    start_time = time.time()

    try:
        logger.debug(f"Performing hybrid retrieval for query: '{query}'")
        initial_results = hybrid_retrieval(query, db, es_bm25, k=k*10)
        logger.debug(f"Initial hybrid retrieval returned {len(initial_results)} results.")

        # **Add logging for all initial chunks retrieved**
        logger.info(f"Total chunks retrieved during hybrid retrieval for query '{query}': {len(initial_results)}")
        logger.info(f"Chunk IDs retrieved during hybrid retrieval: {[res['chunk']['chunk_id'] for res in initial_results]}")

        if not initial_results:
            logger.warning(f"No initial results retrieved for query '{query}'. Skipping reranking.")
            return []

        # Re-rank the retrieved documents
        final_results = rerank_results(query, initial_results, top_k=5)  # Set top_k to 5

    except Exception as e:
        logger.error(f"Error during retrieval or re-ranking for query '{query}': {e}", exc_info=True)
        return []

    end_time = time.time()
    logger.debug(f"Exiting retrieve_with_reranking method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/answer_generator.py
import logging
from typing import List, Dict, Any
import dspy
import asyncio
from metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from utils.utils import check_answer_length

logger = logging.getLogger(__name__)

class QuestionAnswerSignature(dspy.Signature):
    input: str = dspy.InputField(
        desc=(
            "The combined input containing both the question and context. "
            "The format should be 'question: <question_text> context: <context_text>'."
        )
    )
    answer: str = dspy.OutputField(
        desc=(
            "The generated answer to the question. The answer should be concise, directly address "
            "the question, and be grounded in the provided context to ensure factual accuracy."
        )
    )

    def forward(self, input: str, max_tokens: int = 8192) -> Dict[str, str]:
        try:
            # Parse the input to extract question and context
            parts = input.split(' context: ', 1)
            question = parts[0].replace('question: ', '').strip()
            context = parts[1].strip() if len(parts) > 1 else ""

            logger.debug(f"Generating answer for question: '{question}' with context length: {len(context)} characters.")
            answer = self.language_model.generate(
                prompt=(
                    f"You are an expert in qualitative research and thematic analysis.\n\n"
                    f"**Guidelines**:\n"
                    f"- **Relevance:** Extract quotations that are closely related to the key themes.\n"
                    f"- **Diversity:** Ensure a range of perspectives and viewpoints.\n"
                    f"- **Clarity:** Choose clear and understandable quotations.\n"
                    f"- **Impact:** Select impactful quotations that highlight significant aspects of the data.\n"
                    f"- **Authenticity:** Maintain original expressions from participants.\n\n"
                    f"**Transcript Chunk**:\n{question}\n\n"
                    f"**Context:**\n{context}\n\n"
                    f"**Task:** Extract **3-5** relevant quotations from the transcript chunk based on the context provided. "
                    f"Provide each quotation in the following JSON format within a list:\n\n"
                    f"```json\n"
                    f"[\n"
                    f"    {{\"QUOTE\": \"This is the first quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the second quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the third quotation.\"}}\n"
                    f"]\n"
                    f"```"
                    f"Ensure that the response is a valid JSON array containing all relevant quotations. "
                    f"If no quotations are available, respond with an empty array `[]`."
                ),
                max_tokens=max_tokens,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=None
            ).strip()
            logger.info(f"Generated answer for question: '{question}'")
            logger.debug(f"Answer length: {len(answer)} characters.")
            return {"answer": answer}
        except Exception as e:
            logger.error(f"Error in QuestionAnswerSignature.forward: {e}", exc_info=True)
            return {"answer": "I'm sorry, I couldn't generate an answer at this time."}

try:
    qa_module = dspy.Program.load("optimized_program.json")
    logger.info("Optimized DSPy program loaded successfully.")
except Exception as e:
    try:
        qa_module = dspy.ChainOfThought(QuestionAnswerSignature)
        logger.info("Unoptimized DSPy module initialized successfully.")
    except Exception as inner_e:
        logger.error(f"Error initializing unoptimized DSPy module: {inner_e}", exc_info=True)
        raise

async def generate_answer(input: str, max_tokens: int = 8192) -> str:
    try:
        logger.debug(f"Generating answer for input with length: {len(input)} characters.")
        answer = await asyncio.to_thread(qa_module, input=input, max_tokens=max_tokens)
        return answer.get("answer", "I'm sorry, I couldn't generate an answer at this time.")
    except Exception as e:
        logger.error(f"Error in generate_answer: {e}", exc_info=True)
        return "I'm sorry, I couldn't generate an answer at this time."

async def evaluate_answer(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    try:
        logger.debug(f"Evaluating answer for input: '{example.get('input', '')}'")
        return await asyncio.to_thread(is_answer_fully_correct, example, pred)
    except Exception as e:
        logger.error(f"Error in evaluate_answer: {e}", exc_info=True)
        return False

async def generate_answer_dspy(query: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    logger.debug(f"Entering generate_answer_dspy with query='{query}' and {len(retrieved_chunks)} retrieved_chunks.")
    try:
        context = ""
        for i, chunk in enumerate(retrieved_chunks, 1):
            chunk_content = chunk['chunk'].get('original_content', '')
            chunk_context = chunk['chunk'].get('contextualized_content', '')
            context += f"Chunk {i}:\n"
            context += f"Content: {chunk_content}\n"
            context += f"Context: {chunk_context}\n\n"

        if not context.strip():
            logger.warning(f"No valid context found for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't find relevant information to answer your question.",
                "used_chunks": [],
                "num_chunks_used": 0
            }

        logger.debug(f"Formatted context with {len(retrieved_chunks)} sequential chunks:\n{context[:200]}...")
        used_chunks_info = [
            {
                "chunk_id": chunk['chunk'].get('chunk_id', ''),
                "doc_id": chunk['chunk'].get('doc_id', ''),
                "content_snippet": chunk['chunk'].get('original_content', '')[:100] + "..."
            }
            for chunk in retrieved_chunks
        ]
        logger.info(f"Total number of chunks used for context in query '{query}': {len(used_chunks_info)}")
        logger.info(f"Chunks used for context: {used_chunks_info}")
        input_data = f"question: {query} context: {context}"
        answer = await generate_answer(input_data)

        if not answer:
            logger.warning(f"No answer generated for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't generate an answer at this time.",
                "used_chunks": used_chunks_info,
                "num_chunks_used": len(used_chunks_info)
            }

        logger.debug(f"Generated answer for query '{query}': {answer}")
        logger.info(f"Number of chunks used for query '{query}': {len(used_chunks_info)}")
        example = {
            "context": context,
            "question": query
        }
        pred = {
            "answer": answer
        }
        suggestion = await evaluate_answer(example, pred)
        return {
            "answer": answer,
            "used_chunks": used_chunks_info,
            "num_chunks_used": len(used_chunks_info)
        }
    except Exception as e:
        logger.error(f"Error generating answer via DSPy for query '{query}': {e}", exc_info=True)
        return {
            "answer": "I'm sorry, I couldn't generate an answer at this time.",
            "used_chunks": [],
            "num_chunks_used": 0
        }

async def generate_answers_dspy(queries: List[str], retrieved_chunks_list: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    tasks = [
        generate_answer_dspy(query, retrieved_chunks)
        for query, retrieved_chunks in zip(queries, retrieved_chunks_list)
    ]
    return await asyncio.gather(*tasks)

def is_answer_factually_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    score = factuality_metric(example, pred)
    logger.debug(f"Factuality score: {score}")
    return score == 1
################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/select_quotation_module.py
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/select_quotation_module.py

import logging
from typing import Dict, Any, List
import dspy
from select_quotation import SelectQuotationSignature

logger = logging.getLogger(__name__)

class SelectQuotationModule(dspy.Module):
    """
    DSPy module to select quotations based on research objectives and transcript chunks.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(SelectQuotationSignature)

    def forward(self, research_objectives: str, transcript_chunks: List[str]) -> Dict[str, Any]:
        try:
            logger.debug("Running SelectQuotationModule.")
            response = self.chain(research_objectives=research_objectives, transcript_chunks=transcript_chunks)
            quotations = response.get("quotations", [])
            types_and_functions = response.get("types_and_functions", [])
            purpose = response.get("purpose", "")
            logger.info(f"Selected {len(quotations)} quotations.")
            return {
                "quotations": quotations,
                "types_and_functions": types_and_functions,
                "purpose": purpose
            }
        except Exception as e:
            logger.error(f"Error in SelectQuotationModule.forward: {e}", exc_info=True)
            return {
                "quotations": [],
                "types_and_functions": [],
                "purpose": ""
            }

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/retrieval.py
# File: retrieval.py

import logging
import time
from typing import List, Dict, Any
from contextual_vector_db import ContextualVectorDB
from elasticsearch_bm25 import ElasticsearchBM25

from utils.logger import setup_logging
import dspy
from query_generator import QueryGeneratorSignature
from utils.utils import compute_similarity

# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)


def hybrid_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    semantic_weight: float = 0.2,
    bm25_content_weight: float = 0.2,
    bm25_contextual_weight: float = 0.6,
    min_chunks: int = 1
) -> List[Dict[str, Any]]:
    """
    Performs hybrid retrieval by combining FAISS semantic search and dual BM25 contextual search using Reciprocal Rank Fusion.
    """
    logger.debug(
        f"Entering hybrid_retrieval with query='{query}', k={k}, "
        f"semantic_weight={semantic_weight}, bm25_content_weight={bm25_content_weight}, "
        f"bm25_contextual_weight={bm25_contextual_weight}, min_chunks={min_chunks}."
    )
    start_time = time.time()
    num_chunks_to_recall = k * 10  # Retrieve more to improve chances

    # Initialize reciprocal rank fusion score dictionary
    chunk_id_to_score = {}

    while True:
        # Semantic search
        logger.debug(f"Performing semantic search using FAISS for query: '{query}'")
        semantic_results = db.search(query, k=num_chunks_to_recall)
        ranked_semantic = [result['chunk_id'] for result in semantic_results]
        semantic_scores = {result['chunk_id']: result['score'] for result in semantic_results}
        logger.debug(f"Semantic search retrieved {len(ranked_semantic)} chunk IDs.")

        # BM25 search on 'content'
        logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        bm25_content_results = es_bm25.search_content(query, k=num_chunks_to_recall)
        ranked_bm25_content = [result['chunk_id'] for result in bm25_content_results]
        bm25_content_scores = {result['chunk_id']: result['score'] for result in bm25_content_results}
        logger.debug(f"BM25 'content' search retrieved {len(ranked_bm25_content)} chunk IDs.")

        # BM25 search on 'contextualized_content'
        logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        bm25_contextual_results = es_bm25.search_contextualized(query, k=num_chunks_to_recall)
        ranked_bm25_contextual = [result['chunk_id'] for result in bm25_contextual_results]
        bm25_contextual_scores = {result['chunk_id']: result['score'] for result in bm25_contextual_results}
        logger.debug(f"BM25 'contextualized_content' search retrieved {len(ranked_bm25_contextual)} chunk IDs.")

        # Combine all unique chunk IDs
        chunk_ids = list(set(ranked_semantic + ranked_bm25_content + ranked_bm25_contextual))
        logger.debug(f"Total unique chunk IDs after combining: {len(chunk_ids)}")

        # Calculate Reciprocal Rank Fusion scores
        for chunk_id in chunk_ids:
            score = 0
            if chunk_id in ranked_semantic:
                index = ranked_semantic.index(chunk_id)
                score += semantic_weight * (1 / (index + 1))
                logger.debug(
                    f"Added semantic RRF score for chunk_id {chunk_id}: "
                    f"{semantic_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_content:
                index = ranked_bm25_content.index(chunk_id)
                score += bm25_content_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_content_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_contextual:
                index = ranked_bm25_contextual.index(chunk_id)
                score += bm25_contextual_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'contextualized_content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_contextual_weight * (1 / (index + 1))}"
                )
            chunk_id_to_score[chunk_id] = score

        # Sort chunk IDs by their RRF scores in descending order
        sorted_chunk_ids = sorted(
            chunk_id_to_score.keys(),
            key=lambda x: chunk_id_to_score[x],
            reverse=True
        )
        logger.debug(f"Sorted chunk IDs based on RRF scores.")

        # Select top k chunks, ensuring at least min_chunks are returned
        final_results = []
        filtered_count = 0
        for chunk_id in sorted_chunk_ids[:k]:
            chunk_metadata = next(
                (chunk for chunk in db.metadata if chunk['chunk_id'] == chunk_id),
                None
            )
            if not chunk_metadata:
                filtered_count += 1
                logger.warning(f"Chunk metadata not found for chunk_id {chunk_id}")
                continue
            final_results.append({
                'chunk': chunk_metadata,
                'score': chunk_id_to_score[chunk_id]
            })

        logger.info(f"Filtered {filtered_count} chunks due to missing metadata.")
        logger.info(
            f"Total chunks retrieved after filtering: {len(final_results)} "
            f"(required min_chunks={min_chunks})"
        )

        if len(final_results) >= min_chunks or k >= num_chunks_to_recall:
            break
        else:
            k += 5  # Increment k to retrieve more chunks
            logger.info(
                f"Number of retrieved chunks ({len(final_results)}) is less than min_chunks ({min_chunks}). "
                f"Increasing k to {k} and retrying retrieval."
            )

    logger.debug(f"Hybrid retrieval returning {len(final_results)} chunks.")
    logger.info(
        f"Chunks used for hybrid retrieval for query '{query}': "
        f"[{', '.join([res['chunk']['chunk_id'] for res in final_results])}]"
    )
    end_time = time.time()
    logger.debug(f"Exiting hybrid_retrieval method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results


def multi_stage_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    max_hops: int = 3,
    max_results: int = 5,
    similarity_threshold: float = 0.9
) -> List[Dict[str, Any]]:
    accumulated_context = ""
    all_retrieved_chunks = {}
    current_query = query
    previous_query = ""
    query_generator = dspy.ChainOfThought(QueryGeneratorSignature)

    for hop in range(max_hops):
        logger.info(f"Starting hop {hop+1} with query: '{current_query}'")

        retrieved_chunks = hybrid_retrieval(current_query, db, es_bm25, k)

        # Update accumulated retrieved chunks
        for chunk in retrieved_chunks:
            chunk_id = chunk['chunk']['chunk_id']
            if chunk_id not in all_retrieved_chunks:
                all_retrieved_chunks[chunk_id] = chunk

        # Check if we have reached the desired number of results
        if len(all_retrieved_chunks) >= max_results:
            logger.info(f"Retrieved sufficient chunks ({len(all_retrieved_chunks)}). Terminating.")
            break

        # Accumulate new context from retrieved chunks
        new_context = "\n\n".join([
            chunk['chunk'].get('contextualized_content', '') or chunk['chunk'].get('original_content', '')
            for chunk in retrieved_chunks
        ])
        accumulated_context += "\n\n" + new_context

        # Generate a new query based on the accumulated context
        response = query_generator(question=query, context=accumulated_context)
        new_query = response.get('new_query', '').strip()
        if not new_query:
            logger.info("No new query generated. Terminating multi-stage retrieval.")
            break

        # Compute similarity between the new query and the previous query
        similarity = compute_similarity(current_query, new_query)
        logger.debug(f"Similarity between queries: {similarity:.4f}")
        if similarity >= similarity_threshold:
            logger.info("New query is too similar to the current query. Terminating multi-stage retrieval.")
            break

        # Update queries for the next iteration
        previous_query = current_query
        current_query = new_query

    # Sort and return the top results
    final_results = sorted(
        all_retrieved_chunks.values(),
        key=lambda x: x['score'],
        reverse=True
    )[:max_results]

    logger.info(f"Multi-stage retrieval completed with {len(final_results)} chunks.")
    return final_results

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/query_processor.py
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/query_processor.py

import logging
from typing import List, Dict, Any, Callable
from contextual_vector_db import ContextualVectorDB
from elasticsearch_bm25 import ElasticsearchBM25
from retrieval import multi_stage_retrieval
from reranking import retrieve_with_reranking
from select_quotation_module import SelectQuotationModule  # Import the new module
from answer_generator import generate_answer_dspy
from utils.logger import setup_logging
import json
from tqdm import tqdm
from src.decorators import handle_exceptions
from utils.validation_functions import validate_relevance, validate_quality, validate_context_clarity

# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

def validate_queries(queries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Validates the structure of input queries.

    Args:
        queries (List[Dict[str, Any]]): List of query items.

    Returns:
        List[Dict[str, Any]]: List of validated query items.
    """
    valid_queries = []
    for idx, query in enumerate(queries):
        if 'query' not in query or not query['query'].strip():
            logger.warning(f"Query at index {idx} is missing the 'query' field or is empty. Skipping.")
            continue

        # Check for optional golden fields
        has_golden_doc_uuids = 'golden_doc_uuids' in query
        has_golden_chunk_uuids = 'golden_chunk_uuids' in query
        has_golden_documents = 'golden_documents' in query

        # If some but not all golden fields are present, log a warning
        if any([has_golden_doc_uuids, has_golden_chunk_uuids, has_golden_documents]) and not all([has_golden_doc_uuids, has_golden_chunk_uuids, has_golden_documents]):
            logger.warning(f"Query at index {idx} has incomplete golden data. All golden fields should be present together.")

        valid_queries.append(query)

    logger.info(f"Validated {len(valid_queries)} queries out of {len(queries)} provided.")
    return valid_queries

def retrieve_documents(query: str, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int) -> List[Dict[str, Any]]:
    """
    Retrieves documents using multi-stage retrieval with contextual BM25.

    Args:
        query (str): The search query.
        db (ContextualVectorDB): Contextual vector database instance.
        es_bm25 (ElasticsearchBM25): Elasticsearch BM25 instance.
        k (int): Number of top documents to retrieve.

    Returns:
        List[Dict[str, Any]]: List of retrieved documents.
    """
    logger.debug(f"Retrieving documents for query: '{query}' with top {k} results using multi-stage retrieval with contextual BM25.")
    final_results = multi_stage_retrieval(query, db, es_bm25, k)
    logger.debug(f"Multi-stage retrieval with contextual BM25 returned {len(final_results)} results.")
    return final_results

@handle_exceptions
async def process_single_query(query_item: Dict[str, Any], db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int, quotation_module: SelectQuotationModule) -> Dict[str, Any]:
    query_text = query_item.get('query', '').strip()
    if not query_text:
        logger.warning(f"Query is empty. Skipping.")
        return {}

    logger.info(f"Processing query: {query_text}")

    # Retrieve relevant chunks/documents using multi-stage retrieval with contextual BM25
    retrieved_chunks = retrieve_documents(query_text, db, es_bm25, k)
    logger.info(f"Total chunks retrieved for query '{query_text}': {len(retrieved_chunks)}")
    logger.info(f"Retrieved chunk IDs: {[chunk['chunk']['chunk_id'] for chunk in retrieved_chunks]}")

    # Extract transcript chunks from retrieved documents
    transcript_chunks = [chunk['chunk']['original_content'] for chunk in retrieved_chunks]

    # Define research objectives (this could be part of the query_item or defined elsewhere)
    research_objectives = query_item.get('research_objectives', 'Extract relevant quotations based on the provided objectives.')

    # Select quotations using the new DSPy module
    quotations_response = quotation_module.forward(research_objectives=research_objectives, transcript_chunks=transcript_chunks)
    quotations = quotations_response.get("quotations", [])
    types_and_functions = quotations_response.get("types_and_functions", [])
    purpose = quotations_response.get("purpose", "")

    # Generate answer using DSPy with assertions (optional, depending on your use-case)
    qa_response = await generate_answer_dspy(query_text, retrieved_chunks)
    answer = qa_response.get("answer", "")
    used_chunks_info = qa_response.get("used_chunks", [])
    retrieved_chunks_count = qa_response.get("num_chunks_used", 0)

    result = {
        "query": query_text,
        "research_objectives": research_objectives,
        "retrieved_chunks": used_chunks_info,
        "retrieved_chunks_count": retrieved_chunks_count,
        "used_chunk_ids": [chunk['chunk_id'] for chunk in used_chunks_info],
        "quotations": quotations,
        "types_and_functions": types_and_functions,
        "purpose": purpose,
        "answer": {
            "answer": answer
        }
    }

    # Log the chunks used for this query
    logger.info(f"Query '{query_text}' used {retrieved_chunks_count} chunks in answer generation.")
    logger.info(f"Selected {len(quotations)} quotations for query '{query_text}'.")

    return result

def save_results(results: List[Dict[str, Any]], output_file: str):
    """
    Saves the results of the processed queries to a specified output file.

    Args:
        results (List[Dict[str, Any]]): List of results to save.
        output_file (str): Path to the output file.
    """
    try:
        with open(output_file, 'w', encoding='utf-8') as outfile:
            json.dump(results, outfile, indent=4)
        logger.info(f"All query results have been saved to '{output_file}'")
    except Exception as e:
        logger.error(f"Error saving results to '{output_file}': {e}", exc_info=True)

@handle_exceptions
async def process_queries(
    queries: List[Dict[str, Any]],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    output_file: str
):
    logger.info("Starting to process queries.")

    all_results = []
    quotation_module = SelectQuotationModule()  # Initialize the quotation selection module

    try:
        for idx, query_item in enumerate(tqdm(queries, desc="Processing queries")):
            try:
                result = await process_single_query(query_item, db, es_bm25, k, quotation_module)
                if result:
                    all_results.append(result)
            except Exception as e:
                logger.error(f"Error processing query at index {idx}: {e}", exc_info=True)

        save_results(all_results, output_file)

    except KeyboardInterrupt:
        logger.warning("Process interrupted by user. Saving partial results.")
        save_results(all_results, output_file)
        raise

    except Exception as e:
        logger.error(f"Error processing queries: {e}", exc_info=True)
        raise

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/copycode.py
import logging
import os
import asyncio
import aiofiles
from typing import List
from utils.logger import setup_logging
import time

logger = logging.getLogger(__name__)

async def get_code_files(root_dir: str, extensions: List[str] = None) -> List[str]:
    logger.debug(f"Entering get_code_files with root_dir='{root_dir}' and extensions={extensions}.")
    start_time = time.time()
    code_files = []
    try:
        logger.debug(f"Walking through directory: {root_dir}")
        for root, dirs, files in os.walk(root_dir):
            ignored_dirs = {'.venv', 'venv', 'env', '.env', 'myenv'}
            dirs[:] = [d for d in dirs if d not in ignored_dirs]
            for ignored in ignored_dirs:
                if ignored in dirs:
                    dirs.remove(ignored)
                    logger.debug(f"Skipping directory: {os.path.join(root, ignored)}")
            
            for file in files:
                if extensions is None or file.endswith(tuple(extensions)):
                    file_path = os.path.join(root, file)
                    code_files.append(file_path)
                    logger.debug(f"Found code file: {file_path}")
    except Exception as e:
        logger.error(f"Error while retrieving code files: {e}", exc_info=True)
    
    logger.info(f"Total code files found: {len(code_files)}")
    end_time = time.time()
    logger.debug(f"Exiting get_code_files method. Time taken: {end_time - start_time:.2f} seconds.")
    return code_files

async def copy_code_to_file(code_files: List[str], output_file: str):
    logger.debug(f"Entering copy_code_to_file with output_file='{output_file}'.")
    start_time = time.time()
    try:
        logger.info(f"Writing consolidated code to '{output_file}'")
        async with aiofiles.open(output_file, 'w', encoding='utf-8') as outfile:
            for filepath in code_files:
                await outfile.write(f"\n\n# File: {filepath}\n")
                try:
                    async with aiofiles.open(filepath, 'r', encoding='utf-8', errors='ignore') as infile:
                        content = await infile.read()
                        await outfile.write(content)
                    logger.debug(f"Copied content from '{filepath}'")
                except Exception as e:
                    await outfile.write(f"# Error reading file {filepath}: {e}\n")
                    logger.error(f"Error reading file '{filepath}': {e}", exc_info=True)
                await outfile.write("\n" + "#" * 80 + "\n")
        logger.info(f"All code has been copied to '{output_file}'")
    except Exception as e:
        logger.error(f"Error during copying code to file '{output_file}': {e}", exc_info=True)
    end_time = time.time()
    logger.debug(f"Exiting copy_code_to_file method. Time taken: {end_time - start_time:.2f} seconds.")

async def main_async():
    logger.debug("Entering main_async function.")
    start_time = time.time()
    try:
        script_directory = os.path.dirname(os.path.abspath(__file__))
        logger.debug(f"Script directory: {script_directory}")

        source_directory = script_directory

        output_file = os.path.join(script_directory, 'consolidated_code.txt')

        file_extensions = ['.py', '.yaml']
        logger.debug(f"File extensions to include: {file_extensions}")

        code_files = await get_code_files(source_directory, file_extensions)

        await copy_code_to_file(code_files, output_file)
    except Exception as e:
        logger.error(f"Unexpected error in main_async: {e}", exc_info=True)
    end_time = time.time()
    logger.debug(f"Exiting main_async function. Total time taken: {end_time - start_time:.2f} seconds.")

def main():
    logger.debug("Entering main function.")
    start_time = time.time()
    try:
        asyncio.run(main_async())
    except KeyboardInterrupt:
        logger.warning("Code copying interrupted by user.")
    except Exception as e:
        logger.error(f"Unexpected error during code copying: {e}", exc_info=True)
    end_time = time.time()
    logger.debug(f"Exiting main function. Total time taken: {end_time - start_time:.2f} seconds.")

if __name__ == "__main__":
    main()

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/contextual_vector_db.py
import os
import pickle
import numpy as np
import threading
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from dotenv import load_dotenv

from openai_client import OpenAIClient
import dspy
import logging
import faiss
from utils.logger import setup_logging
setup_logging()
logger = logging.getLogger(__name__)


class SituateContext(dspy.Module):
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(SituateContextSignature)

    def forward(self, doc: str, chunk: str):
        prompt = f"""
                <document>
                {doc}
                </document>
            

                CHUNK_CONTEXT_PROMPT = 
                Here is the chunk we want to situate within the whole document
                <chunk>
                {chunk}
                </chunk>

                Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
                Answer only with the succinct context and nothing else.
    """
        
        return self.chain(doc=doc, chunk=chunk, prompt=prompt)


class SituateContextSignature(dspy.Signature):
    doc = dspy.InputField(desc="Full document content")
    chunk = dspy.InputField(desc="Specific chunk content")
    reasoning = dspy.OutputField(desc="Chain of thought reasoning")
    contextualized_content = dspy.OutputField(desc="Contextualized content for the chunk")
        

class ContextualVectorDB:
    def __init__(self, name: str, openai_api_key: str = None):
        if openai_api_key is None:
            openai_api_key = os.getenv("OPENAI_API_KEY")
        
        self.name = name
        self.embeddings = []
        self.metadata = []
        self.db_path = f"./data/{name}/contextual_vector_db.pkl"
        self.faiss_index_path = f"./data/{name}/faiss_index.bin"

        self.client = OpenAIClient(api_key=openai_api_key)
        logger.debug(f"Initialized OpenAIClient for ContextualVectorDB '{self.name}'")

    def situate_context(self, doc: str, chunk: str) -> Tuple[str, Any]:
        logger.debug(f"Entering situate_context with doc length={len(doc)} and chunk length={len(chunk)}.")
        try:
            if not hasattr(self, 'situate_context_module'):
                self.situate_context_module = SituateContext()
                logger.debug("Initialized SituateContext module")

            response = self.situate_context_module(doc=doc, chunk=chunk)
            contextualized_content = response.contextualized_content
            logger.debug("Generated contextualized_content using DSPy.")
            usage_metrics = {}
            return contextualized_content, usage_metrics
        except Exception as e:
            logger.error(f"Error during DSPy situate_context: {e}", exc_info=True)
            return "", None

    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 1):
        logger.debug("Entering load_data method.")
        if self.embeddings and self.metadata and os.path.exists(self.faiss_index_path):
            logger.info("Vector database is already loaded. Skipping data loading.")
            return
        if os.path.exists(self.db_path) and os.path.exists(self.faiss_index_path):
            logger.info("Loading vector database and FAISS index from disk.")
            self.load_db()
            self.load_faiss_index()
            return

        texts_to_embed, metadata = self._process_dataset(dataset, parallel_threads)
        
        self._embed_and_store(texts_to_embed, metadata)
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}")


    def _process_dataset(self, dataset: List[Dict[str, Any]], parallel_threads: int) -> Tuple[List[str], List[Dict[str, Any]]]:
        texts_to_embed = []
        metadata = []
        total_chunks = sum(len(doc.get('chunks', [])) for doc in dataset)
        logger.info(f"Total chunks to process: {total_chunks}")

        logger.info(f"Processing {total_chunks} chunks with {parallel_threads} threads.")
        try:
            with ThreadPoolExecutor(max_workers=parallel_threads) as executor:
                futures = []
                for doc in dataset:
                    for chunk in doc.get('chunks', []):
                        futures.append(executor.submit(self._generate_contextualized_content, doc, chunk))
                
                for future in tqdm(as_completed(futures), total=total_chunks, desc="Processing chunks"):
                    result = future.result()
                    if result:
                        texts_to_embed.append(result['text_to_embed'])
                        metadata.append(result['metadata'])
        except Exception as e:
            logger.error(f"Error during processing chunks: {e}", exc_info=True)
            return [], []

        return texts_to_embed, metadata

    def _generate_contextualized_content(self, doc: Dict[str, Any], chunk: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(doc, dict):
            logger.error(f"Document is not a dictionary: {doc}")
            return None

        if isinstance(chunk, dict):
            chunk_id = chunk.get('chunk_id')
            if not chunk_id:
                # Assign a unique chunk_id combining doc_id and chunk's index
                chunk_index = chunk.get('index', 0)
                chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_{chunk_index}"
                chunk['chunk_id'] = chunk_id
            content = chunk.get('content', '')
            original_index = chunk.get('original_index', chunk.get('index', 0))
        elif isinstance(chunk, str):
            # Handle case where chunk is a string
            content = chunk
            chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_0"
            original_index = 0
            logger.warning(f"Chunk is a string. Expected a dict. Assigning default values.")
        else:
            logger.error(f"Unsupported chunk type: {type(chunk)}. Skipping chunk.")
            return None

        logger.debug(f"Processing chunk_id='{chunk_id}' in doc_id='{doc.get('doc_id', 'unknown_doc_id')}'")
        contextualized_text, usage = self.situate_context(doc.get('content', ''), content)
        return {
            'text_to_embed': f"{content}\n\n{contextualized_text}",
            'metadata': {
                'doc_id': doc.get('doc_id', ''),
                'original_uuid': doc.get('original_uuid', ''),
                'chunk_id': chunk_id,
                'original_index': original_index,
                'original_content': content,
                'contextualized_content': contextualized_text
            }
        }

    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):
        logger.debug("Entering _embed_and_store method.")
        batch_size = 128
        embeddings = []
        logger.info("Starting embedding generation.")
        try:
            with tqdm(total=len(texts), desc="Embedding chunks") as pbar:
                for i in range(0, len(texts), batch_size):
                    batch = texts[i : i + batch_size]
                    try:
                        response = self.client.create_embeddings(
                            model="text-embedding-3-small",
                            input=batch
                        )
                        embeddings_batch = [item['embedding'] for item in response['data']]
                        embeddings.extend(embeddings_batch)
                        pbar.update(len(batch))
                        logger.debug(f"Processed batch {i // batch_size + 1}: {len(batch)} embeddings.")
                    except Exception as e:
                        logger.error(f"Error during OpenAI embeddings for batch starting at index {i}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Unexpected error during embedding generation: {e}", exc_info=True)
        
        self.embeddings = embeddings
        self.metadata = data
        logger.info("Embedding generation completed.")

    def _build_faiss_index(self):
        self.create_faiss_index()
        self.save_faiss_index()

    def create_faiss_index(self):
        logger.debug("Entering create_faiss_index method.")
        try:
            embedding_dim = len(self.embeddings[0])
            logger.info(f"Embedding dimension: {embedding_dim}")
            embeddings_np = np.array(self.embeddings).astype('float32')
            faiss.normalize_L2(embeddings_np)
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.index.add(embeddings_np)
            logger.info(f"FAISS index created with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error creating FAISS index: {e}", exc_info=True)
            raise

    def save_faiss_index(self):
        logger.debug("Entering save_faiss_index method.")
        os.makedirs(os.path.dirname(self.faiss_index_path), exist_ok=True)
        try:
            faiss.write_index(self.index, self.faiss_index_path)
            logger.info(f"FAISS index saved to '{self.faiss_index_path}'")
        except Exception as e:
            logger.error(f"Error saving FAISS index to '{self.faiss_index_path}': {e}", exc_info=True)

    def load_faiss_index(self):
        logger.debug("Entering load_faiss_index method.")
        if not os.path.exists(self.faiss_index_path):
            logger.error(f"FAISS index file not found at '{self.faiss_index_path}'.")
            raise ValueError("FAISS index file not found.")
        try:
            self.index = faiss.read_index(self.faiss_index_path)
            logger.info(f"FAISS index loaded from '{self.faiss_index_path}' with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error loading FAISS index from '{self.faiss_index_path}': {e}", exc_info=True)
            raise

    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        logger.debug(f"Entering search method with query='{query}' and k={k}.")
        if not self.embeddings or not self.metadata:
            logger.error("Embeddings or metadata are not loaded. Cannot perform search.")
            return []
        if not hasattr(self, 'index'):
            logger.error("FAISS index is not loaded.")
            return []
        
        try:
            response = self.client.create_embeddings(
                model="text-embedding-3-small",
                input=[query]
            )
            query_embedding = response['data'][0]['embedding']
            logger.debug(f"Generated embedding for query: '{query}'")
        except Exception as e:
            logger.error(f"Error generating embedding for query '{query}': {e}", exc_info=True)
            return []

        query_embedding_np = np.array([query_embedding]).astype('float32')
        faiss.normalize_L2(query_embedding_np)

        logger.debug("Performing FAISS search.")
        try:
            distances, indices = self.index.search(query_embedding_np, k)
            indices = indices.flatten()
            distances = distances.flatten()
        except Exception as e:
            logger.error(f"Error during FAISS search: {e}", exc_info=True)
            return []

        top_results = []
        for idx, score in zip(indices, distances):
            if idx < len(self.metadata):
                meta = self.metadata[idx]
                result = {
                    "doc_id": meta['doc_id'],
                    "chunk_id": meta['chunk_id'],
                    "original_index": meta.get('original_index', 0),
                    "content": meta['original_content'],
                    "contextualized_content": meta.get('contextualized_content'),
                    "score": float(score),
                    "metadata": meta
                }
                top_results.append(result)
            else:
                logger.warning(f"Index {idx} out of bounds for metadata.")
        logger.debug(f"FAISS search returned {len(top_results)} results for query: '{query}'")
        logger.info(f"Chunks retrieved for query '{query}': {[res['chunk_id'] for res in top_results]}")

        return top_results

    def save_db(self):
        logger.debug("Entering save_db method.")
        data = {
            "metadata": self.metadata,
        }
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        try:
            with open(self.db_path, "wb") as file:
                pickle.dump(data, file)
            logger.info(f"Vector database metadata saved to '{self.db_path}'")
        except Exception as e:
            logger.error(f"Error saving vector database metadata to '{self.db_path}': {e}", exc_info=True)

    def load_db(self):
        logger.debug("Entering load_db method.")
        if not os.path.exists(self.db_path):
            logger.error(f"Vector database file not found at '{self.db_path}'. Use load_data to create a new database.")
            raise ValueError("Vector database file not found.")
        try:
            with open(self.db_path, "rb") as file:
                data = pickle.load(file)
            self.metadata = data.get("metadata", [])
            logger.info(f"Vector database metadata loaded from '{self.db_path}' with {len(self.metadata)} entries.")
            logger.info(f"Chunks loaded: {[meta['chunk_id'] for meta in self.metadata]}")
        except Exception as e:
            logger.error(f"Error loading vector database metadata from '{self.db_path}': {e}", exc_info=True)
            raise

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/openai_client.py
import logging
import os
from typing import List, Dict, Any
from openai import OpenAI

# Initialize logger
logger = logging.getLogger(__name__)

class OpenAIClient:
    """
    Wrapper for OpenAI API interactions using the updated SDK.
    """
    def __init__(self, api_key: str):
        """
        Initializes the OpenAI client with the provided API key.
        
        Args:
            api_key (str): OpenAI API key.
        """
        if not api_key:
            logger.error("OpenAI API key is not provided.")
            raise ValueError("OpenAI API key must be provided.")
        self.client = OpenAI(api_key=api_key)
        logger.debug("OpenAI client initialized successfully.")

    def create_chat_completion(self, model: str, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> Dict[str, Any]:
        """
        Creates a chat completion using OpenAI's API.
        
        Args:
            model (str): Model name to use.
            messages (List[Dict[str, str]]): List of message dictionaries.
            max_tokens (int): Maximum number of tokens in the response.
            temperature (float): Sampling temperature.
        
        Returns:
            Dict[str, Any]: API response as a dictionary.
        """
        try:
            response = self.client.chat.completions.create(
                model='gpt-4o-mini',
                messages=messages,
                max_tokens='8192',
                temperature=temperature,
            )
            logger.debug(f"Chat completion created successfully for model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating chat completion: {e}")
            raise

    def create_embeddings(self, model: str, input: List[str]) -> Dict[str, Any]:
        """
        Creates embeddings for the given input texts using OpenAI's API.
        
        Args:
            model (str): Embedding model to use.
            input (List[str]): List of input texts.
        
        Returns:
            Dict[str, Any]: API response containing embeddings.
        """
        try:
            response = self.client.embeddings.create(
                model=model,
                input=input
            )
            logger.debug(f"Embeddings created successfully using model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            raise

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/main.py
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/main.py

import gc

# Clear any existing caches
gc.collect()

import logging
import os
from typing import List, Dict, Any
from utils.logger import setup_logging
from contextual_vector_db import ContextualVectorDB
from elasticsearch_bm25 import ElasticsearchBM25  # Import ElasticsearchBM25 class

from data_loader import load_codebase_chunks, load_queries
from query_processor import validate_queries, process_queries
from evaluation import evaluate_complete_pipeline
from metrics import comprehensive_metric
from dspy.teleprompt import BootstrapFewShotWithRandomSearch
import dspy
from dspy.datasets import DataLoader
from answer_generator import generate_answer_dspy, QuestionAnswerSignature
from reranking import retrieve_with_reranking
import answer_generator
import asyncio
from src.decorators import handle_exceptions

# Initialize logging
setup_logging()
logger = logging.getLogger(__name__)

def create_elasticsearch_bm25_index(db: ContextualVectorDB) -> ElasticsearchBM25:
    """
    Create and index documents in Elasticsearch BM25.

    Args:
        db (ContextualVectorDB): Contextual vector database instance.

    Returns:
        ElasticsearchBM25: Initialized ElasticsearchBM25 instance.
    """
    logger.debug("Entering create_elasticsearch_bm25_index method.")
    try:
        es_bm25 = ElasticsearchBM25()
        logger.info("ElasticsearchBM25 instance created.")
        success_count, failed_docs = es_bm25.index_documents(db.metadata)
        logger.info(f"Elasticsearch BM25 index created successfully with {success_count} documents indexed.")
        if failed_docs:
            logger.warning(f"{len(failed_docs)} documents failed to index.")
    except Exception as e:
        logger.error(f"Error creating Elasticsearch BM25 index: {e}", exc_info=True)
        raise
    return es_bm25

@handle_exceptions
async def main():
    """
    Main function to load data, process queries, and generate outputs.
    """
    logger.debug("Entering main function.")
    try:
        # Configure DSPy Language Model using LiteLLM
        logger.info("Configuring DSPy Language Model with LiteLLM")

        lm = dspy.LM('openai/gpt-4o-mini', max_dtokens=8192)
        dspy.configure(lm=lm)

        # Define file paths
        codebase_chunks_file = 'data/codebase_chunks.json'  # Ensure this file exists
        queries_file = 'data/queries.json'                  # Ensure this file exists
        evaluation_set_file = 'data/evaluation_set.jsonl'   # Ensure this file exists
        output_filename = 'query_results.json'              # Modified to use a single output file

        dl = DataLoader()
        # Initialize the DataLoader

        # Load the training data from the new CSV format
        # Load the training data from the new CSV format
        logger.info(f"Loading training data from 'new_training_data.csv'")
        train_dataset = dl.from_csv(
            "data/new_training_data.csv",
            fields=("input", "output"),
            input_keys=("input",)
        )
        # Load the codebase chunks
        logger.info(f"Loading codebase chunks from '{codebase_chunks_file}'")
        codebase_chunks = load_codebase_chunks(codebase_chunks_file)

        # Initialize the ContextualVectorDB
        logger.info("Initializing ContextualVectorDB")
        contextual_db = ContextualVectorDB("contextual_db")

        # Load and process the data with parallel threads
        try:
            logger.info("Loading data into ContextualVectorDB with parallel threads")
            contextual_db.load_data(codebase_chunks, parallel_threads=5)
        except Exception as e:
            logger.error(f"Error loading data into ContextualVectorDB: {e}", exc_info=True)
            return

        # Create the Elasticsearch BM25 index
        try:
            logger.info("Creating Elasticsearch BM25 index")
            es_bm25 = create_elasticsearch_bm25_index(contextual_db)
        except Exception as e:
            logger.error(f"Error creating Elasticsearch BM25 index: {e}", exc_info=True)
            return

        # Load the queries
        logger.info(f"Loading queries from '{queries_file}'")
        queries = load_queries(queries_file)

        if not queries:
            logger.error("No queries found to process.")
            return

        # Validate queries
        logger.info("Validating input queries")
        validated_queries = validate_queries(queries)

        if not validated_queries:
            logger.error("No valid queries to process after validation. Exiting.")
            return

        # Load the evaluation set
        logger.info(f"Loading evaluation set from '{evaluation_set_file}'")
        evaluation_set = load_queries(evaluation_set_file)

        if not evaluation_set:
            logger.error("No evaluation queries found. Ensure the evaluation set file is correctly formatted and exists.")
            return

        # Define k value (number of top documents/chunks to retrieve)
        k = 20  # Ensure k is set correctly and not overridden elsewhere

        # Initialize 'qa_module' to None before the try-except block
        qa_module = None

        # Attempt to load the optimized program
        logger.info("Attempting to load optimized DSPy program")
        try:
            qa_module = dspy.Program.load("optimized_program.json")
            logger.info("Optimized DSPy program loaded successfully.")
        except Exception as e:
            # If loading fails, use the unoptimized module
            logger.warning("Failed to load optimized program. Using unoptimized module.")
            try:
                qa_module = dspy.ChainOfThought(QuestionAnswerSignature)
                logger.info("Unoptimized DSPy module initialized successfully.")
            except Exception as inner_e:
                logger.error(f"Error initializing unoptimized DSPy module: {inner_e}", exc_info=True)
                raise

        # Initialize DSPy Optimizer with Comprehensive Metric
        logger.info("Initializing DSPy Optimizer with Comprehensive Metric")
        try:
            optimizer_config = {
                'max_bootstrapped_demos': 4,       # Number of generated demonstrations
                'max_labeled_demos': 4,            # Number of labeled demonstrations from trainset
                'num_candidate_programs': 10,      # Number of candidate programs to evaluate
                'num_threads': 4                    # Number of parallel threads
            }
            teleprompter = BootstrapFewShotWithRandomSearch(
                metric=comprehensive_metric,  # Use the comprehensive metric
                **optimizer_config
            )
            logger.info("DSPy Optimizer initialized successfully with Comprehensive Metric.")
        except Exception as e:
            logger.error(f"Error initializing DSPy Optimizer: {e}", exc_info=True)
            return

        # Compile the program using the optimizer
        logger.info("Compiling the program using the optimizer")
        try:
            optimized_program = teleprompter.compile(
                student=qa_module,
                teacher=qa_module,  # Assuming teacher is the same as student; adjust as needed
                trainset=train_dataset
            )
            logger.info("Program compiled and optimized successfully.")
        except Exception as e:
            logger.error(f"Error during program compilation: {e}", exc_info=True)
            return

        # Save the optimized program
        try:
            optimized_program.save("optimized_program.json")
            logger.info("Optimized program saved to 'optimized_program.json'")
        except Exception as e:
            logger.error(f"Error saving optimized program: {e}", exc_info=True)

        # Assign the optimized program to answer_generator.qa_module
        try:
            answer_generator.qa_module = optimized_program
            logger.info("Assigned optimized_program to answer_generator.qa_module successfully.")
        except Exception as e:
            logger.error(f"Error assigning optimized program to answer_generator.qa_module: {e}", exc_info=True)
            return

        # Initialize SelectQuotationModule
        logger.info("Initializing SelectQuotationModule")
        try:
            from select_quotation_module import SelectQuotationModule
            quotation_module = SelectQuotationModule()
            logger.info("SelectQuotationModule initialized successfully.")
        except Exception as e:
            logger.error(f"Error initializing SelectQuotationModule: {e}", exc_info=True)
            return

        # Proceed with processing queries using the optimized program and quotation selection
        logger.info("Starting to process queries with the optimized program and quotation selection")
        try:
            await process_queries(
                validated_queries,
                contextual_db,
                es_bm25,
                k,
                output_filename  # Modified to pass only one output file
            )
        except Exception as e:
            logger.error(f"Error processing queries: {e}", exc_info=True)
            return

        # Define k values for evaluation
        k_values = [5, 10, 20]

        # Perform evaluation
        logger.info("Starting evaluation of the retrieval pipeline")
        try:
            evaluate_complete_pipeline(
                contextual_db,
                es_bm25,
                k_values,
                evaluation_set,
                retrieve_with_reranking  # Pass the correct retrieval function
            )
            logger.info("Evaluation completed successfully.")
        except Exception as e:
            logger.error(f"Error during evaluation: {e}", exc_info=True)

        logger.info("All operations completed successfully.")
    except Exception as e:
        logger.error(f"Unexpected error in main function: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/evaluator.py
import logging
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from contextual_vector_db import ContextualVectorDB
from elasticsearch_bm25 import ElasticsearchBM25

logger = logging.getLogger(__name__)

def evaluate_pipeline(queries: List[Dict[str, Any]], retrieval_function: Callable, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int = 20) -> Dict[str, float]:
    total_score = 0
    total_queries = len(queries)
    queries_with_golden = 0
    queries_without_golden = 0

    logger.info(f"Starting evaluation of {total_queries} queries.")

    for query_item in tqdm(queries, desc="Evaluating retrieval"):
        query = query_item.get('query', '').strip()

        has_golden_data = all([
            'golden_doc_uuids' in query_item,
            'golden_chunk_uuids' in query_item,
            'golden_documents' in query_item
        ])

        if has_golden_data:
            queries_with_golden += 1
            golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
            golden_contents = []

            for doc_uuid, chunk_index in golden_chunk_uuids:
                golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                if not golden_doc:
                    logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                    continue
                golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                if not golden_chunk:
                    logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                    continue
                golden_contents.append(golden_chunk.get('content', '').strip())

            if not golden_contents:
                logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                continue

            retrieved_docs = retrieval_function(query, db, es_bm25, k)

            chunks_found = 0
            for golden_content in golden_contents:
                for doc in retrieved_docs[:k]:
                    retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                    if retrieved_content == golden_content:
                        chunks_found += 1
                        break

            query_score = chunks_found / len(golden_contents)
            total_score += query_score
            logger.debug(f"Query '{query}' score: {query_score}")
        else:
            queries_without_golden += 1
            logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
            continue

    average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
    pass_at_n = average_score * 100

    logger.info(f"Evaluation completed.")
    logger.info(f"Total Queries: {total_queries}")
    logger.info(f"Queries with Golden Data: {queries_with_golden}")
    logger.info(f"Queries without Golden Data: {queries_without_golden}")
    logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")

    return {
        "pass_at_n": pass_at_n,
        "average_score": average_score,
        "total_queries": total_queries,
        "queries_with_golden": queries_with_golden,
        "queries_without_golden": queries_without_golden
    }

def evaluate_complete_pipeline(db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k_values: List[int], evaluation_set: List[Dict[str, Any]], retrieval_function: Callable):
    for k in k_values:
        logger.info(f"Starting evaluation for Pass@{k}")
        results = evaluate_pipeline(evaluation_set, retrieval_function, db, es_bm25, k)
        logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
        logger.info(f"Average Score: {results['average_score']:.4f}")
        logger.info(f"Total Queries: {results['total_queries']}")
        logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
        logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/decorators.py
import logging
import functools

logger = logging.getLogger(__name__)

def handle_exceptions(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}", exc_info=True)
            return {"error": "An error occurred. Please try again later."}
    return wrapper

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/validation_functions.py
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/validation_functions.py

import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def validate_relevance(quotations: List[str], research_objectives: str) -> bool:
    """
    Validates that each quotation is relevant to the research objectives.
    """
    try:
        for quote in quotations:
            # Simple keyword matching; can be enhanced with NLP techniques
            if not any(obj.lower() in quote.lower() for obj in research_objectives.split()):
                logger.debug(f"Quotation '{quote}' is not relevant to the research objectives.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_relevance: {e}", exc_info=True)
        return False

def validate_quality(quotations: List[str]) -> bool:
    """
    Validates the quality and representation of each quotation.
    """
    try:
        for quote in quotations:
            if len(quote.strip()) < 10:  # Example quality check
                logger.debug(f"Quotation '{quote}' is too short to be considered high quality.")
                return False
            # Additional quality checks can be added here
        return True
    except Exception as e:
        logger.error(f"Error in validate_quality: {e}", exc_info=True)
        return False

def validate_context_clarity(quotations: List[str], context: str) -> bool:
    """
    Validates that each quotation is clear and has sufficient context.
    """
    try:
        for quote in quotations:
            if quote.lower() not in context.lower():
                logger.debug(f"Quotation '{quote}' lacks sufficient context.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_context_clarity: {e}", exc_info=True)
        return False

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/__init__.py

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/logger.py
# File: src/utils/logger.py

import logging
import logging.config
import os
import yaml

def setup_logging(default_path='config/logging_config.yaml', default_level=logging.INFO):
    """
    Setup logging configuration from a YAML file.
    Ensures that the log directory exists before configuring handlers.
    """
    if os.path.exists(default_path):
        with open(default_path, 'r') as f:
            config = yaml.safe_load(f.read())

        # Extract all file handler paths to ensure directories exist
        handlers = config.get('handlers', {})
        for handler_name, handler in handlers.items():
            if 'filename' in handler:
                log_file = handler['filename']
                log_dir = os.path.dirname(log_file)
                if log_dir and not os.path.exists(log_dir):
                    try:
                        os.makedirs(log_dir, exist_ok=True)
                        print(f"Created log directory: {log_dir}")
                    except Exception as e:
                        print(f"Failed to create log directory '{log_dir}': {e}")

        # Apply the logging configuration
        logging.config.dictConfig(config)
    else:
        # If the logging configuration file is missing, use basic configuration
        logging.basicConfig(level=default_level)
        logging.warning(f"Logging configuration file not found at '{default_path}'. Using basic configuration.")

# Initialize logging when this module is imported
setup_logging()

################################################################################


# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/utils.py
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/utils.py

import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

def check_answer_length(answer: str, max_length: int = 500) -> bool:
    """
    Checks if the answer length is within the specified limit.

    Args:
        answer (str): The generated answer to evaluate.
        max_length (int, optional): The maximum allowed length for the answer. Defaults to 500.

    Returns:
        bool: True if the answer length is within the limit, False otherwise.
    """
    return len(answer) <= max_length

def compute_similarity(query1: str, query2: str) -> float:
    """
    Computes cosine similarity between two queries using TF-IDF vectorization.

    Args:
        query1 (str): The first query string.
        query2 (str): The second query string.

    Returns:
        float: Cosine similarity score between query1 and query2.
    """
    try:
        vectorizer = TfidfVectorizer().fit_transform([query1, query2])
        vectors = vectorizer.toarray()
        similarity = cosine_similarity([vectors[0]], [vectors[1]])[0][0]
        return similarity
    except Exception as e:
        logger.error(f"Error computing similarity: {e}", exc_info=True)
        return 0.0

################################################################################
