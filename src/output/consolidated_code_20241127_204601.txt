# Consolidated Source Code
# ==============================================================================



################################################################################
# Module: root
################################################################################


# File: __init__.py
#------------------------------------------------------------------------------
"""
Thematic Analysis Package
A package for analyzing textual data using thematic analysis techniques.
"""

__version__ = '0.1.0'


################################################################################
# Module: analysis
################################################################################


# File: analysis/__init__.py
#------------------------------------------------------------------------------
#init
"""
Analysis modules for thematic analysis.
Contains metrics and quotation selection functionality.
"""

from .metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from .select_quotation import EnhancedQuotationSignature
from .select_quotation_module import SelectQuotationModule

__all__ = [
    'comprehensive_metric',
    'is_answer_fully_correct',
    'factuality_metric',
    'EnhancedQuotationSignature',
    'SelectQuotationModule'
]

# File: analysis/metrics.py
#------------------------------------------------------------------------------
#metrics.py
import logging
from typing import List, Dict, Any, Callable
import dspy

from src.utils.utils import check_answer_length
from src.utils.logger import setup_logging
# Initialize logger
logger = logging.getLogger(__name__)

class BaseAssessment(dspy.Signature):
    """
    Base class for all assessment signatures.
    """
    context: str = dspy.InputField(
        desc=(
            "The contextual information provided to generate the answer. This includes all relevant "
            "documents, data chunks, or information sources that the answer is based upon."
        )
    )
    question: str = dspy.InputField(
        desc=(
            "The original question that was posed. This is used to understand the intent and scope "
            "of the answer in relation to the provided context."
        )
    )
    answer: str = dspy.InputField(
        desc=(
            "The answer generated by the system that needs to be evaluated for factual correctness "
            "based on the provided context."
        )
    )

    def generate_prompt(self, context: str, question: str, answer: str, task: str) -> str:
        return (
            f"Context: {context}\n"
            f"Question: {question}\n"
            f"Answer: {answer}\n\n"
            f"{task}"
        )

class Assess(BaseAssessment):
    """
    Assess the factual correctness of an answer based on the provided context.

    This signature evaluates whether the generated answer accurately reflects the information
    present in the given context. It leverages a language model to perform a nuanced analysis
    beyond simple keyword matching, ensuring a thorough assessment of factual accuracy.
    """
    factually_correct: str = dspy.OutputField(
        desc=(
            "Indicator of whether the answer is factually correct based on the context. "
            "Should be 'Yes' if the answer accurately reflects the information in the context, "
            "and 'No' otherwise."
        )
    )

    def forward(self, context: str, question: str, answer: str) -> Dict[str, str]:
        try:
            logger.debug(f"Assessing factual correctness for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Based on the context provided, evaluate whether the answer is factually correct.\n"
                "Respond with 'Yes' if the answer accurately reflects the information in the context.\n"
                "Respond with 'No' if the answer contains factual inaccuracies or is not supported by the context.\n"
                "Please respond with 'Yes' or 'No' only."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=3,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Factuality assessment response: '{response}'")
            if response.lower() in ['yes', 'no']:
                result = response.capitalize()
            else:
                logger.warning(f"Unexpected response from factuality assessment: '{response}'. Defaulting to 'No'.")
                result = 'No'
            logger.info(f"Factuality assessment result: {result} for question: '{question}'")
            return {"factually_correct": result}
        except Exception as e:
            logger.error(f"Error in Assess.forward: {e}", exc_info=True)
            return {"factually_correct": "No"}


class AssessRelevance(BaseAssessment):
    """
    Assess the relevance of an answer to the given question and context.
    
    This signature evaluates whether the generated answer directly and comprehensively 
    addresses the user's query, considering the provided context.
    """
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug(f"Assessing relevance for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the relevance of the answer to the question based on the context.\n"
                "Provide a relevance score between 1 (not relevant) and 5 (highly relevant)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Relevance assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected relevance score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid relevance score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Relevance assessment result: {result} for question: '{question}'")
            return {"relevance_score": result}
        except Exception as e:
            logger.error(f"Error in AssessRelevance.forward: {e}", exc_info=True)
            return {"relevance_score": 1}


class AssessCoherence(BaseAssessment):
    """
    Assess the coherence of an answer.
    
    This signature evaluates whether the generated answer is well-structured and logically consistent.
    """
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing coherence of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the coherence of the above answer.\n"
                "Provide a coherence score between 1 (not coherent) and 5 (highly coherent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Coherence assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected coherence score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid coherence score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Coherence assessment result: {result}")
            return {"coherence_score": result}
        except Exception as e:
            logger.error(f"Error in AssessCoherence.forward: {e}", exc_info=True)
            return {"coherence_score": 1}


class AssessConciseness(BaseAssessment):
    """
    Assess the conciseness of an answer.
    
    This signature evaluates whether the generated answer is succinct and free from unnecessary verbosity.
    """
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing conciseness of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the conciseness of the above answer.\n"
                "Provide a conciseness score between 1 (not concise) and 5 (highly concise)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Conciseness assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected conciseness score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid conciseness score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Conciseness assessment result: {result}")
            return {"conciseness_score": result}
        except Exception as e:
            logger.error(f"Error in AssessConciseness.forward: {e}", exc_info=True)
            return {"conciseness_score": 1}


class AssessFluency(BaseAssessment):
    """
    Assess the fluency of an answer.
    
    This signature evaluates whether the generated answer exhibits natural language flow and is free from grammatical errors.
    """
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing fluency of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the fluency of the above answer.\n"
                "Provide a fluency score between 1 (not fluent) and 5 (highly fluent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Fluency assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected fluency score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid fluency score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Fluency assessment result: {result}")
            return {"fluency_score": result}
        except Exception as e:
            logger.error(f"Error in AssessFluency.forward: {e}", exc_info=True)
            return {"fluency_score": 1}


class ComprehensiveAssessment(BaseAssessment):
    """
    Comprehensive assessment of generated answers across multiple dimensions.
    """
    factually_correct: str = dspy.OutputField(desc="Whether the answer is factually correct ('Yes'/'No').")
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, Any]:
        try:
            logger.debug(f"Performing comprehensive assessment for question: '{question}'")
            
            # Assess factual correctness
            factuality = Assess()(context=context, question=question, answer=answer)['factually_correct']
            
            # Assess relevance
            relevance = AssessRelevance()(context=context, question=question, answer=answer)['relevance_score']
            
            # Assess coherence
            coherence = AssessCoherence()(context=context, question=question, answer=answer)['coherence_score']
            
            # Assess conciseness
            conciseness = AssessConciseness()(context=context, question=question, answer=answer)['conciseness_score']
            
            # Assess fluency
            fluency = AssessFluency()(context=context, question=question, answer=answer)['fluency_score']
            
            # Aggregate scores with adjusted weights
            composite_score = {
                "factually_correct": factuality,
                "relevance_score": relevance,
                "coherence_score": coherence,
                "conciseness_score": conciseness,
                "fluency_score": fluency
            }
            logger.info(f"Comprehensive assessment result: {composite_score}")
            return composite_score
        except Exception as e:
            logger.error(f"Error in ComprehensiveAssessment.forward: {e}", exc_info=True)
            return {
                "factually_correct": "No",
                "relevance_score": 1,
                "coherence_score": 1,
                "conciseness_score": 1,
                "fluency_score": 1
            }


def comprehensive_metric(example: Dict[str, Any], pred: Dict[str, Any], trace: Any = None) -> float:
    """
    Comprehensive metric to evaluate the answer across multiple dimensions.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
        trace (Any, optional): Trace information for optimization (unused here).
    
    Returns:
        float: A combined score representing the quality of the answer.
    """
    try:
        logger.debug(f"Evaluating comprehensive metrics for question: '{example.get('question', '')}'")
        assessment = comprehensive_assessment_module(
            context=example.get('context', ''),
            question=example.get('question', ''),
            answer=pred.get('answer', '')
        )
        logger.debug(f"Comprehensive assessment: {assessment}")
        
        # Convert 'factually_correct' to binary
        factually_correct = 1 if assessment.get("factually_correct") == "Yes" else 0
        
        # Normalize scores between 0 and 1
        relevance = assessment.get("relevance_score", 1) / 5
        coherence = assessment.get("coherence_score", 1) / 5
        conciseness = assessment.get("conciseness_score", 1) / 5
        fluency = assessment.get("fluency_score", 1) / 5
        
        # Define adjusted weights for each metric
        weights = {
            "factually_correct": 0.5,
            "relevance": 0.2,
            "coherence": 0.1,
            "conciseness": 0.1,
            "fluency": 0.1
        }
        
        # Calculate the composite score
        composite_score = (
            weights["factually_correct"] * factually_correct +
            weights["relevance"] * relevance +
            weights["coherence"] * coherence +
            weights["conciseness"] * conciseness +
            weights["fluency"] * fluency
        )
        
        logger.info(f"Comprehensive metric score: {composite_score}")
        return composite_score
    except Exception as e:
        logger.error(f"Error in comprehensive_metric: {e}", exc_info=True)
        return 0.0


def is_answer_fully_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    """
    Determines if the answer meets all quality metrics.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
    
    Returns:
        bool: True if all metrics meet the desired thresholds, False otherwise.
    """
    scores = comprehensive_metric(example, pred)
    # Define threshold (e.g., composite score should be at least 0.8)
    is_factual = scores >= 0.8
    logger.debug(f"Is answer fully correct (scores >= 0.8): {is_factual}")
    return is_factual


def factuality_metric(example: Dict[str, Any], pred: Dict[str, Any]) -> int:
    """
    Metric to evaluate factual correctness of the answer.

    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.

    Returns:
        int: 1 if factually correct, 0 otherwise.
    """
    try:
        assess = Assess()
        result = assess(context=example.get('context', ''), question=example.get('question', ''), answer=pred.get('answer', ''))
        factually_correct = result.get('factually_correct', 'No')
        logger.debug(f"Factuality metric result: {factually_correct}")
        return 1 if factually_correct == 'Yes' else 0
    except Exception as e:
        logger.error(f"Error in factuality_metric: {e}", exc_info=True)
        return 0 


# Initialize the assessment modules
try:
    # Use the unoptimized module directly without caching
    comprehensive_assessment_module = dspy.TypedChainOfThought(ComprehensiveAssessment)
    logger.info("Comprehensive Assessment DSPy module initialized successfully.")
except Exception as e:
    logger.error(f"Error initializing Comprehensive Assessment DSPy module: {e}", exc_info=True)
    raise


# File: analysis/select_keyword.py
#------------------------------------------------------------------------------
# src/analysis/select_keyword.py
import logging
from typing import List, Dict, Any
import dspy
import json

from src.assertions_keyword import (
    assert_keywords_extracted,
    assert_realness,
    assert_keywords_not_exclusive_to_context,
    assert_richness,
    assert_repetition,
    assert_rationale,
    assert_repartee,
    assert_regal
)

logger = logging.getLogger(__name__)

class KeywordExtractionSignature(dspy.Signature):
    """
    Signature for extracting keywords from individual quotations using the 6Rs framework.
    """
    research_objectives: str = dspy.InputField(
        desc="The research objectives that provide focus for conducting the analysis"
    )
    quotation: str = dspy.InputField(
        desc="The specific quotation from which to extract keywords"
    )
    contextual_info: List[str] = dspy.InputField(
        desc="List of contextualized content providing background for the quotation"
    )
    theoretical_framework: str = dspy.InputField(
        desc="The theoretical and philosophical framework guiding the analysis (optional)"
    )
    keywords: List[Dict[str, str]] = dspy.OutputField(
        desc="List of extracted keywords with their type and context"
    )

    def parse_response(self, response: str) -> List[Dict[str, str]]:
        """Parses the LM response into a list of keyword objects."""
        try:
            response_json = json.loads(response)
            keywords = response_json.get("keywords", [])
            validated_keywords = []
            for kw in keywords:
                if all(key in kw for key in ['keyword', 'type', 'context']):  # Changed from 'keywords_to_use_coding'
                    validated_keywords.append({
                        "keyword": kw['keyword'],  # Changed field name
                        "type": kw['type'],
                        "context": kw['context']
                    })
                else:
                    logger.warning(f"Keyword entry missing fields: {kw}")
            return validated_keywords
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}")
            return []
        except Exception as e:
            logger.error(f"Error parsing response: {e}")
            return []

    def create_prompt(self, research_objectives: str, quotation: str, contextual_info: List[str], theoretical_framework: str = None) -> str:
        """Creates the prompt for the language model."""
        contextual_text = "\n".join(contextual_info) if contextual_info else ""
        theoretical_part = f"Theoretical Framework:\n{theoretical_framework}\n\n" if theoretical_framework else ""
        prompt = (
            f"You are conducting a thematic analysis using keyword extraction based on the 6Rs framework.\n\n"
            f"Research Objectives:\n{research_objectives}\n\n"
            f"{theoretical_part}"
            f"Quotation:\n{quotation}\n\n"
            f"Contextual Information:\n{contextual_text}\n\n"
            f"Task:\n"
            f"**Extract meaningful keywords exclusively from the Quotation provided above using the 6Rs framework.**\n"
            f"Use the Contextual Information only to better understand the Quotation.\n\n"
            f"Ensure that the keywords satisfy the following criteria:\n"
            f"1. Realness: Keywords that reflect the genuine experiences and perceptions of the participants.\n"
            f"2. Richness: Keywords that are rich in meaning and provide a detailed understanding of the phenomenon.\n"
            f"3. Repetition: Keywords that frequently occur in the data, indicating their significance.\n"
            f"4. Rationale: Keywords connected to the theoretical or philosophical foundation of the research.\n"
            f"5. Repartee: Keywords that are insightful, evocative, and stimulate further discussion.\n"
            f"6. Regal: Keywords that are central to understanding the phenomenon and contribute significantly to the literature.\n\n"
            f"Return the list of extracted keywords with their types and contexts in valid JSON format:\n"
            f"{{\n"
            f"  \"keywords\": [\n"
            f"    {{\n"
            f"      \"keyword\": \"string\",\n"  # Changed field name
            f"      \"type\": \"string\",\n"
            f"      \"context\": \"string\"\n"
            f"    }}\n"
            f"  ]\n"
            f"}}\n"
        )
        return prompt

    def forward(self, research_objectives: str, quotation: str, contextual_info: List[str], theoretical_framework: str = None) -> Dict[str, Any]:
        try:
            logger.debug("Starting keyword extraction process for individual quotation.")

            # Generate the prompt
            prompt = self.create_prompt(research_objectives, quotation, contextual_info, theoretical_framework)

            # Generate response
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=500,
                temperature=1.0
            ).strip()

            # Parse response
            keywords = self.parse_response(response)

            # Extract keyword texts for assertions
            extracted_keyword_texts = [kw['keywords_to_use_coding'] for kw in keywords]

            # Apply assertions
            assert_keywords_extracted(extracted_keyword_texts)
            assert_realness(extracted_keyword_texts, quotation)
            assert_keywords_not_exclusive_to_context(extracted_keyword_texts, quotation, contextual_info)
            assert_richness(extracted_keyword_texts)
            assert_repetition(extracted_keyword_texts, quotation)
            assert_rationale(extracted_keyword_texts, theoretical_framework)
            assert_repartee(extracted_keyword_texts)
            assert_regal(extracted_keyword_texts, research_objectives)

            logger.info(f"Successfully extracted {len(keywords)} keywords.")

            return {
                "keywords": keywords
            }

        except AssertionError as af:
            logger.warning(f"Assertion failed during keyword extraction: {af}")
            return self.handle_failed_assertion(af, research_objectives, quotation, contextual_info, theoretical_framework)
        except Exception as e:
            logger.error(f"Error in KeywordExtractionSignature.forward: {e}", exc_info=True)
            return {
                "keywords": [],
                "error": f"Error occurred during keyword extraction: {str(e)}"
            }

    def handle_failed_assertion(self, assertion_failure: AssertionError,
                                research_objectives: str, quotation: str,
                                contextual_info: List[str],
                                theoretical_framework: str = None) -> Dict[str, Any]:
        """Handles failed assertions by attempting to generate improved keywords."""
        try:
            contextual_text = "\n".join(contextual_info) if contextual_info else ""
            theoretical_part = f"Theoretical Framework:\n{theoretical_framework}\n\n" if theoretical_framework else ""
            focused_prompt = (
                f"The previous attempt failed because: {assertion_failure}\n\n"
                f"Research Objectives:\n{research_objectives}\n\n"
                f"{theoretical_part}"
                f"Quotation:\n{quotation}\n\n"
                f"Contextual Information:\n{contextual_text}\n\n"
                "Please ensure the following in your next attempt:\n"
                "Extract meaningful keywords **only** from the quotation using the 6Rs framework, utilizing the contextual information to better understand the quotation.\n"
                "Make sure the keywords satisfy the following criteria:\n"
                f"- Realness: Keywords must be present in the quotation and reflect participants' experiences.\n"
                f"- Richness: Keywords should be meaningful and provide detailed understanding.\n"
                f"- Repetition: Keywords should occur frequently in the data.\n"
                f"- Rationale: Keywords should be connected to the theoretical framework.\n"
                f"- Repartee: Keywords should be insightful and evocative.\n"
                f"- Regal: Keywords should be central to understanding the phenomenon.\n\n"
                f"Quotation:\n{quotation}"
            )

            response = self.language_model.generate(
                prompt=focused_prompt,
                max_tokens=500,
                temperature=0.5
            ).strip()

            keywords = self.parse_response(response)

            # Re-apply assertions
            extracted_keywords = []
            for kw in keywords:
                keyword = kw.get('keywords_to_use_coding', '').strip()
                kw_type = kw.get('type', '').strip()
                kw_context = kw.get('context', '').strip()
                if keyword:
                    extracted_keywords.append({
                        "keywords_to_use_coding": keyword,
                        "type": kw_type,
                        "context": kw_context
                    })

            # Extract keyword texts for assertions
            extracted_keyword_texts = [kw['keywords_to_use_coding'] for kw in extracted_keywords]

            assert_keywords_extracted(extracted_keyword_texts)
            assert_realness(extracted_keyword_texts, quotation)
            assert_keywords_not_exclusive_to_context(extracted_keyword_texts, quotation, contextual_info)
            assert_richness(extracted_keyword_texts)
            assert_repetition(extracted_keyword_texts, quotation)
            assert_rationale(extracted_keyword_texts, theoretical_framework)
            assert_repartee(extracted_keyword_texts)
            assert_regal(extracted_keyword_texts, research_objectives)

            logger.info("Keywords successfully extracted after refinement.")
            return {
                "keywords": extracted_keywords,
                "note": f"Keywords were refined to address: {assertion_failure}"
            }

        except AssertionError as af_inner:
            logger.error(f"Refined keywords still failed assertions: {af_inner}")
            return {
                "keywords": [],
                "error": f"Failed to extract appropriate keywords after refinement: {str(af_inner)}"
            }
        except Exception as e:
            logger.error(f"Error in handle_failed_assertion: {e}", exc_info=True)
            return {
                "keywords": [],
                "error": f"Failed to extract appropriate keywords: {str(assertion_failure)}"
            }


# File: analysis/select_keyword_module.py
#------------------------------------------------------------------------------
# src/analysis/select_keyword_module.py
import logging
from typing import Dict, Any, List
import dspy

from src.analysis.select_keyword import KeywordExtractionSignature

logger = logging.getLogger(__name__)

class SelectKeywordModule(dspy.Module):
    """
    DSPy module to extract keywords from individual quotations using the 6Rs framework.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(KeywordExtractionSignature)

    def forward(self, research_objectives: str, quotation: str, contextual_info: List[str], theoretical_framework: str = None) -> Dict[str, Any]:
        try:
            logger.debug(f"Running keyword extraction for quotation: {quotation[:100]}...")
            logger.debug(f"Research objectives: {research_objectives[:100]}...")
            logger.debug(f"Context info count: {len(contextual_info)}")
            
            response = self.chain(
                research_objectives=research_objectives,
                quotation=quotation,
                contextual_info=contextual_info,
                theoretical_framework=theoretical_framework
            )
            
            keywords = response.get("keywords", [])
            if not keywords:
                logger.warning("No keywords returned from chain")
            else:
                logger.info(f"Successfully extracted {len(keywords)} keywords")
                
            return {
                "keywords": keywords
            }
        except Exception as e:
            logger.error(f"Error in SelectKeywordModule.forward: {e}", exc_info=True)
            return {
                "keywords": [],
                "error": f"Error occurred during keyword extraction: {str(e)}"
            }

# File: analysis/select_quotation.py
#------------------------------------------------------------------------------
# analysis/select_quotation.py
import logging
from typing import List, Dict, Any
import dspy
import json
import re
from src.assertions import (
    assert_relevant_quotations,
    assert_confidentiality,
    assert_diversity_of_quotations,
    assert_contextual_adequacy,
    assert_philosophical_alignment,
    assert_patterns_identified,
    assert_theoretical_interpretation,
    assert_research_alignment
)

logger = logging.getLogger(__name__)

class EnhancedQuotationSignature(dspy.Signature):
    """
    Enhanced signature for selecting and analyzing quotations using Braun and Clarke's thematic analysis.
    """
    research_objectives: str = dspy.InputField(
        desc="The research objectives that provide focus for conducting the analysis"
    )
    transcript_chunk: str = dspy.InputField(
        desc="The main transcript chunk being analyzed"
    )
    contextualized_contents: List[str] = dspy.InputField(
        desc="List of related content providing additional context"
    )
    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc="The theoretical and philosophical framework guiding the analysis"
    )
    transcript_info: Dict[str, Any] = dspy.OutputField(
        desc="Information about the transcript and analysis context"
    )
    quotations: List[Dict[str, Any]] = dspy.OutputField(
        desc="List of selected quotations with their classifications and analyses"
    )
    creswell_category: str = dspy.OutputField(
        desc="Creswell's classification category for the quotation"
    )
    analysis: Dict[str, Any] = dspy.OutputField(
        desc="Comprehensive analysis including patterns and theoretical interpretation"
    )
    answer: Dict[str, Any] = dspy.OutputField(
        desc="Summary of findings and contributions"
    )

    def create_prompt(self, research_objectives: str, transcript_chunk: str, 
                      contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> str:
        """Creates the prompt for the language model."""
        
        # Format contextualized contents
        chunks_formatted = "\n\n".join([
            f"Content {i+1}:\n{content}" 
            for i, content in enumerate([transcript_chunk] + contextualized_contents)
        ])

        theory = theoretical_framework.get("theory", "")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "")
        rationale = theoretical_framework.get("rationale", "")

        prompt = (
            f"You are an experienced qualitative researcher conducting a thematic analysis of "
            f"interview transcripts using Braun and Clarke's (2006) approach. Your task is to "
            f"analyze the provided transcript chunks while adhering to key principles from their "
            f"thematic analysis methodology.\n\n"

            f"First, review the transcript chunks and contextualized_content:\n\n"
            f"{chunks_formatted}\n\n"
            
            f"Research Objectives:\n"
            f"<research_objectives>\n"
            f"{research_objectives}\n"
            f"</research_objectives>\n\n"

            f"Theoretical Framework:\n"
            f"<theoretical_framework>\n"
            f"Theory: {theory}\n"
            f"Philosophical Approach: {philosophical_approach}\n"
            f"Rationale: {rationale}\n"
            f"</theoretical_framework>\n\n"
                    
            f"Your analysis should follow these steps:\n\n"

            f"1. **Quotation Selection**:\n"
            f"   - Select quotes that demonstrate robust patterns in the data.\n"
            f"   - Classify quotes using Creswell's categories:\n"
            f"     a) Longer quotations: For complex understandings\n"
            f"     b) Discrete quotations: For diverse perspectives\n"
            f"     c) Embedded quotations: Brief phrases showing text shifts\n"
            f"   - Ensure quotes enhance reader engagement and highlight unique findings.\n"
            f"   - Provide adequate context for accurate comprehension.\n\n"

            f"2. **Pattern Recognition**:\n"
            f"   - Identify patterns emerging from data rather than predetermined categories.\n"
            f"   - Support patterns with multiple quotations.\n"
            f"   - Maintain theoretical alignment while remaining open to emerging themes.\n"
            f"   - Document methodological decisions transparently.\n\n"

            f"3. **Theoretical Integration**:\n"
            f"   - Demonstrate clear philosophical underpinning.\n"
            f"   - Show how findings connect to the theoretical framework.\n"
            f"   - Practice researcher reflexivity throughout analysis.\n"
            f"   - Balance selectivity with comprehensiveness.\n\n"

            f"For each step of your analysis, wrap your analysis process in <analysis_process> tags to explain your thought process and reasoning before providing the final output. It's OK for this section to be quite long.\n\n"

            f"Your final output should follow this JSON structure:\n\n"

            "{\n"
            "  \"transcript_info\": {\n"
            "    \"transcript_chunk\": \"\",                    // Selected transcript content\n"
            "    \"research_objectives\": \"\",                 // Research goals guiding analysis\n"
            "    \"theoretical_framework\": {\n"
            "      \"theory\": \"\",                            // Primary theoretical approach\n"
            "      \"philosophical_approach\": \"\",            // Philosophical foundation\n"
            "      \"rationale\": \"\"                          // Justification for approach\n"
            "    }\n"
            "  },\n"
            "  \"retrieved_chunks\": [],                        // List of retrieved chunks (if needed)\n"
            "  \"retrieved_chunks_count\": 0,                   // Count of retrieved chunks\n"
            "  \"contextualized_contents\": [],                 // List of contextualized contents\n"
            "  \"used_chunk_ids\": [],                          // List of used chunk IDs\n"
            "  \"quotations\": [\n"
            "    {\n"
            "      \"quotation\": \"\",                         // Exact quote text\n"
            "      \"creswell_category\": \"\",                 // longer/discrete/embedded\n"
            "      \"classification\": \"\",                    // Content type\n"
            "      \"context\": {\n"
            "        \"preceding_question\": \"\",              // Prior question\n"
            "        \"situation\": \"\",                       // Context description\n"
            "        \"pattern_representation\": \"\"           // Pattern linkage\n"
            "      },\n"
            "      \"analysis_value\": {\n"
            "        \"relevance\": \"\",                       // Research objective alignment\n"
            "        \"pattern_support\": \"\",                 // Pattern evidence\n"
            "        \"theoretical_alignment\": \"\"            // Framework connection\n"
            "      }\n"
            "    }\n"
            "  ],\n"
            "  \"analysis\": {\n"
            "    \"philosophical_underpinning\": \"\",          // Analysis approach\n"
            "    \"patterns_identified\": [\"\"],               // Key patterns found\n"
            "    \"theoretical_interpretation\": \"\",          // Framework application\n"
            "    \"methodological_reflection\": {\n"
            "      \"pattern_robustness\": \"\",                // Pattern evidence\n"
            "      \"theoretical_alignment\": \"\",             // Framework fit\n"
            "      \"researcher_reflexivity\": \"\"             // Interpretation awareness\n"
            "    },\n"
            "    \"practical_implications\": \"\"               // Applied insights\n"
            "  },\n"
            "  \"answer\": {\n"
            "    \"summary\": \"\",                            // Key findings\n"
            "    \"theoretical_contribution\": \"\",            // Theory advancement\n"
            "    \"methodological_contribution\": {\n"
            "      \"approach\": \"\",                         // Method used\n"
            "      \"pattern_validity\": \"\",                 // Evidence quality\n"
            "      \"theoretical_integration\": \"\"           // Theory-data synthesis\n"
            "    }\n"
            "  }\n"
            "}\n\n"

            f"**Important Instructions:**\n"
            f"- **Your final output must strictly follow the JSON structure provided, including all fields exactly as specified. All fields must be filled with appropriate content based on the analysis. Do not leave any fields empty or use placeholders like 'N/A'.**\n"
            f"- **Use double quotes for all strings.**\n"
            f"- **Do not include any additional commentary or text outside of the JSON structure.**\n\n"
            
            f"**If you do not have information for a specific field, please provide a brief explanation or a relevant placeholder that accurately reflects the absence of data, rather than leaving it empty.**\n\n"
            
            f"Remember to wrap your analysis process in <analysis_process> tags throughout your analysis to show your chain of thought before providing the final JSON output.\n\n"

            )
        return prompt

    def get_default_structure(self) -> Dict[str, Any]:
        """Provides the complete default structure."""
        return {
            "transcript_info": {
                "transcript_chunk": "N/A",
                "research_objectives": "N/A",
                "theoretical_framework": {
                    "theory": "N/A",
                    "philosophical_approach": "N/A",
                    "rationale": "N/A"
                }
            },
            "retrieved_chunks": [],
            "retrieved_chunks_count": 0,
            "filtered_chunks_count": 0,
            "contextualized_contents": [],
            "used_chunk_ids": [],
            "quotations": [
                {
                    "quotation": "N/A",
                    "creswell_category": "N/A",
                    "classification": "N/A",
                    "context": {
                        "preceding_question": "N/A",
                        "situation": "N/A",
                        "pattern_representation": "N/A"
                    },
                    "analysis_value": {
                        "relevance": "N/A",
                        "pattern_support": "N/A",
                        "theoretical_alignment": "N/A"
                    }
                }
            ],
            "analysis": {
                "philosophical_underpinning": "N/A",
                "patterns_identified": ["N/A"],
                "theoretical_interpretation": "N/A",
                "methodological_reflection": {
                    "pattern_robustness": "N/A",
                    "theoretical_alignment": "N/A",
                    "researcher_reflexivity": "N/A"
                },
                "practical_implications": "N/A"
            },
            "answer": {
                "summary": "N/A",
                "theoretical_contribution": "N/A",
                "methodological_contribution": {
                    "approach": "N/A",
                    "pattern_validity": "N/A",
                    "theoretical_integration": "N/A"
                }
            }
        }

    def get_quotation_structure(self) -> Dict[str, Any]:
        """Provides the structure for individual quotations."""
        return {
            "quotation": "N/A",
            "creswell_category": "N/A",
            "classification": "N/A",
            "context": {
                "preceding_question": "N/A",
                "situation": "N/A",
                "pattern_representation": "N/A"
            },
            "analysis_value": {
                "relevance": "N/A",
                "pattern_support": "N/A",
                "theoretical_alignment": "N/A"
            }
        }

    def merge_structures(self, source: Any, default: Any) -> Any:
        """Recursively merges two structures, giving preference to source."""
        if isinstance(default, dict):
            result = {}
            for key in default:
                if key in source and source[key] is not None:
                    result[key] = self.merge_structures(source[key], default[key])
                else:
                    result[key] = default[key]
            return result
        elif isinstance(default, list):
            if isinstance(source, list) and source:
                return [self.merge_structures(item, default[0]) if isinstance(default[0], dict) else item for item in source]
            else:
                return default
        else:
            return source if source else "N/A"

    def replace_empty_strings(self, data: Any) -> Any:
        """Recursively ensure all fields are filled with 'N/A' if they are empty."""
        if isinstance(data, dict):
            return {k: self.replace_empty_strings(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self.replace_empty_strings(item) for item in data]
        elif isinstance(data, str):
            return data if data.strip() else "N/A"
        else:
            return data

    def parse_response(self, response: str) -> Dict[str, Any]:
        """Parses the response and ensures it matches the desired JSON structure."""
        try:
            # Attempt to extract JSON from code block tagged as json
            json_match = re.search(r"```json\s*(\{.*?\})\s*```", response, re.DOTALL)
            if json_match:
                json_string = json_match.group(1)
            else:
                # Try to find any JSON in the response
                json_match = re.search(r"(\{.*\})", response, re.DOTALL)
                if json_match:
                    json_string = json_match.group(1)
                else:
                    logger.error("No valid JSON found in response.")
                    return self.get_default_structure()
            # Parse JSON
            response_json = json.loads(json_string)
            # Ensure structure matches default structure
            default_structure = self.get_default_structure()
            response_json = self.merge_structures(response_json, default_structure)
            # Ensure all quotations have the required structure
            if "quotations" in response_json:
                quotation_structure = self.get_quotation_structure()
                response_json["quotations"] = [
                    self.merge_structures(quote, quotation_structure)
                    for quote in response_json["quotations"]
                ]
            else:
                response_json["quotations"] = [self.get_quotation_structure()]
            # Replace empty strings with "N/A"
            response_json = self.replace_empty_strings(response_json)
            return response_json
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}")
            return self.get_default_structure()
        except Exception as e:
            logger.error(f"Error parsing response: {e}")
            return self.get_default_structure()

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Starting enhanced quotation selection and analysis process.")
            
            # Generate the prompt
            prompt = self.create_prompt(
                research_objectives,
                transcript_chunk,
                contextualized_contents,
                theoretical_framework
            )
            
            # Generate response
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=6000,
                temperature=0.5
            ).strip()
            
            # Parse the complete response
            parsed_response = self.parse_response(response)
            
            # Extract components
            quotations = parsed_response.get("quotations", [])
            analysis = parsed_response.get("analysis", {})
            
            # Apply assertions
            assert_relevant_quotations(quotations, research_objectives)
            assert_confidentiality(quotations, sensitive_keywords=['confidential', 'secret'])
            assert_diversity_of_quotations(quotations, min_participants=3)
            assert_contextual_adequacy(quotations, [transcript_chunk] + contextualized_contents)
            assert_philosophical_alignment(quotations, theoretical_framework)
            
            # Additional assertions for theoretical analysis
            patterns = analysis.get("patterns_identified", [])
            theoretical_interpretation = analysis.get("theoretical_interpretation", "")
            research_alignment = analysis.get("methodological_reflection", {}).get("theoretical_alignment", "")
            
            assert_patterns_identified(patterns)
            assert_theoretical_interpretation(theoretical_interpretation)
            assert_research_alignment(research_alignment)
            
            logger.info(f"Successfully completed analysis with {len(quotations)} quotations.")
            return parsed_response

        except AssertionError as af:
            logger.warning(f"Assertion failed during analysis: {af}")
            return self.handle_failed_assertion(af, research_objectives, transcript_chunk, 
                                                 contextualized_contents, theoretical_framework)
        except Exception as e:
            logger.error(f"Error in EnhancedQuotationSignature.forward: {e}", exc_info=True)
            return self.get_default_structure()

    def handle_failed_assertion(self, assertion_failure: AssertionError,
                                research_objectives: str, transcript_chunk: str,
                                contextualized_contents: List[str],
                                theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """Handles failed assertions by attempting to generate improved analysis."""
        try:
            focused_prompt = self.create_prompt(
                research_objectives,
                transcript_chunk,
                contextualized_contents,
                theoretical_framework
            )
            
            focused_prompt += (
                f"\n\nThe previous attempt failed because: {assertion_failure}\n"
                f"Please ensure that your analysis addresses this specific issue while maintaining "
                f"all other requirements for thorough theoretical analysis."
            )

            response = self.language_model.generate(
                prompt=focused_prompt,
                max_tokens=2000,
                temperature=0.5
            ).strip()

            parsed_response = self.parse_response(response)
            
            # Re-apply all assertions
            quotations = parsed_response.get("quotations", [])
            analysis = parsed_response.get("analysis", {})
            
            assert_relevant_quotations(quotations, research_objectives)
            assert_confidentiality(quotations, sensitive_keywords=['confidential', 'secret'])
            assert_diversity_of_quotations(quotations, min_participants=3)
            assert_contextual_adequacy(quotations, [transcript_chunk] + contextualized_contents)
            assert_philosophical_alignment(quotations, theoretical_framework)
            
            patterns = analysis.get("patterns_identified", [])
            theoretical_interpretation = analysis.get("theoretical_interpretation", "")
            research_alignment = analysis.get("methodological_reflection", {}).get("theoretical_alignment", "")
            
            assert_patterns_identified(patterns)
            assert_theoretical_interpretation(theoretical_interpretation)
            assert_research_alignment(research_alignment)

            return parsed_response

        except AssertionError as af_inner:
            logger.error(f"Refined analysis still failed assertions: {af_inner}")
            return self.get_default_structure()
        except Exception as e:
            logger.error(f"Error in handle_failed_assertion: {e}", exc_info=True)
            return self.get_default_structure()

class EnhancedQuotationModule(dspy.Module):
    """
    DSPy module implementing the enhanced quotation selection and theoretical analysis functionality.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(EnhancedQuotationSignature)

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Running EnhancedQuotationModule with integrated theoretical analysis.")
            response = self.chain(
                research_objectives=research_objectives,
                transcript_chunk=transcript_chunk,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            return response
        except Exception as e:
            logger.error(f"Error in EnhancedQuotationModule.forward: {e}", exc_info=True)
            return {}


# File: analysis/select_quotation_alt.py
#------------------------------------------------------------------------------
# File: src/analysis/select_quotation_alt.py
import logging
from typing import List, Dict, Any
import dspy
import json

from src.assertions_alt import (
    assert_relevant_quotations,
    assert_confidentiality,
    assert_diversity_of_quotations,
    assert_contextual_adequacy,
    assert_philosophical_alignment
)

logger = logging.getLogger(__name__)

class EnhancedQuotationSignatureAlt(dspy.Signature):
    """
    Enhanced signature for selecting relevant quotations based on an alternative thematic analysis approach.
    """
    research_objectives: str = dspy.InputField(
        desc="The research objectives that provide focus for conducting the analysis"
    )
    transcript_chunks: List[str] = dspy.InputField(
        desc="Chunks of transcript from which quotations are to be selected"
    )
    theoretical_framework: str = dspy.InputField(
        desc="The theoretical and philosophical framework guiding the analysis"
    )
    quotations: List[Dict[str, Any]] = dspy.OutputField(
        desc="List of selected quotations with their types and analytical context"
    )
    analysis: str = dspy.OutputField(
        desc="Analysis of how the selected quotations support the research objectives"
    )

    def parse_quotations(self, response: str) -> List[Dict[str, Any]]:
        """Parses quotations from the LM response."""
        try:
            # Assuming the LM returns a JSON-formatted string for easy parsing
            response_json = json.loads(response)
            quotation_list = response_json.get("quotations", [])
            return quotation_list
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}")
            return []
        except Exception as e:
            logger.error(f"Error parsing quotations: {e}")
            return []

    def extract_analysis(self, response: str) -> str:
        """Extracts analysis section from the LM response."""
        try:
            # Assuming the LM returns a JSON-formatted string with an 'analysis' field
            response_json = json.loads(response)
            analysis = response_json.get("analysis", "")
            return analysis
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}")
            return ""
        except Exception as e:
            logger.error(f"Error extracting analysis: {e}")
            return ""

    def create_prompt(self, research_objectives: str, transcript_chunks: List[str], theoretical_framework: str) -> str:
        """Creates the alternative prompt for the language model."""
        chunks_formatted = "\n".join([f"Chunk {i+1}: {chunk}" for i, chunk in enumerate(transcript_chunks)])
        prompt = (
            f"Alternative Thematic Analysis Prompt:\n\n"
            f"Research Objectives:\n{research_objectives}\n\n"
            f"Theoretical Framework:\n{theoretical_framework}\n\n"
            f"Transcript Chunks:\n{chunks_formatted}\n\n"
            f"Task: [Your alternative prompt instructions here]\n\n"
            f"Return the analysis in valid JSON format."
        )
        return prompt

    def forward(self, research_objectives: str, transcript_chunks: List[str], theoretical_framework: str) -> Dict[str, Any]:
        try:
            logger.debug("Starting enhanced quotation selection process with theoretical framework (Alt).")
            
            # Generate the prompt
            prompt = self.create_prompt(research_objectives, transcript_chunks, theoretical_framework)
            
            # Generate response
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1500,
                temperature=1.0
            ).strip()
            
            # Parse quotations and analysis
            quotation_list = self.parse_quotations(response)
            analysis = self.extract_analysis(response)
            
            # Apply assertions
            assert_relevant_quotations(quotation_list, self.research_objectives)
            assert_confidentiality(quotation_list, sensitive_keywords=['confidential', 'secret'])
            assert_diversity_of_quotations(quotation_list, min_participants=3)
            assert_contextual_adequacy(quotation_list, self.transcript_chunks)
            assert_philosophical_alignment(quotation_list, self.theoretical_framework)
            
            logger.info(f"Successfully selected {len(quotation_list)} quotations aligned with theoretical framework (Alt).")
            return {
                "quotations": quotation_list,
                "analysis": analysis
            }

        except AssertionError as af:
            logger.warning(f"Assertion failed during quotation selection (Alt): {af}")
            return self.handle_failed_assertion(af, research_objectives, transcript_chunks, theoretical_framework)
        except Exception as e:
            logger.error(f"Error in EnhancedQuotationSignatureAlt.forward: {e}", exc_info=True)
            return {
                "quotations": [],
                "analysis": f"Error occurred during quotation selection (Alt): {str(e)}"
            }

    def handle_failed_assertion(self, assertion_failure: AssertionError,
                               research_objectives: str, transcript_chunks: List[str],
                               theoretical_framework: str) -> Dict[str, Any]:
        """Handles failed assertions by attempting to generate improved quotations."""
        try:
            focused_prompt = (
                f"The previous attempt failed because: {assertion_failure}\n\n"
                f"Research Objectives:\n{research_objectives}\n\n"
                f"Theoretical Framework:\n{theoretical_framework}\n\n"
                "Please select quotations that specifically address this issue by ensuring:\n"
                "1. Proper quotation type (Discrete/Embedded/Longer)\n"
                "2. Clear function and purpose\n"
                "3. Sufficient context and ethical considerations\n"
                "4. Strong theoretical alignment\n"
                "5. Diverse participant representation\n\n"
                "Transcript Chunks:\n" +
                "\n".join([f"Chunk {i+1}: {chunk}" for i, chunk in enumerate(transcript_chunks)])
            )

            response = self.language_model.generate(
                prompt=focused_prompt,
                max_tokens=1500,
                temperature=0.5
            ).strip()

            quotation_list = self.parse_quotations(response)
            analysis = self.extract_analysis(response)

            # Re-apply assertions on the new quotations
            assert_relevant_quotations(quotation_list, research_objectives)
            assert_confidentiality(quotation_list, sensitive_keywords=['confidential', 'secret'])
            assert_diversity_of_quotations(quotation_list, min_participants=3)
            assert_contextual_adequacy(quotation_list, transcript_chunks)
            assert_philosophical_alignment(quotation_list, theoretical_framework)

            return {
                "quotations": quotation_list,
                "analysis": f"Note: Quotations were refined to address: {assertion_failure}\n\n{analysis}"
            }

        except AssertionError as af_inner:
            logger.error(f"Refined quotations still failed assertions (Alt): {af_inner}")
            return {
                "quotations": [],
                "analysis": f"Failed to select appropriate quotations after refinement (Alt): {str(af_inner)}"
            }
        except Exception as e:
            logger.error(f"Error in handle_failed_assertion (Alt): {e}", exc_info=True)
            return {
                "quotations": [],
                "analysis": f"Failed to select appropriate quotations (Alt): {str(assertion_failure)}"
            }

class EnhancedQuotationModuleAlt(dspy.Module):
    """
    DSPy module implementing the enhanced quotation selection functionality with an alternative prompt.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(EnhancedQuotationSignatureAlt)

    def forward(self, research_objectives: str, transcript_chunks: List[str], theoretical_framework: str) -> Dict[str, Any]:
        try:
            logger.debug("Running EnhancedQuotationModuleAlt with theoretical framework.")
            response = self.chain(
                research_objectives=research_objectives,
                transcript_chunks=transcript_chunks,
                theoretical_framework=theoretical_framework
            )
            quotations = response.get("quotations", [])
            analysis = response.get("analysis", "")
            logger.info(f"Selected {len(quotations)} quotations aligned with theoretical framework (Alt).")
            return {
                "quotations": quotations,
                "analysis": analysis
            }
        except Exception as e:
            logger.error(f"Error in EnhancedQuotationModuleAlt.forward: {e}", exc_info=True)
            return {
                "quotations": [],
                "analysis": ""
            }


# File: analysis/select_quotation_module.py
#------------------------------------------------------------------------------
# analysis/select_quotation_module.py
import logging
from typing import Dict, Any, List
import dspy

from src.analysis.select_quotation import EnhancedQuotationModule

logger = logging.getLogger(__name__)

class SelectQuotationModule(dspy.Module):
    """
    DSPy module to select and analyze quotations based on research objectives,
    transcript chunks, and theoretical framework.
    """
    def __init__(self):
        super().__init__()
        self.enhanced_module = EnhancedQuotationModule()

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Running SelectQuotationModule with integrated theoretical analysis.")
            response = self.enhanced_module.forward(
                research_objectives=research_objectives,
                transcript_chunk=transcript_chunk,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            # The response already includes 'transcript_info', 'quotations', 'analysis', and 'answer'
            return response
        except Exception as e:
            logger.error(f"Error in SelectQuotationModule.forward: {e}", exc_info=True)
            return {}


# File: analysis/select_quotation_module_alt.py
#------------------------------------------------------------------------------
# src/analysis/select_quotation_module_alt.py
import logging
from typing import Dict, Any, List
import dspy

from src.analysis.select_quotation_alt import EnhancedQuotationModuleAlt
from src.assertions_alt import (  # Import alternative assertions
    assert_relevant_quotations,
    assert_confidentiality,
    assert_diversity_of_quotations,
    assert_contextual_adequacy,
    assert_philosophical_alignment
)

logger = logging.getLogger(__name__)

class SelectQuotationModuleAlt(dspy.Module):
    """
    DSPy module to select quotations based on research objectives, transcript chunks, and theoretical framework.
    Utilizes EnhancedQuotationModuleAlt for robust assertion-based selection with an alternative prompt.
    """
    def __init__(self):
        super().__init__()
        self.enhanced_module = EnhancedQuotationModuleAlt()

    def forward(self, research_objectives: str, transcript_chunks: List[str], theoretical_framework: str) -> Dict[str, Any]:
        try:
            logger.debug("Running SelectQuotationModuleAlt with theoretical framework.")
            response = self.enhanced_module.forward(
                research_objectives=research_objectives,
                transcript_chunks=transcript_chunks,
                theoretical_framework=theoretical_framework
            )
            quotations = response.get("quotations", [])
            analysis = response.get("analysis", "")
            logger.info(f"Selected {len(quotations)} quotations aligned with theoretical framework (Alt).")
            return {
                "quotations": quotations,
                "analysis": analysis
            }
        except Exception as e:
            logger.error(f"Error in SelectQuotationModuleAlt.forward: {e}", exc_info=True)
            return {
                "quotations": [],
                "analysis": ""
            }



################################################################################
# Module: root
################################################################################


# File: assertions.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def assert_relevant_quotations(quotations: List[Dict[str, Any]], research_objectives: str) -> None:
    """
    Ensure that each quotation is relevant to the research objectives.

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        research_objectives (str): The research objectives guiding the analysis.

    Raises:
        AssertionError: If a quotation does not align with the research objectives.
    """
    for quote in quotations:
        if research_objectives.lower() not in quote.get('analysis_value', {}).get('relevance', '').lower():
            error_msg = f"Quotation '{quote.get('quotation', '')[:50]}...' does not align with research objectives."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_confidentiality(quotations: List[Dict[str, Any]], sensitive_keywords: List[str]) -> None:
    """
    Ensure that no quotations contain sensitive or identifiable information.

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        sensitive_keywords (List[str]): List of keywords that should not appear in quotations.

    Raises:
        AssertionError: If a quotation contains sensitive information.
    """
    for quote in quotations:
        quote_text = quote.get("quotation", "").lower()
        for keyword in sensitive_keywords:
            if keyword.lower() in quote_text:
                error_msg = f"Quotation '{quote.get('quotation', '')[:50]}...' contains sensitive keyword '{keyword}'."
                logger.error(error_msg)
                raise AssertionError(error_msg)

def assert_diversity_of_quotations(quotations: List[Dict[str, Any]], min_participants: int = 3) -> None:
    """
    Ensure that quotations represent a diverse set of participants.

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        min_participants (int): Minimum number of different participants required.

    Raises:
        AssertionError: If quotations do not represent the required diversity.
    """
    participant_ids = {q.get("participant_id") for q in quotations if q.get("participant_id")}
    if len(participant_ids) < min_participants:
        error_msg = f"Only {len(participant_ids)} unique participants are represented in quotations; minimum required is {min_participants}."
        logger.error(error_msg)
        raise AssertionError(error_msg)

def assert_contextual_adequacy(quotations: List[Dict[str, Any]], transcript_chunks: List[str]) -> None:
    """
    Ensure that each quotation has adequate context.

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        transcript_chunks (List[str]): List of transcript chunks.

    Raises:
        AssertionError: If a quotation lacks necessary context or is not found in transcript.
    """
    for quote in quotations:
        quote_text = quote.get("quotation", "")
        # Check if the quote exists in any of the transcript chunks
        in_transcript = any(quote_text in chunk for chunk in transcript_chunks)
        if not in_transcript:
            error_msg = f"Quotation '{quote_text[:50]}...' not found in any transcript chunk."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        context = quote.get("context", {}).get("situation", "").strip()
        if not context:
            error_msg = f"Quotation '{quote_text[:50]}...' lacks contextual information."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_philosophical_alignment(quotations: List[Dict[str, Any]], theoretical_framework: Dict[str, str]) -> None:
    """
    Ensure that quotations align with the researcher's philosophical stance.

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        theoretical_framework (Dict[str, str]): The theoretical and philosophical framework guiding the analysis.

    Raises:
        AssertionError: If any quotation does not align with the specified orientation.
    """
    philosophical_approach = theoretical_framework.get("philosophical_approach", "").lower()
    for quote in quotations:
        theoretical_alignment = quote.get("analysis_value", {}).get("theoretical_alignment", "").lower()
        if philosophical_approach not in theoretical_alignment:
            error_msg = f"Quotation '{quote.get('quotation', '')[:50]}...' does not align with the philosophical approach '{philosophical_approach}'."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_patterns_identified(patterns_identified: List[str]) -> None:
    """
    Ensure that patterns have been identified and are valid.

    Args:
        patterns_identified (List[str]): List of identified patterns.

    Raises:
        AssertionError: If no patterns are identified or if any pattern is invalid.
    """
    if not patterns_identified:
        error_msg = "No patterns were identified in the analysis."
        logger.error(error_msg)
        raise AssertionError(error_msg)
    for pattern in patterns_identified:
        if not isinstance(pattern, str) or not pattern.strip():
            error_msg = f"Invalid pattern identified: '{pattern}'."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_theoretical_interpretation(theoretical_interpretation: str) -> None:
    """
    Ensure that the theoretical interpretation is adequate.

    Args:
        theoretical_interpretation (str): The theoretical interpretation text.

    Raises:
        AssertionError: If the interpretation is empty or insufficient.
    """
    if not theoretical_interpretation.strip():
        error_msg = "Theoretical interpretation is empty."
        logger.error(error_msg)
        raise AssertionError(error_msg)
    if len(theoretical_interpretation.strip().split()) < 20:
        error_msg = "Theoretical interpretation is too brief."
        logger.error(error_msg)
        raise AssertionError(error_msg)

def assert_research_alignment(research_alignment: str) -> None:
    """
    Ensure that the research alignment explanation is adequate.

    Args:
        research_alignment (str): The research alignment text.

    Raises:
        AssertionError: If the alignment explanation is empty or insufficient.
    """
    if not research_alignment.strip():
        error_msg = "Research alignment explanation is empty."
        logger.error(error_msg)
        raise AssertionError(error_msg)
    if len(research_alignment.strip().split()) < 20:
        error_msg = "Research alignment explanation is too brief."
        logger.error(error_msg)
        raise AssertionError(error_msg)


# File: assertions_alt.py
#------------------------------------------------------------------------------
# src/assertions.py
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def assert_relevant_quotations(quotations: List[Dict[str, Any]], themes: List[str]) -> None:
    """
    Ensure that each quotation is relevant to at least one identified theme.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        themes (List[str]): List of identified themes in the analysis.
    
    Raises:
        AssertionError: If a quotation does not align with any theme.
    """
    for quote in quotations:
        if not any(theme.lower() in [t.lower() for t in quote.get('themes', [])] for theme in themes):
            error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with any identified theme."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_confidentiality(quotations: List[Dict[str, Any]], sensitive_keywords: List[str]) -> None:
    """
    Ensure that no quotations contain sensitive or identifiable information.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        sensitive_keywords (List[str]): List of keywords that should not appear in quotations.
    
    Raises:
        AssertionError: If a quotation contains sensitive information.
    """
    for quote in quotations:
        quote_text = quote.get("quotation", "").lower()
        for keyword in sensitive_keywords:
            if keyword.lower() in quote_text:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' contains sensitive keyword '{keyword}'."
                logger.error(error_msg)
                raise AssertionError(error_msg)

def assert_diversity_of_quotations(quotations: List[Dict[str, Any]], min_participants: int = 3) -> None:
    """
    Ensure that quotations represent a diverse set of participants.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        min_participants (int): Minimum number of different participants required.
    
    Raises:
        AssertionError: If quotations do not represent the required diversity.
    """
    participant_ids = {q.get("participant_id") for q in quotations}
    if len(participant_ids) < min_participants:
        error_msg = f"Only {len(participant_ids)} unique participants are represented in quotations; minimum required is {min_participants}."
        logger.error(error_msg)
        raise AssertionError(error_msg)

def assert_contextual_adequacy(quotations: List[Dict[str, Any]], transcript_chunks: List[str]) -> None:
    """
    Ensure that each quotation has adequate context.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        transcript_chunks (List[str]): List of transcript chunks.
    
    Raises:
        AssertionError: If a quotation lacks necessary context or is not found in transcript.
    """
    for quote in quotations:
        quote_text = quote.get("quotation", "")
        context = quote.get("context", "").strip()
        # Check if the quote exists in any of the transcript chunks
        in_transcript = any(quote_text in chunk for chunk in transcript_chunks)
        if not in_transcript:
            error_msg = f"Quotation '{quote_text[:50]}...' not found in any transcript chunk."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        if not context:
            error_msg = f"Quotation '{quote_text[:50]}...' lacks contextual information."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_philosophical_alignment(quotations: List[Dict[str, Any]], theoretical_framework: str) -> None:
    """
    Ensure that quotations align with the researcher's philosophical stance.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        theoretical_framework (str): The theoretical and philosophical framework guiding the analysis.
    
    Raises:
        AssertionError: If any quotation does not align with the specified orientation.
    """
    framework_lower = theoretical_framework.lower()
    for quote in quotations:
        alignment_score = float(quote.get("alignment_score", 0.0))
        analysis_notes = quote.get("analysis_notes", "").lower()
        if "constructivist" in framework_lower:
            if alignment_score < 0.7 or "subjective" not in analysis_notes:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with constructivist orientation."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        elif "critical realism" in framework_lower:
            if alignment_score < 0.7 or "objective" not in analysis_notes:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with critical realism orientation."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        else:
            if alignment_score < 0.7:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' has insufficient alignment score."
                logger.error(error_msg)
                raise AssertionError(error_msg)


# File: assertions_keyword.py
#------------------------------------------------------------------------------
# src/analysis/assertions_keyword.py
import logging
from typing import List
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)

# Ensure NLTK data is downloaded
nltk.download('punkt')
nltk.download('stopwords')

def assert_keywords_extracted(keywords: List[str]) -> None:
    """
    Ensure that keywords have been extracted from the transcript chunks.

    Args:
        keywords (List[str]): List of extracted keywords.

    Raises:
        AssertionError: If no keywords were extracted.
    """
    if not keywords:
        error_msg = "No keywords were extracted from the transcript chunks."
        logger.error(error_msg)
        raise AssertionError(error_msg)

def assert_realness(keywords: List[str], quotation: str) -> None:
    """
    Checks if keywords genuinely reflect participants' experiences by verifying their presence in the quotation.

    Args:
        keywords (List[str]): List of extracted keywords.
        quotation (str): The specific quotation from which to extract keywords.

    Raises:
        AssertionError: If a keyword is not present in the quotation.
    """
    quotation_text = quotation.lower()
    for keyword in keywords:
        if keyword.lower() not in quotation_text:
            error_msg = f"Keyword '{keyword}' does not reflect participants' experiences (not found in quotation)."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_keywords_not_exclusive_to_context(keywords: List[str], quotation: str, contextual_info: List[str]) -> None:
    """
    Ensures that keywords are not exclusively derived from the contextual content.
    If a keyword exists in contextual_info, it must also exist in the quotation.

    Args:
        keywords (List[str]): List of extracted keywords.
        quotation (str): The specific quotation from which to extract keywords.
        contextual_info (List[str]): List of contextualized content providing background for the quotation.

    Raises:
        AssertionError: If a keyword is found exclusively in contextual_info.
    """
    quotation_text = quotation.lower()
    contextual_text = ' '.join(contextual_info).lower()

    for keyword in keywords:
        keyword_lower = keyword.lower()
        in_quotation = keyword_lower in quotation_text
        in_context = keyword_lower in contextual_text

        if in_context and not in_quotation:
            error_msg = f"Keyword '{keyword}' is exclusively present in contextual content and not in the quotation."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_richness(keywords: List[str]) -> None:
    """
    Assesses the richness of the keywords by ensuring they are meaningful and provide detailed understanding.

    Args:
        keywords (List[str]): List of extracted keywords.

    Raises:
        AssertionError: If keywords are not rich in meaning.
    """
    stop_words = set(stopwords.words('english'))
    for keyword in keywords:
        words = word_tokenize(keyword)
        content_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]
        if len(content_words) == 0:
            error_msg = f"Keyword '{keyword}' is not rich in meaning."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_repetition(keywords: List[str], quotation: str) -> None:
    """
    Checks that the keywords frequently occur in the data.

    Args:
        keywords (List[str]): List of extracted keywords.
        quotation (str): The specific quotation from which to extract keywords.

    Raises:
        AssertionError: If a keyword does not occur frequently in the data.
    """
    quotation_text = quotation.lower()
    word_counts = Counter(word_tokenize(quotation_text))
    total_words = sum(word_counts.values())
    for keyword in keywords:
        keyword_count = quotation_text.count(keyword.lower())
        frequency = keyword_count / total_words if total_words > 0 else 0
        if frequency < 0.001:  # Threshold can be adjusted
            error_msg = f"Keyword '{keyword}' does not occur frequently in the data."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_rationale(keywords: List[str], theoretical_framework: str) -> None:
    """
    Checks if the keywords are connected to the theoretical framework.

    Args:
        keywords (List[str]): List of extracted keywords.
        theoretical_framework (str): The theoretical framework text.

    Raises:
        AssertionError: If a keyword is not connected to the theoretical framework.
    """
    if not theoretical_framework:
        return
    framework_text = theoretical_framework.lower()
    for keyword in keywords:
        similarity = SequenceMatcher(None, keyword.lower(), framework_text).ratio()
        if similarity < 0.1:  # Threshold can be adjusted
            error_msg = f"Keyword '{keyword}' is not connected to the theoretical framework."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_repartee(keywords: List[str]) -> None:
    """
    Checks that the keywords are insightful, evocative, and stimulate further discussion.

    Args:
        keywords (List[str]): List of extracted keywords.

    Raises:
        AssertionError: If keywords are not insightful.
    """
    # Placeholder: In practice, this would require more advanced NLP techniques
    for keyword in keywords:
        if len(keyword.split()) < 2:  # Assuming insightful keywords are phrases
            error_msg = f"Keyword '{keyword}' may not be sufficiently insightful or evocative."
            logger.warning(error_msg)
            # Optionally, you can raise an AssertionError here

def assert_regal(keywords: List[str], research_objectives: str) -> None:
    """
    Checks that the keywords are central to understanding the phenomenon.

    Args:
        keywords (List[str]): List of extracted keywords.
        research_objectives (str): The research objectives text.

    Raises:
        AssertionError: If a keyword is not central to understanding the phenomenon.
    """
    objectives_text = research_objectives.lower()
    for keyword in keywords:
        if keyword.lower() not in objectives_text:
            error_msg = f"Keyword '{keyword}' may not be central to understanding the phenomenon."
            logger.warning(error_msg)
            # Optionally, you can raise an AssertionError here


# File: clearcache.py
#------------------------------------------------------------------------------
import os
import shutil
import logging
from elasticsearch import Elasticsearch
from typing import List, Optional
import json

def setup_logging() -> logging.Logger:
    """Configure and return a logger instance."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def clear_elasticsearch_cache(logger: logging.Logger, es_host: str = "http://localhost:9200") -> None:
    """Clear Elasticsearch indices."""
    try:
        es = Elasticsearch(es_host)
        if es.ping():
            indices_to_delete = ["contextual_bm25_index"]
            for index in indices_to_delete:
                if es.indices.exists(index=index):
                    es.indices.delete(index=index)
                    logger.info(f"Deleted Elasticsearch index: {index}")
            logger.info("Elasticsearch cache cleared successfully")
    except Exception as e:
        logger.error(f"Error clearing Elasticsearch cache: {e}")

def remove_file_or_directory(path: str, logger: logging.Logger) -> None:
    """Remove a file or directory with proper error handling."""
    try:
        if os.path.isfile(path):
            os.remove(path)
            logger.info(f"Removed file: {path}")
        elif os.path.isdir(path):
            shutil.rmtree(path, ignore_errors=True)
            logger.info(f"Removed directory: {path}")
    except Exception as e:
        logger.error(f"Error removing {path}: {e}")

def clear_pickle_files(data_dir: str, logger: logging.Logger) -> None:
    """Remove all pickle files in the specified directory and its subdirectories."""
    try:
        for root, _, files in os.walk(data_dir):
            for file in files:
                if file.endswith((".pkl", ".pickle")):
                    file_path = os.path.join(root, file)
                    os.remove(file_path)
                    logger.info(f"Removed pickle file: {file_path}")
    except Exception as e:
        logger.error(f"Error clearing pickle files: {e}")

def clear_temporary_files(data_dir: str, logger: logging.Logger) -> None:
    """Remove temporary files like .pyc, .pyo, and __pycache__ directories."""
    try:
        # First, collect all paths to remove
        cache_dirs = []
        compiled_files = []
        
        for root, dirs, files in os.walk(data_dir, topdown=True):
            # Collect __pycache__ directories
            if "__pycache__" in dirs:
                cache_dir = os.path.join(root, "__pycache__")
                cache_dirs.append(cache_dir)
                dirs.remove("__pycache__")  # Prevent recursing into __pycache__
            
            # Collect .pyc and .pyo files
            for file in files:
                if file.endswith((".pyc", ".pyo")):
                    file_path = os.path.join(root, file)
                    compiled_files.append(file_path)
        
        # Remove collected paths
        for cache_dir in cache_dirs:
            if os.path.exists(cache_dir):  # Check again in case it was already removed
                shutil.rmtree(cache_dir, ignore_errors=True)
                logger.info(f"Removed __pycache__ directory: {cache_dir}")
        
        for file_path in compiled_files:
            if os.path.exists(file_path):  # Check again in case it was already removed
                os.remove(file_path)
                logger.info(f"Removed compiled Python file: {file_path}")
                
    except Exception as e:
        logger.error(f"Error clearing temporary files: {e}")

def create_directories(directories: List[str], logger: logging.Logger) -> None:
    """Create necessary directories."""
    for directory in directories:
        try:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"Created directory: {directory}")
        except Exception as e:
            logger.error(f"Error creating directory {directory}: {e}")

def clear_model_cache(cache_dir: str, logger: logging.Logger) -> None:
    """Clear any cached model files or artifacts."""
    try:
        model_paths = [
            os.path.join(cache_dir, "models"),
            os.path.join(cache_dir, "optimized_models"),
            os.path.join(cache_dir, "checkpoints")
        ]
        for path in model_paths:
            if os.path.exists(path):
                shutil.rmtree(path, ignore_errors=True)
                logger.info(f"Cleared model cache: {path}")
    except Exception as e:
        logger.error(f"Error clearing model cache: {e}")

def clear_output_files(output_dir: str, logger: logging.Logger) -> None:
    """Clear generated output files."""
    try:
        # Clear JSON output files
        for root, _, files in os.walk(output_dir):
            for file in files:
                if file.endswith((".json", ".jsonl")):
                    file_path = os.path.join(root, file)
                    os.remove(file_path)
                    logger.info(f"Removed output file: {file_path}")
    except Exception as e:
        logger.error(f"Error clearing output files: {e}")

def clear_all_cache(base_dir: Optional[str] = None) -> None:
    """Main function to clear all cache and temporary files."""
    logger = setup_logging()
    
    if base_dir is None:
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    logger.info(f"Starting cache clearing process in: {base_dir}")
    
    # Define paths relative to base directory
    data_dir = os.path.join(base_dir, "data")
    cache_dir = os.path.join(base_dir, "cache")
    logs_dir = os.path.join(base_dir, "logs")
    output_dir = os.path.join(base_dir, "output")
    
    # 1. Clear Elasticsearch cache
    clear_elasticsearch_cache(logger)
    
    # 2. Clear data directories
    data_paths = [
        os.path.join(data_dir, "contextual_db"),
        os.path.join(data_dir, "optimized_program.json"),
        logs_dir,
        cache_dir,
        os.path.join(data_dir, "contextual_db", "faiss_index.bin"),
        output_dir
    ]
    
    for path in data_paths:
        remove_file_or_directory(path, logger)
    
    # 3. Clear pickle files
    clear_pickle_files(data_dir, logger)
    
    # 4. Clear temporary Python files from the entire project directory
    clear_temporary_files(base_dir, logger)
    
    # 5. Clear model cache
    clear_model_cache(cache_dir, logger)
    
    # 6. Clear output files
    clear_output_files(output_dir, logger)
    
    # 7. Recreate necessary directories
    required_dirs = [
        data_dir,
        os.path.join(data_dir, "contextual_db"),
        logs_dir,
        cache_dir,
        output_dir
    ]
    create_directories(required_dirs, logger)
    
    logger.info("Cache clearing completed successfully")

if __name__ == "__main__":
    clear_all_cache()


################################################################################
# Module: core
################################################################################


# File: core/__init__.py
#------------------------------------------------------------------------------
"""
Core functionality for the thematic analysis package.
Contains database and client implementations.
"""

from .contextual_vector_db import ContextualVectorDB
from .elasticsearch_bm25 import ElasticsearchBM25
from .openai_client import OpenAIClient

__all__ = ['ContextualVectorDB', 'ElasticsearchBM25', 'OpenAIClient']

# File: core/contextual_vector_db.py
#------------------------------------------------------------------------------
import os
import pickle
import numpy as np
import threading
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from dotenv import load_dotenv
import logging
import faiss
import dspy

from src.core.openai_client import OpenAIClient
from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)


class SituateContext(dspy.Module):
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(SituateContextSignature)

    def forward(self, doc: str, chunk: str):
        prompt = f"""
                <document>
                {doc}
                </document>
            

                CHUNK_CONTEXT_PROMPT = 
                Here is the chunk we want to situate within the whole document
                <chunk>
                {chunk}
                </chunk>

                Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
                Answer only with the succinct context and nothing else.
    """
        
        return self.chain(doc=doc, chunk=chunk, prompt=prompt)


class SituateContextSignature(dspy.Signature):
    doc = dspy.InputField(desc="Full document content")
    chunk = dspy.InputField(desc="Specific chunk content")
    reasoning = dspy.OutputField(desc="Chain of thought reasoning")
    contextualized_content = dspy.OutputField(desc="Contextualized content for the chunk")
        

class ContextualVectorDB:
    def __init__(self, name: str, openai_api_key: str = None):
        if openai_api_key is None:
            openai_api_key = os.getenv("OPENAI_API_KEY")
        
        self.name = name
        self.embeddings = []
        self.metadata = []
        self.db_path = f"./data/{name}/contextual_vector_db.pkl"
        self.faiss_index_path = f"./data/{name}/faiss_index.bin"

        self.client = OpenAIClient(api_key=openai_api_key)
        logger.debug(f"Initialized OpenAIClient for ContextualVectorDB '{self.name}'")

    def situate_context(self, doc: str, chunk: str) -> Tuple[str, Any]:
        logger.debug(f"Entering situate_context with doc length={len(doc)} and chunk length={len(chunk)}.")
        try:
            if not hasattr(self, 'situate_context_module'):
                self.situate_context_module = SituateContext()
                logger.debug("Initialized SituateContext module")

            response = self.situate_context_module(doc=doc, chunk=chunk)
            contextualized_content = response.contextualized_content
            logger.debug("Generated contextualized_content using DSPy.")
            usage_metrics = {}
            return contextualized_content, usage_metrics
        except Exception as e:
            logger.error(f"Error during DSPy situate_context: {e}", exc_info=True)
            return "", None

    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 1):
        logger.debug("Entering load_data method.")
        if self.embeddings and self.metadata and os.path.exists(self.faiss_index_path):
            logger.info("Vector database is already loaded. Skipping data loading.")
            return
        if os.path.exists(self.db_path) and os.path.exists(self.faiss_index_path):
            logger.info("Loading vector database and FAISS index from disk.")
            self.load_db()
            self.load_faiss_index()
            return

        texts_to_embed, metadata = self._process_dataset(dataset, parallel_threads)
        
        self._embed_and_store(texts_to_embed, metadata)
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}")


    def _process_dataset(self, dataset: List[Dict[str, Any]], parallel_threads: int) -> Tuple[List[str], List[Dict[str, Any]]]:
        texts_to_embed = []
        metadata = []
        total_chunks = sum(len(doc.get('chunks', [])) for doc in dataset)
        logger.info(f"Total chunks to process: {total_chunks}")

        logger.info(f"Processing {total_chunks} chunks with {parallel_threads} threads.")
        try:
            with ThreadPoolExecutor(max_workers=parallel_threads) as executor:
                futures = []
                for doc in dataset:
                    for chunk in doc.get('chunks', []):
                        futures.append(executor.submit(self._generate_contextualized_content, doc, chunk))
                
                for future in tqdm(as_completed(futures), total=total_chunks, desc="Processing chunks"):
                    result = future.result()
                    if result:
                        texts_to_embed.append(result['text_to_embed'])
                        metadata.append(result['metadata'])
        except Exception as e:
            logger.error(f"Error during processing chunks: {e}", exc_info=True)
            return [], []

        return texts_to_embed, metadata

    def _generate_contextualized_content(self, doc: Dict[str, Any], chunk: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(doc, dict):
            logger.error(f"Document is not a dictionary: {doc}")
            return None

        if isinstance(chunk, dict):
            chunk_id = chunk.get('chunk_id')
            if not chunk_id:
                # Assign a unique chunk_id combining doc_id and chunk's index
                chunk_index = chunk.get('index', 0)
                chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_{chunk_index}"
                chunk['chunk_id'] = chunk_id
            content = chunk.get('content', '')
            original_index = chunk.get('original_index', chunk.get('index', 0))
        elif isinstance(chunk, str):
            # Handle case where chunk is a string
            content = chunk
            chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_0"
            original_index = 0
            logger.warning(f"Chunk is a string. Expected a dict. Assigning default values.")
        else:
            logger.error(f"Unsupported chunk type: {type(chunk)}. Skipping chunk.")
            return None

        logger.debug(f"Processing chunk_id='{chunk_id}' in doc_id='{doc.get('doc_id', 'unknown_doc_id')}'")
        contextualized_text, usage = self.situate_context(doc.get('content', ''), content)
        return {
            'text_to_embed': f"{content}\n\n{contextualized_text}",
            'metadata': {
                'doc_id': doc.get('doc_id', ''),
                'original_uuid': doc.get('original_uuid', ''),
                'chunk_id': chunk_id,
                'original_index': original_index,
                'original_content': content,
                'contextualized_content': contextualized_text
            }
        }

    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):
        logger.debug("Entering _embed_and_store method.")
        batch_size = 128
        embeddings = []
        logger.info("Starting embedding generation.")
        try:
            with tqdm(total=len(texts), desc="Embedding chunks") as pbar:
                for i in range(0, len(texts), batch_size):
                    batch = texts[i : i + batch_size]
                    try:
                        response = self.client.create_embeddings(
                            model="text-embedding-3-small",
                            input=batch
                        )
                        embeddings_batch = [item['embedding'] for item in response['data']]
                        embeddings.extend(embeddings_batch)
                        pbar.update(len(batch))
                        logger.debug(f"Processed batch {i // batch_size + 1}: {len(batch)} embeddings.")
                    except Exception as e:
                        logger.error(f"Error during OpenAI embeddings for batch starting at index {i}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Unexpected error during embedding generation: {e}", exc_info=True)
        
        self.embeddings = embeddings
        self.metadata = data
        logger.info("Embedding generation completed.")

    def _build_faiss_index(self):
        self.create_faiss_index()
        self.save_faiss_index()

    def create_faiss_index(self):
        logger.debug("Entering create_faiss_index method.")
        try:
            embedding_dim = len(self.embeddings[0])
            logger.info(f"Embedding dimension: {embedding_dim}")
            embeddings_np = np.array(self.embeddings).astype('float32')
            faiss.normalize_L2(embeddings_np)
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.index.add(embeddings_np)
            logger.info(f"FAISS index created with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error creating FAISS index: {e}", exc_info=True)
            raise

    def save_faiss_index(self):
        logger.debug("Entering save_faiss_index method.")
        os.makedirs(os.path.dirname(self.faiss_index_path), exist_ok=True)
        try:
            faiss.write_index(self.index, self.faiss_index_path)
            logger.info(f"FAISS index saved to '{self.faiss_index_path}'")
        except Exception as e:
            logger.error(f"Error saving FAISS index to '{self.faiss_index_path}': {e}", exc_info=True)

    def load_faiss_index(self):
        logger.debug("Entering load_faiss_index method.")
        if not os.path.exists(self.faiss_index_path):
            logger.error(f"FAISS index file not found at '{self.faiss_index_path}'.")
            raise ValueError("FAISS index file not found.")
        try:
            self.index = faiss.read_index(self.faiss_index_path)
            logger.info(f"FAISS index loaded from '{self.faiss_index_path}' with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error loading FAISS index from '{self.faiss_index_path}': {e}", exc_info=True)
            raise

    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        logger.debug(f"Entering search method with query='{query}' and k={k}.")
        if not self.embeddings or not self.metadata:
            logger.error("Embeddings or metadata are not loaded. Cannot perform search.")
            return []
        if not hasattr(self, 'index'):
            logger.error("FAISS index is not loaded.")
            return []
        
        try:
            response = self.client.create_embeddings(
                model="text-embedding-3-small",
                input=[query]
            )
            query_embedding = response['data'][0]['embedding']
            logger.debug(f"Generated embedding for query: '{query}'")
        except Exception as e:
            logger.error(f"Error generating embedding for query '{query}': {e}", exc_info=True)
            return []

        query_embedding_np = np.array([query_embedding]).astype('float32')
        faiss.normalize_L2(query_embedding_np)

        logger.debug("Performing FAISS search.")
        try:
            distances, indices = self.index.search(query_embedding_np, k)
            indices = indices.flatten()
            distances = distances.flatten()
        except Exception as e:
            logger.error(f"Error during FAISS search: {e}", exc_info=True)
            return []

        top_results = []
        for idx, score in zip(indices, distances):
            if idx < len(self.metadata):
                meta = self.metadata[idx]
                result = {
                    "doc_id": meta['doc_id'],
                    "chunk_id": meta['chunk_id'],
                    "original_index": meta.get('original_index', 0),
                    "content": meta['original_content'],
                    "contextualized_content": meta.get('contextualized_content'),
                    "score": float(score),
                    "metadata": meta
                }
                top_results.append(result)
            else:
                logger.warning(f"Index {idx} out of bounds for metadata.")
        logger.debug(f"FAISS search returned {len(top_results)} results for query: '{query}'")
        logger.info(f"Chunks retrieved for query '{query}': {[res['chunk_id'] for res in top_results]}")

        return top_results

    def save_db(self):
        logger.debug("Entering save_db method.")
        data = {
            "metadata": self.metadata,
        }
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        try:
            with open(self.db_path, "wb") as file:
                pickle.dump(data, file)
            logger.info(f"Vector database metadata saved to '{self.db_path}'")
        except Exception as e:
            logger.error(f"Error saving vector database metadata to '{self.db_path}': {e}", exc_info=True)

    def load_db(self):
        logger.debug("Entering load_db method.")
        if not os.path.exists(self.db_path):
            logger.error(f"Vector database file not found at '{self.db_path}'. Use load_data to create a new database.")
            raise ValueError("Vector database file not found.")
        try:
            with open(self.db_path, "rb") as file:
                data = pickle.load(file)
            self.metadata = data.get("metadata", [])
            logger.info(f"Vector database metadata loaded from '{self.db_path}' with {len(self.metadata)} entries.")
            logger.info(f"Chunks loaded: {[meta['chunk_id'] for meta in self.metadata]}")
        except Exception as e:
            logger.error(f"Error loading vector database metadata from '{self.db_path}': {e}", exc_info=True)
            raise


# File: core/elasticsearch_bm25.py
#------------------------------------------------------------------------------
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk, scan
from elasticsearch.exceptions import NotFoundError, RequestError
import json
import os

from src.utils.logger import setup_logging
setup_logging()
logger = logging.getLogger(__name__)

es_host = "http://localhost:9200"
index_name = "contextual_bm25_index"
es_client = Elasticsearch(es_host)

if es_client.indices.exists(index=index_name):
    es_client.indices.delete(index=index_name)
    print(f"Index '{index_name}' deleted.")

class ElasticsearchBM25:
    
    def __init__(
        self, 
        index_name: str = "contextual_bm25_index",
        es_host: str = "http://localhost:9200",
        logger: Optional[logging.Logger] = None
    ):
        self.logger = logger or logging.getLogger(__name__)
        self.index_name = index_name
        self.es_client = Elasticsearch(es_host)
        
        if not self.es_client.ping():
            raise ConnectionError(f"Failed to connect to Elasticsearch at {es_host}")
            
        self._create_index()
        
    def _create_index(self) -> None:
        index_settings = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "english"
                        },
                        "custom_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "porter_stem"]
                        }
                    }
                },
                "similarity": {
                    "custom_bm25": {
                        "type": "BM25",
                        "k1": 1.2,
                        "b": 0.75,
                    }
                },
                "index": {
                    "refresh_interval": "1s",
                    "number_of_shards": 1,
                    "number_of_replicas": 0
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "contextualized_content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "doc_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "original_index": {"type": "integer"},
                    "metadata": {"type": "object", "enabled": True}
                }
            }
        }
        
        index_data_dir = f"./data/{self.index_name}"
        os.makedirs(index_data_dir, exist_ok=True)
        self.logger.debug(f"Ensured existence of index data directory: {index_data_dir}")
        
        try:
            if self.es_client.indices.exists(index=self.index_name):
                self.es_client.indices.close(index=self.index_name)
                self.es_client.indices.put_settings(
                    index=self.index_name,
                    body=index_settings["settings"]
                )
                self.es_client.indices.open(index=self.index_name)
            else:
                self.es_client.indices.create(
                    index=self.index_name,
                    body=index_settings
                )
            self.logger.info(f"Successfully configured index: {self.index_name}")
        except Exception as e:
            self.logger.error(f"Failed to create/update index: {str(e)}")
            raise

    def index_documents(
        self,
        documents: List[Dict[str, Any]],
        batch_size: int = 500
    ) -> Tuple[int, List[Dict[str, Any]]]:
        if not documents:
            self.logger.warning("No documents provided for indexing")
            return 0, []

        failed_docs = []
        success_count = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            actions = [{
                "_index": self.index_name,
                "_source": {
                    "content": doc.get("original_content", ""),
                    "contextualized_content": doc.get("contextualized_content", ""),
                    "doc_id": doc.get("doc_id", ""),
                    "chunk_id": doc.get("chunk_id", ""),
                    "original_index": doc.get("original_index", 0),
                    "metadata": doc.get("metadata", {})
                }
            } for doc in batch]
            
            try:
                success, failed = bulk(
                    self.es_client,
                    actions,
                    raise_on_error=False,
                    raise_on_exception=False
                )
                success_count += success
                if failed:
                    for fail in failed:
                        failed_doc = batch[fail.get('index', 0)]
                        failed_docs.append(failed_doc)
                    self.logger.warning(f"Failed to index {len(failed)} documents in batch")
                    
            except Exception as e:
                self.logger.error(f"Batch indexing error: {str(e)}")
                failed_docs.extend(batch)
                
        self.es_client.indices.refresh(index=self.index_name)
        self.logger.info(f"Indexed {success_count}/{len(documents)} documents successfully")
        return success_count, failed_docs

    def search(
        self,
        query: str,
        k: int = 20,
        min_score: float = 0.1,
        fields: List[str] = None,
        operator: str = "or",
        minimum_should_match: str = "30%"
    ) -> List[Dict[str, Any]]:
        if not fields:
            fields = ["content^1", "contextualized_content^1.5"]
            
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "operator": operator,
                    "minimum_should_match": minimum_should_match,
                    "type": "best_fields",
                    "tie_breaker": 0.3
                }
            },
            "min_score": min_score,
            "size": k,
            "_source": True
        }

        try:
            search_body_json = json.dumps(search_body, indent=2)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")
        except Exception as e:
            self.logger.error(f"Error converting search body to JSON: {e}")
            search_body_json = str(search_body)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")

        try:
            response = self.es_client.search(
                index=self.index_name,
                body=search_body
            )
            
            hits = [{
                "doc_id": hit["_source"]["doc_id"],
                "chunk_id": hit["_source"]["chunk_id"],
                "content": hit["_source"]["content"],
                "contextualized_content": hit["_source"].get("contextualized_content"),
                "score": hit["_score"],
                "metadata": hit["_source"].get("metadata", {})
            } for hit in response["hits"]["hits"]]
            
            self.logger.debug(
                f"Search for '{query}' returned {len(hits)} results "
                f"(max_score: {response['hits'].get('max_score', 0)})"
            )
            return hits
            
        except Exception as e:
            self.logger.error(f"Search error: {str(e)}")
            raise

    def search_content(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["content^1"],
            operator="or",
            minimum_should_match="30%"
        )

    def search_contextualized(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["contextualized_content^1.5"],
            operator="or",
            minimum_should_match="30%"
        )


# File: core/openai_client.py
#------------------------------------------------------------------------------
import logging
import os
from typing import List, Dict, Any
from openai import OpenAI
# Initialize logger
logger = logging.getLogger(__name__)

class OpenAIClient:
    """
    Wrapper for OpenAI API interactions using the updated SDK.
    """
    def __init__(self, api_key: str):
        """
        Initializes the OpenAI client with the provided API key.
        
        Args:
            api_key (str): OpenAI API key.
        """
        if not api_key:
            logger.error("OpenAI API key is not provided.")
            raise ValueError("OpenAI API key must be provided.")
        self.client = OpenAI(api_key=api_key)
        logger.debug("OpenAI client initialized successfully.")

    def create_chat_completion(self, model: str, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> Dict[str, Any]:
        """
        Creates a chat completion using OpenAI's API.
        
        Args:
            model (str): Model name to use.
            messages (List[Dict[str, str]]): List of message dictionaries.
            max_tokens (int): Maximum number of tokens in the response.
            temperature (float): Sampling temperature.
        
        Returns:
            Dict[str, Any]: API response as a dictionary.
        """
        if not model or not messages:
            logger.error("Model and messages must be provided for chat completion.")
            raise ValueError("Model and messages must be provided for chat completion.")
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            logger.debug(f"Chat completion created successfully for model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating chat completion: {e}")
            raise

    def create_embeddings(self, model: str, input: List[str]) -> Dict[str, Any]:
        """
        Creates embeddings for the given input texts using OpenAI's API.
        
        Args:
            model (str): Embedding model to use.
            input (List[str]): List of input texts.
        
        Returns:
            Dict[str, Any]: API response containing embeddings.
        """
        if not model or not input:
            logger.error("Model and input must be provided for creating embeddings.")
            raise ValueError("Model and input must be provided for creating embeddings.")
        
        try:
            response = self.client.embeddings.create(
                model=model,
                input=input
            )
            logger.debug(f"Embeddings created successfully using model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            raise



################################################################################
# Module: data
################################################################################


# File: data/__init__.py
#------------------------------------------------------------------------------
"""
Data handling modules for loading and processing data.
"""

from .data_loader import load_codebase_chunks, load_queries
from .copycode import CodeFileHandler

__all__ = ['load_codebase_chunks', 'load_queries', 'CodeFileHandler']

# File: data/copycode.py
#------------------------------------------------------------------------------
import logging
import os
import asyncio
import aiofiles
from typing import List, Set
import time
from pathlib import Path

# Initialize logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CodeFileHandler:
    def __init__(self, project_root: str, extensions: Set[str] = None):
        """
        Initialize the CodeFileHandler.
        
        Args:
            project_root (str): Root directory of the project
            extensions (Set[str]): Set of file extensions to include
        """
        self.project_root = project_root
        self.src_dir = os.path.join(project_root, '')
        self.extensions = extensions or {'.py', '.yaml', '.yml'}
        self.ignored_dirs = {'.venv', 'venv', 'env', '.env', 'myenv', '__pycache__', '.git', 'node_modules'}
        self.output_dir = os.path.join(project_root, 'output')

    async def get_code_files(self) -> List[str]:
        """
        Get all code files from the src directory.
        
        Returns:
            List[str]: List of file paths
        """
        logger.info(f"Scanning for code files in: {self.src_dir}")
        start_time = time.time()
        code_files = []
        
        try:
            for root, dirs, files in os.walk(self.src_dir):
                # Remove ignored directories
                dirs[:] = [d for d in dirs if d not in self.ignored_dirs]
                
                # Process files
                for file in files:
                    if file.endswith(tuple(self.extensions)):
                        file_path = os.path.join(root, file)
                        relative_path = os.path.relpath(file_path, self.src_dir)
                        code_files.append((file_path, relative_path))
                        logger.debug(f"Found code file: {relative_path}")
        
        except Exception as e:
            logger.error(f"Error scanning code files: {e}", exc_info=True)
            return []

        end_time = time.time()
        logger.info(f"Found {len(code_files)} code files in {end_time - start_time:.2f} seconds")
        return sorted(code_files, key=lambda x: x[1])  # Sort by relative path

    async def copy_code_to_file(self, code_files: List[tuple], output_file: str):
        """
        Copy code files to a single output file with proper organization.
        
        Args:
            code_files (List[tuple]): List of (file_path, relative_path) tuples
            output_file (str): Output file path
        """
        logger.info(f"Writing consolidated code to: {output_file}")
        start_time = time.time()
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        try:
            async with aiofiles.open(output_file, 'w', encoding='utf-8') as outfile:
                # Write header
                await outfile.write("# Consolidated Source Code\n")
                await outfile.write("# " + "=" * 78 + "\n\n")

                current_module = None
                for file_path, relative_path in code_files:
                    # Get module name (first directory in relative path)
                    module = relative_path.split(os.sep)[0] if os.sep in relative_path else 'root'
                    
                    # Write module header if changed
                    if module != current_module:
                        await outfile.write(f"\n\n{'#' * 80}\n")
                        await outfile.write(f"# Module: {module}\n")
                        await outfile.write(f"{'#' * 80}\n\n")
                        current_module = module

                    # Write file header
                    await outfile.write(f"\n# File: {relative_path}\n")
                    await outfile.write("#" + "-" * 78 + "\n")

                    try:
                        async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                            content = await infile.read()
                            await outfile.write(content)
                            await outfile.write("\n")
                    except Exception as e:
                        error_msg = f"# Error reading file {relative_path}: {str(e)}\n"
                        await outfile.write(error_msg)
                        logger.error(error_msg)

        except Exception as e:
            logger.error(f"Error writing to output file: {e}", exc_info=True)
            return

        end_time = time.time()
        logger.info(f"Code consolidation completed in {end_time - start_time:.2f} seconds")

async def main_async():
    """
    Main async function to handle code consolidation.
    """
    try:
        # Get project root (parent of src directory)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        logger.info(f"Project root: {project_root}")

        # Initialize handler
        handler = CodeFileHandler(
            project_root=project_root,
            extensions={'.py', '.yaml', '.yml'}
        )

        # Create output directory if it doesn't exist
        output_dir = os.path.join(project_root, 'output')
        os.makedirs(output_dir, exist_ok=True)

        # Get code files
        code_files = await handler.get_code_files()
        
        if not code_files:
            logger.error("No code files found to process")
            return

        # Generate output file path
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f'consolidated_code_{timestamp}.txt')

        # Copy code files
        await handler.copy_code_to_file(code_files, output_file)
        logger.info(f"Code consolidated successfully to: {output_file}")

    except Exception as e:
        logger.error(f"Error in main_async: {e}", exc_info=True)

def main():
    """
    Main entry point of the script.
    """
    try:
        asyncio.run(main_async())
        logger.info("Code consolidation completed successfully")
    except KeyboardInterrupt:
        logger.warning("Process interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)

if __name__ == "__main__":
    main()

# File: data/data_loader.py
#------------------------------------------------------------------------------
import json
import logging
from typing import List, Dict, Any

from src.utils.logger import setup_logging
logger = logging.getLogger(__name__)

class JSONLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Dict[str, Any]]:
        data = []
        try:
            if self.file_path.endswith('.jsonl'):
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    for line_number, line in enumerate(f, start=1):
                        if line.strip():
                            try:
                                obj = json.loads(line)
                                data.append(obj)
                            except json.JSONDecodeError as e:
                                logger.error(f"JSON parsing error in file '{self.file_path}' at line {line_number}: {e}")
                logger.info(f"Loaded JSONL file '{self.file_path}' with {len(data)} entries successfully.")
            else:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logger.info(f"Loaded JSON file '{self.file_path}' successfully with {len(data)} entries.")
            return data
        except FileNotFoundError:
            logger.error(f"Error loading JSON file '{self.file_path}': File does not exist.")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"Error loading JSON file '{self.file_path}': {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error loading JSON file '{self.file_path}': {e}")
            return []

def load_codebase_chunks(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading codebase chunks from '{file_path}'.")
    loader = JSONLoader(file_path)
    return loader.load()

def load_queries(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading queries from '{file_path}'.")
    loader = JSONLoader(file_path)
    return loader.load()



################################################################################
# Module: root
################################################################################


# File: decorators.py
#------------------------------------------------------------------------------
import logging
import functools

logger = logging.getLogger(__name__)

def handle_exceptions(func):
    """
    Decorator to handle exceptions in functions.
    
    Args:
        func (callable): The function to wrap with exception handling.
    
    Returns:
        callable: The wrapped function with exception handling.
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}", exc_info=True)
            return {"error": "An error occurred. Please try again later."}
    return wrapper



################################################################################
# Module: evaluation
################################################################################


# File: evaluation/__init__.py
#------------------------------------------------------------------------------
"""
Evaluation modules for assessing model performance.
"""

from .evaluation import PipelineEvaluator
from .evaluator import PipelineEvaluator as Evaluator

__all__ = ['PipelineEvaluator', 'Evaluator']

# File: evaluation/evaluation.py
#------------------------------------------------------------------------------
import logging
import time
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.analysis.metrics import comprehensive_metric, is_answer_fully_correct

setup_logging()
logger = logging.getLogger(__name__)

class PipelineEvaluator:
    def __init__(self, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, retrieval_function: Callable):
        self.db = db
        self.es_bm25 = es_bm25
        self.retrieval_function = retrieval_function

    def evaluate_pipeline(self, queries: List[Dict[str, Any]], k: int = 20) -> Dict[str, float]:
        total_score = 0
        total_queries = len(queries)
        queries_with_golden = 0
        queries_without_golden = 0

        semantic_success = 0
        bm25_contextual_success = 0
        total_semantic_hits = 0
        total_bm25_contextual_hits = 0

        logger.info(f"Starting evaluation of {total_queries} queries.")

        for query_item in tqdm(queries, desc="Evaluating retrieval"):
            query = query_item.get('query', '').strip()

            has_golden_data = all([
                'golden_doc_uuids' in query_item,
                'golden_chunk_uuids' in query_item,
                'golden_documents' in query_item
            ])

            if has_golden_data:
                queries_with_golden += 1
                golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
                golden_contents = []

                for doc_uuid, chunk_index in golden_chunk_uuids:
                    golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                    if not golden_doc:
                        logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                    if not golden_chunk:
                        logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_contents.append(golden_chunk.get('content', '').strip())

                if not golden_contents:
                    logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                    continue

                retrieved_docs = self.retrieval_function(query, self.db, self.es_bm25, k)

                chunks_found = 0
                semantic_hits = 0
                bm25_contextual_hits = 0

                for golden_content in golden_contents:
                    for doc in retrieved_docs[:k]:
                        retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                        contextualized_content = doc.get('chunk', {}).get('contextualized_content', '').strip()
                        if retrieved_content == golden_content:
                            chunks_found += 1
                            semantic_hits += 1
                            break
                        elif contextualized_content == golden_content:
                            chunks_found += 1
                            bm25_contextual_hits += 1
                            break

                query_score = chunks_found / len(golden_contents)
                total_score += query_score
                logger.debug(f"Query '{query}' score: {query_score}")

                semantic_success += semantic_hits
                bm25_contextual_success += bm25_contextual_hits
                total_semantic_hits += semantic_hits
                total_bm25_contextual_hits += bm25_contextual_hits
            else:
                queries_without_golden += 1
                logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
                continue

        average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
        pass_at_n = average_score * 100

        semantic_percentage = (semantic_success / total_semantic_hits) * 100 if total_semantic_hits > 0 else 0
        bm25_contextual_percentage = (bm25_contextual_success / total_bm25_contextual_hits) * 100 if total_bm25_contextual_hits > 0 else 0

        logger.info(f"Evaluation completed.")
        logger.info(f"Total Queries: {total_queries}")
        logger.info(f"Queries with Golden Data: {queries_with_golden}")
        logger.info(f"Queries without Golden Data: {queries_without_golden}")
        logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")
        logger.info(f"Semantic Hits: {semantic_percentage:.2f}%")
        logger.info(f"BM25 Contextual Hits: {bm25_contextual_percentage:.2f}%")

        return {
            "pass_at_n": pass_at_n,
            "average_score": average_score,
            "semantic_hit_percentage": semantic_percentage,
            "bm25_contextual_hit_percentage": bm25_contextual_percentage,
            "total_queries": total_queries,
            "queries_with_golden": queries_with_golden,
            "queries_without_golden": queries_without_golden
        }

    def evaluate_complete_pipeline(self, k_values: List[int], evaluation_set: List[Dict[str, Any]]):
        for k in k_values:
            logger.info(f"Starting evaluation for Pass@{k}")
            results = self.evaluate_pipeline(evaluation_set, k)
            logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
            logger.info(f"Average Score: {results['average_score']:.4f}")
            logger.info(f"Semantic Hit Percentage: {results['semantic_hit_percentage']:.2f}%")
            logger.info(f"BM25 Contextual Hit Percentage: {results['bm25_contextual_hit_percentage']:.2f}%")
            logger.info(f"Total Queries: {results['total_queries']}")
            logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
            logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")


# File: evaluation/evaluator.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25

logger = logging.getLogger(__name__)

class PipelineEvaluator:
    def __init__(self, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, retrieval_function: Callable):
        self.db = db
        self.es_bm25 = es_bm25
        self.retrieval_function = retrieval_function

    def evaluate_pipeline(self, queries: List[Dict[str, Any]], k: int = 20) -> Dict[str, float]:
        total_score = 0
        total_queries = len(queries)
        queries_with_golden = 0
        queries_without_golden = 0

        logger.info(f"Starting evaluation of {total_queries} queries.")

        for query_item in tqdm(queries, desc="Evaluating retrieval"):
            query = query_item.get('query', '').strip()

            has_golden_data = all([
                'golden_doc_uuids' in query_item,
                'golden_chunk_uuids' in query_item,
                'golden_documents' in query_item
            ])

            if has_golden_data:
                queries_with_golden += 1
                golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
                golden_contents = []

                for doc_uuid, chunk_index in golden_chunk_uuids:
                    golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                    if not golden_doc:
                        logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                    if not golden_chunk:
                        logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_contents.append(golden_chunk.get('content', '').strip())

                if not golden_contents:
                    logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                    continue

                retrieved_docs = self.retrieval_function(query, self.db, self.es_bm25, k)

                chunks_found = 0
                for golden_content in golden_contents:
                    for doc in retrieved_docs[:k]:
                        retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                        if retrieved_content == golden_content:
                            chunks_found += 1
                            break

                query_score = chunks_found / len(golden_contents)
                total_score += query_score
                logger.debug(f"Query '{query}' score: {query_score}")
            else:
                queries_without_golden += 1
                logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
                continue

        average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
        pass_at_n = average_score * 100

        logger.info(f"Evaluation completed.")
        logger.info(f"Total Queries: {total_queries}")
        logger.info(f"Queries with Golden Data: {queries_with_golden}")
        logger.info(f"Queries without Golden Data: {queries_without_golden}")
        logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")

        return {
            "pass_at_n": pass_at_n,
            "average_score": average_score,
            "total_queries": total_queries,
            "queries_with_golden": queries_with_golden,
            "queries_without_golden": queries_without_golden
        }

    def evaluate_complete_pipeline(self, k_values: List[int], evaluation_set: List[Dict[str, Any]]):
        for k in k_values:
            logger.info(f"Starting evaluation for Pass@{k}")
            results = self.evaluate_pipeline(evaluation_set, k)
            logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
            logger.info(f"Average Score: {results['average_score']:.4f}")
            logger.info(f"Total Queries: {results['total_queries']}")
            logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
            logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")



################################################################################
# Module: root
################################################################################


# File: main.py
#------------------------------------------------------------------------------

import gc
import logging
import os
from typing import List, Dict, Any
import asyncio
import dspy
from dspy.teleprompt import BootstrapFewShotWithRandomSearch
from dspy.datasets import DataLoader
from dspy.primitives.assertions import assert_transform_module, backtrack_handler
import threading

from src.utils.logger import setup_logging
from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.data.data_loader import load_codebase_chunks, load_queries
from src.processing.query_processor import process_queries, validate_queries
from src.evaluation.evaluation import PipelineEvaluator
from src.analysis.metrics import comprehensive_metric
from src.processing.answer_generator import generate_answer_dspy, QuestionAnswerSignature
from src.retrieval.reranking import retrieve_with_reranking
from src.analysis.select_quotation_module import EnhancedQuotationModule
from src.decorators import handle_exceptions

# Initialize logging
setup_logging()
logger = logging.getLogger(__name__)

# Configure DSPy settings
dspy.settings.configure(main_thread_only=True)

# Introduce a thread lock mechanism
thread_lock = threading.Lock()

class ThematicAnalysisPipeline:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.contextual_db = None
        self.es_bm25 = None
        self.quotation_qa_module = None
        self.quotation_teleprompter = None
        self.optimized_quotation_program = None
        self.enhanced_quotation_module = None

    def create_elasticsearch_bm25_index(self) -> ElasticsearchBM25:
        """
        Create and index documents in Elasticsearch BM25.
        """
        logger.debug("Entering create_elasticsearch_bm25_index method.")
        try:
            es_bm25 = ElasticsearchBM25()
            logger.info("ElasticsearchBM25 instance created.")
            success_count, failed_docs = es_bm25.index_documents(self.contextual_db.metadata)
            logger.info(f"Elasticsearch BM25 index created successfully with {success_count} documents indexed.")
            if failed_docs:
                logger.warning(f"{len(failed_docs)} documents failed to index.")
        except Exception as e:
            logger.error(f"Error creating Elasticsearch BM25 index: {e}", exc_info=True)
            raise
        return es_bm25

    async def initialize_quotation_optimizer(self):
        """
        Initialize the quotation selection optimizer.
        """
        logger.info("Initializing quotation selection optimizer")
        dl = DataLoader()
        quotation_train_dataset = dl.from_csv(
            self.config['quotation_training_data'],
            fields=("input", "output"),
            input_keys=("input",)
        )

        self.quotation_qa_module = dspy.TypedChainOfThought(QuestionAnswerSignature)
        
        optimizer_config = {
            'max_bootstrapped_demos': 4,
            'max_labeled_demos': 4,
            'num_candidate_programs': 10,
            'num_threads': 1
        }
        
        self.quotation_teleprompter = BootstrapFewShotWithRandomSearch(
            metric=comprehensive_metric,
            **optimizer_config
        )

        self.optimized_quotation_program = self.quotation_teleprompter.compile(
            student=self.quotation_qa_module,
            teacher=self.quotation_qa_module,
            trainset=quotation_train_dataset
        )
        
        self.optimized_quotation_program.save(self.config['optimized_quotation_program'])
        logger.info("Quotation optimizer initialized successfully")

    @handle_exceptions
    async def run_pipeline(self):
        """
        Main function to load data, process queries, and generate outputs.
        """
        logger.debug("Entering run_pipeline method.")
        try:
            # Configure DSPy Language Model
            logger.info("Configuring DSPy Language Model")
            lm = dspy.LM('openai/gpt-4o-mini', max_tokens=8192)
            dspy.configure(lm=lm)
            dspy.Cache = False

            # Define file paths from config
            codebase_chunks_file = self.config['codebase_chunks_file']
            queries_file_standard = self.config['queries_file_standard']
            evaluation_set_file = self.config['evaluation_set_file']
            output_filename_primary = self.config['output_filename_primary']

            dl = DataLoader()

            # Load the codebase chunks
            logger.info(f"Loading codebase chunks from '{codebase_chunks_file}'")
            codebase_chunks = load_codebase_chunks(codebase_chunks_file)

            # Initialize the ContextualVectorDB
            logger.info("Initializing ContextualVectorDB")
            self.contextual_db = ContextualVectorDB("contextual_db")

            # Load and process the data
            try:
                logger.info("Loading data into ContextualVectorDB")
                self.contextual_db.load_data(codebase_chunks, parallel_threads=1)
            except Exception as e:
                logger.error(f"Error loading data into ContextualVectorDB: {e}", exc_info=True)
                return

            # Create the Elasticsearch BM25 index
            try:
                logger.info("Creating Elasticsearch BM25 index")
                self.es_bm25 = self.create_elasticsearch_bm25_index()
            except Exception as e:
                logger.error(f"Error creating Elasticsearch BM25 index: {e}", exc_info=True)
                return

            # Load queries
            logger.info(f"Loading standard queries from '{queries_file_standard}'")
            standard_queries = load_queries(queries_file_standard)

            if not standard_queries:
                logger.error("No standard queries found to process.")

            # Validate queries
            logger.info("Validating standard queries")
            validated_standard_queries = validate_queries(standard_queries)

            # Initialize quotation optimizer
            await self.initialize_quotation_optimizer()

            # Initialize EnhancedQuotationModule with assertions
            try:
                logger.info("Initializing EnhancedQuotationModule")
                self.enhanced_quotation_module = EnhancedQuotationModule()
                self.enhanced_quotation_module = assert_transform_module(
                    self.enhanced_quotation_module, 
                    backtrack_handler
                )
                logger.info("EnhancedQuotationModule initialized successfully with assertions activated.")
            except Exception as e:
                logger.error(f"Error initializing EnhancedQuotationModule: {e}", exc_info=True)
                return

            # Define k value for standard queries
            k_standard = 20

            # Process standard queries with EnhancedQuotationModule
            logger.info("Processing standard queries with EnhancedQuotationModule")
            await process_queries(
                validated_standard_queries,
                self.contextual_db,
                self.es_bm25,
                k=k_standard,
                output_file=self.config['output_filename_primary'],
                optimized_program=self.optimized_quotation_program,
                module=self.enhanced_quotation_module
            )

            # Define k values for evaluation
            k_values = [5, 10, 20]

            # Initialize the evaluator
            logger.info("Starting evaluation of the retrieval pipeline")
            try:
                evaluator = PipelineEvaluator(
                    db=self.contextual_db,
                    es_bm25=self.es_bm25,
                    retrieval_function=retrieve_with_reranking
                )

                # Perform evaluation
                evaluation_set = load_queries(self.config['evaluation_set_file'])
                evaluator.evaluate_complete_pipeline(
                    k_values=k_values,
                    evaluation_set=evaluation_set
                )
                logger.info("Evaluation completed successfully.")
            except Exception as e:
                logger.error(f"Error during evaluation: {e}", exc_info=True)

            logger.info("All operations completed successfully.")
        except Exception as e:
            logger.error(f"Unexpected error in run_pipeline: {e}", exc_info=True)

if __name__ == "__main__":
    config = {
        'codebase_chunks_file': 'data/codebase_chunks.json',
        'queries_file_standard': 'data/queries.json',
        'evaluation_set_file': 'data/evaluation_set.jsonl',
        'output_filename_primary': 'query_results_quotation.json',
        'quotation_training_data': 'data/quotation_training_data.csv',
        'optimized_quotation_program': 'optimized_quotation_program.json'
    }
    pipeline = ThematicAnalysisPipeline(config)
    asyncio.run(pipeline.run_pipeline())



################################################################################
# Module: processing
################################################################################


# File: processing/__init__.py
#------------------------------------------------------------------------------
"""
Processing modules for handling queries and generating answers.
"""

from .answer_generator import generate_answer_dspy, QuestionAnswerSignature
from .query_processor import validate_queries, process_queries

__all__ = [
    'generate_answer_dspy',
    'QuestionAnswerSignature',
    'validate_queries',
    'process_queries'
]

# File: processing/answer_generator.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
import dspy
import asyncio

from src.analysis.metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from src.utils.utils import check_answer_length

logger = logging.getLogger(__name__)

class QuestionAnswerSignature(dspy.Signature):
    input: str = dspy.InputField(
        desc=(
            "The combined input containing both the question and context. "
            "The format should be 'question: <question_text> context: <context_text>'."
        )
    )
    answer: str = dspy.OutputField(
        desc=(
            "The generated answer to the question. The answer should be concise, directly address "
            "the question, and be grounded in the provided context to ensure factual accuracy."
        )
    )

    def forward(self, input: str, max_tokens: int = 8192) -> Dict[str, str]:
        try:
            # Parse the input to extract question and context
            parts = input.split(' context: ', 1)
            question = parts[0].replace('question: ', '').strip()
            context = parts[1].strip() if len(parts) > 1 else ""

            logger.debug(f"Generating answer for question: '{question}' with context length: {len(context)} characters.")
            answer = self.language_model.generate(
                prompt=(
                    f"You are an expert in qualitative research and thematic analysis.\n\n"
                    f"**Guidelines**:\n"
                    f"- **Relevance:** Extract quotations that are closely related to the key themes.\n"
                    f"- **Diversity:** Ensure a range of perspectives and viewpoints.\n"
                    f"- **Clarity:** Choose clear and understandable quotations.\n"
                    f"- **Impact:** Select impactful quotations that highlight significant aspects of the data.\n"
                    f"- **Authenticity:** Maintain original expressions from participants.\n\n"
                    f"**Transcript Chunk**:\n{question}\n\n"
                    f"**Context:**\n{context}\n\n"
                    f"**Task:** Extract **3-5** relevant quotations from the transcript chunk based on the context provided. "
                    f"Provide each quotation in the following JSON format within a list:\n\n"
                    f"```json\n"
                    f"[\n"
                    f"    {{\"QUOTE\": \"This is the first quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the second quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the third quotation.\"}}\n"
                    f"]\n"
                    f"```"
                    f"Ensure that the response is a valid JSON array containing all relevant quotations. "
                    f"If no quotations are available, respond with an empty array `[]`."
                ),
                max_tokens=max_tokens,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=None
            ).strip()
            logger.info(f"Generated answer for question: '{question}'")
            logger.debug(f"Answer length: {len(answer)} characters.")
            return {"answer": answer}
        except Exception as e:
            logger.error(f"Error in QuestionAnswerSignature.forward: {e}", exc_info=True)
            return {"answer": "I'm sorry, I couldn't generate an answer at this time."}

try:
    qa_module = dspy.Program.load("optimized_program.json")
    logger.info("Optimized DSPy program loaded successfully.")
except Exception as e:
    try:
        qa_module = dspy.TypedChainOfThought(QuestionAnswerSignature)
        logger.info("Unoptimized DSPy module initialized successfully.")
    except Exception as inner_e:
        logger.error(f"Error initializing unoptimized DSPy module: {inner_e}", exc_info=True)
        raise

async def generate_answer(input: str, max_tokens: int = 8192) -> str:
    try:
        logger.debug(f"Generating answer for input with length: {len(input)} characters.")
        answer = await asyncio.to_thread(qa_module, input=input, max_tokens=max_tokens)
        return answer.get("answer", "I'm sorry, I couldn't generate an answer at this time.")
    except Exception as e:
        logger.error(f"Error in generate_answer: {e}", exc_info=True)
        return "I'm sorry, I couldn't generate an answer at this time."

async def evaluate_answer(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    try:
        logger.debug(f"Evaluating answer for input: '{example.get('input', '')}'")
        return await asyncio.to_thread(is_answer_fully_correct, example, pred)
    except Exception as e:
        logger.error(f"Error in evaluate_answer: {e}", exc_info=True)
        return False

async def generate_answer_dspy(query: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    logger.debug(f"Entering generate_answer_dspy with query='{query}' and {len(retrieved_chunks)} retrieved_chunks.")
    try:
        context = ""
        for i, chunk in enumerate(retrieved_chunks, 1):
            chunk_content = chunk['chunk'].get('original_content', '')
            chunk_context = chunk['chunk'].get('contextualized_content', '')
            context += f"Chunk {i}:\n"
            context += f"Content: {chunk_content}\n"
            context += f"Context: {chunk_context}\n\n"

        if not context.strip():
            logger.warning(f"No valid context found for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't find relevant information to answer your question.",
                "used_chunks": [],
                "num_chunks_used": 0
            }

        logger.debug(f"Formatted context with {len(retrieved_chunks)} sequential chunks:\n{context[:200]}...")
        used_chunks_info = [
            {
                "chunk_id": chunk['chunk'].get('chunk_id', ''),
                "doc_id": chunk['chunk'].get('doc_id', ''),
                "content_snippet": chunk['chunk'].get('original_content', '')[:100] + "..."
            }
            for chunk in retrieved_chunks
        ]
        logger.info(f"Total number of chunks used for context in query '{query}': {len(used_chunks_info)}")
        logger.info(f"Chunks used for context: {used_chunks_info}")
        input_data = f"question: {query} context: {context}"
        answer = await generate_answer(input_data)

        if not answer:
            logger.warning(f"No answer generated for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't generate an answer at this time.",
                "used_chunks": used_chunks_info,
                "num_chunks_used": len(used_chunks_info)
            }

        logger.debug(f"Generated answer for query '{query}': {answer}")
        logger.info(f"Number of chunks used for query '{query}': {len(used_chunks_info)}")
        example = {
            "context": context,
            "question": query
        }
        pred = {
            "answer": answer
        }
        suggestion = await evaluate_answer(example, pred)
        return {
            "answer": answer,
            "used_chunks": used_chunks_info,
            "num_chunks_used": len(used_chunks_info)
        }
    except Exception as e:
        logger.error(f"Error generating answer via DSPy for query '{query}': {e}", exc_info=True)
        return {
            "answer": "I'm sorry, I couldn't generate an answer at this time.",
            "used_chunks": [],
            "num_chunks_used": 0
        }

async def generate_answers_dspy(queries: List[str], retrieved_chunks_list: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    tasks = [
        generate_answer_dspy(query, retrieved_chunks)
        for query, retrieved_chunks in zip(queries, retrieved_chunks_list)
    ]
    return await asyncio.gather(*tasks)

def is_answer_factually_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    score = factuality_metric(example, pred)
    logger.debug(f"Factuality score: {score}")
    return score == 1


# File: processing/query_processor.py
#------------------------------------------------------------------------------
# processing/query_processor.py
import logging
from typing import List, Dict, Any
import json
from tqdm import tqdm
import asyncio

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.retrieval.retrieval import multi_stage_retrieval
from src.utils.logger import setup_logging
from src.decorators import handle_exceptions
import dspy

from src.analysis.select_quotation_module import EnhancedQuotationModule
from src.evaluation.evaluation import PipelineEvaluator
from src.analysis.metrics import comprehensive_metric

logger = logging.getLogger(__name__)

class QuotationStructure:
    """Defines the exact structure for quotation analysis output."""
    def __init__(self):
        self.structure = {
            "transcript_info": {
                "transcript_chunk": "",                    # Selected transcript content
                "research_objectives": "",                 # Research goals guiding analysis
                "theoretical_framework": {
                    "theory": "",                         # Primary theoretical approach
                    "philosophical_approach": "",          # Philosophical foundation
                    "rationale": ""                       # Justification for approach
                }
            },
            "retrieved_chunks": [],                       # Retrieved document chunks
            "retrieved_chunks_count": 0,                  # Count of retrieved chunks
            "filtered_chunks_count": 0,                   # Count of filtered chunks
            "contextualized_contents": [],                # List of contextualized contents
            "used_chunk_ids": [],                        # List of used chunk IDs
            "quotations": [],                            # List of analyzed quotations
            "analysis": {
                "philosophical_underpinning": "",         # Analysis approach
                "patterns_identified": [""],              # Key patterns found
                "theoretical_interpretation": "",         # Framework application
                "methodological_reflection": {
                    "pattern_robustness": "",            # Pattern evidence
                    "theoretical_alignment": "",          # Framework fit
                    "researcher_reflexivity": ""         # Interpretation awareness
                },
                "practical_implications": ""             # Applied insights
            },
            "answer": {
                "summary": "",                           # Key findings
                "theoretical_contribution": "",          # Theory advancement
                "methodological_contribution": {
                    "approach": "",                     # Method used
                    "pattern_validity": "",             # Evidence quality
                    "theoretical_integration": ""       # Theory-data synthesis
                }
            }
        }

    def create_quotation(self) -> Dict[str, Any]:
        """Create a properly structured quotation entry."""
        return {
            "quotation": "",                             # Exact quote text
            "creswell_category": "",                     # longer/discrete/embedded
            "classification": "",                        # Content type
            "context": {
                "preceding_question": "",                # Prior question
                "situation": "",                         # Context description
                "pattern_representation": ""             # Pattern linkage
            },
            "analysis_value": {
                "relevance": "",                        # Research objective alignment
                "pattern_support": "",                  # Pattern evidence
                "theoretical_alignment": ""             # Framework connection
            }
        }

    def get_empty_structure(self) -> Dict[str, Any]:
        """Return a copy of the empty structure."""
        return json.loads(json.dumps(self.structure))  # Deep copy

def validate_queries(transcripts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Validates the structure of input transcripts."""
    valid_transcripts = []
    required_fields = ['transcript_chunk', 'research_objectives', 'theoretical_framework']
    framework_fields = ['theory', 'philosophical_approach', 'rationale']
    
    for idx, transcript in enumerate(transcripts):
        # Check for required fields
        if not all(field in transcript for field in required_fields):
            logger.warning(f"Transcript at index {idx} missing required fields. Skipping.")
            continue
            
        # Validate transcript chunk
        if not transcript['transcript_chunk'].strip():
            logger.warning(f"Transcript at index {idx} has empty transcript_chunk. Skipping.")
            continue
            
        # Validate theoretical framework structure
        framework = transcript.get('theoretical_framework', {})
        if not isinstance(framework, dict) or not all(field in framework for field in framework_fields):
            logger.warning(f"Transcript at index {idx} has invalid theoretical_framework structure. Skipping.")
            continue
            
        valid_transcripts.append(transcript)

    logger.info(f"Validated {len(valid_transcripts)} transcripts out of {len(transcripts)} provided.")
    return valid_transcripts

async def process_single_transcript(
    transcript_item: Dict[str, Any],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    module: dspy.Module
) -> Dict[str, Any]:
    """
    Processes a single transcript chunk with exact output structure.
    """
    structure = QuotationStructure()
    result = structure.get_empty_structure()
    
    try:
        # Extract transcript information
        transcript_chunk = transcript_item.get('transcript_chunk', '').strip()
        if not transcript_chunk:
            logger.warning("Empty transcript chunk provided.")
            return result

        # Update transcript info
        result['transcript_info'].update({
            'transcript_chunk': transcript_chunk,
            'research_objectives': transcript_item.get('research_objectives', 'N/A'),
            'theoretical_framework': transcript_item.get('theoretical_framework', {
                "theory": "N/A",
                "philosophical_approach": "N/A",
                "rationale": "N/A"
            })
        })

        # Retrieve and process chunks
        retrieved_chunks = retrieve_documents(transcript_chunk, db, es_bm25, k)
        result['retrieved_chunks'] = retrieved_chunks
        result['retrieved_chunks_count'] = len(retrieved_chunks)

        # Filter chunks
        filtered_chunks = [chunk for chunk in retrieved_chunks if chunk['score'] >= 0.7]
        result['filtered_chunks_count'] = len(filtered_chunks)

        # Process contextualized contents
        contextualized_contents = [
            chunk['chunk'].get('contextualized_content', 'N/A') 
            for chunk in filtered_chunks
        ]
        result['contextualized_contents'] = contextualized_contents
        result['used_chunk_ids'] = [chunk['chunk']['chunk_id'] for chunk in filtered_chunks]

        # Process with module
        module_response = module.forward(
            research_objectives=result['transcript_info']['research_objectives'],
            transcript_chunk=transcript_chunk,
            contextualized_contents=contextualized_contents,
            theoretical_framework=result['transcript_info']['theoretical_framework']
        )

        # Update with module response
        if module_response:
            # Process quotations
            result['quotations'] = []
            for q in module_response.get('quotations', []):
                quotation = structure.create_quotation()
                quotation.update({
                    'quotation': q.get('quotation', 'N/A'),
                    'creswell_category': q.get('creswell_category', 'N/A'),
                    'classification': q.get('classification', 'N/A'),
                    'context': q.get('context', {
                        "preceding_question": "N/A",
                        "situation": "N/A",
                        "pattern_representation": "N/A"
                    }),
                    'analysis_value': q.get('analysis_value', {
                        "relevance": "N/A",
                        "pattern_support": "N/A",
                        "theoretical_alignment": "N/A"
                    })
                })
                result['quotations'].append(quotation)

            # Update analysis and answer sections
            if 'analysis' in module_response:
                analysis = module_response['analysis']
                result['analysis'].update({
                    'philosophical_underpinning': analysis.get('philosophical_underpinning', 'N/A'),
                    'patterns_identified': analysis.get('patterns_identified', ["N/A"]),
                    'theoretical_interpretation': analysis.get('theoretical_interpretation', 'N/A'),
                    'methodological_reflection': analysis.get('methodological_reflection', {
                        "pattern_robustness": "N/A",
                        "theoretical_alignment": "N/A",
                        "researcher_reflexivity": "N/A"
                    }),
                    'practical_implications': analysis.get('practical_implications', 'N/A')
                })
            if 'answer' in module_response:
                answer = module_response['answer']
                result['answer'].update({
                    'summary': answer.get('summary', 'N/A'),
                    'theoretical_contribution': answer.get('theoretical_contribution', 'N/A'),
                    'methodological_contribution': answer.get('methodological_contribution', {
                        "approach": "N/A",
                        "pattern_validity": "N/A",
                        "theoretical_integration": "N/A"
                    })
                })

        if not result['quotations']:
            logger.warning(f"No quotations selected for transcript chunk: '{transcript_chunk[:100]}...'")
            result['answer']['summary'] = "No relevant quotations were found to generate an answer."

        logger.info(f"Selected {len(result['quotations'])} quotations for transcript chunk.")
        return result

    except Exception as e:
        logger.error(f"Error processing transcript: {e}", exc_info=True)
        return result

def retrieve_documents(
    transcript_chunk: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int
) -> List[Dict[str, Any]]:
    """Retrieves documents using multi-stage retrieval."""
    try:
        logger.debug(f"Retrieving documents for transcript chunk: '{transcript_chunk[:100]}...'")
        final_results = multi_stage_retrieval(transcript_chunk, db, es_bm25, k)
        logger.debug(f"Retrieved {len(final_results)} results")
        return final_results
    except Exception as e:
        logger.error(f"Error retrieving documents: {e}", exc_info=True)
        return []

def save_results(results: List[Dict[str, Any]], output_file: str):
    """Saves results with exact structure."""
    try:
        with open(output_file, 'w', encoding='utf-8') as outfile:
            json.dump(results, outfile, indent=2, ensure_ascii=False)
        logger.info(f"Results saved to '{output_file}'")
    except Exception as e:
        logger.error(f"Error saving results: {e}", exc_info=True)

@handle_exceptions
async def process_queries(
    transcripts: List[Dict[str, Any]],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    output_file: str,
    optimized_program: dspy.Program,
    module: dspy.Module
):
    """
    Process transcripts with exact output structure.
    """
    logger.info(f"Processing {len(transcripts)} transcripts")
    
    all_results = []
    try:
        for idx, transcript_item in enumerate(tqdm(transcripts, desc="Processing transcripts")):
            try:
                result = await process_single_transcript(
                    transcript_item,
                    db,
                    es_bm25,
                    k,
                    module
                )
                if result:
                    all_results.append(result)
            except Exception as e:
                logger.error(f"Error processing transcript at index {idx}: {e}", exc_info=True)

        save_results(all_results, output_file)

    except KeyboardInterrupt:
        logger.warning("Process interrupted. Saving partial results.")
        save_results(all_results, output_file)
        raise
    except Exception as e:
        logger.error(f"Error in process_queries: {e}", exc_info=True)
        raise

    return all_results


################################################################################
# Module: retrieval
################################################################################


# File: retrieval/__init__.py
#------------------------------------------------------------------------------
"""
Retrieval functionality for searching and ranking content.
"""

from .query_generator import QueryGeneratorSignature
from .retrieval import hybrid_retrieval, multi_stage_retrieval
from .reranking import retrieve_with_reranking
from .reranker import SentenceTransformerReRanker

__all__ = [
    'QueryGeneratorSignature',
    'hybrid_retrieval',
    'multi_stage_retrieval',
    'retrieve_with_reranking',
    'SentenceTransformerReRanker'
]

# File: retrieval/query_generator.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Example/src/query_generator.py
# query_generator.py
import dspy
import logging
from typing import Dict

from src.utils.logger import setup_logging

logger = logging.getLogger(__name__)

class QueryGeneratorSignature(dspy.Signature):
    question: str = dspy.InputField(desc="The original user question.")
    context: str = dspy.InputField(desc="The accumulated context from previous retrievals.")
    new_query: str = dspy.OutputField(desc="The generated query for the next retrieval hop.")

    def forward(self, question: str, context: str) -> Dict[str, str]:
        try:
            if not question or not context:
                raise ValueError("Both 'question' and 'context' must be provided and non-empty.")
            
            prompt = (
                f"Given the question: '{question}'\n"
                f"and the context retrieved so far:\n{context}\n"
                "Generate a search query that will help find additional information needed to answer the question."
            )
            new_query = self.language_model.generate(
                prompt=prompt,
                max_tokens=50,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=["\n"]
            ).strip()
            logger.info(f"Generated new query: '{new_query}'")
            return {"new_query": new_query}
        except ValueError as ve:
            logger.error(f"ValueError in QueryGeneratorSignature.forward: {ve}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question
        except Exception as e:
            logger.error(f"Error in QueryGeneratorSignature.forward: {e}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question


# File: retrieval/reranker.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util
import torch

from src.utils.logger import setup_logging
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class SentenceTransformerReRanker:
    """
    Re-ranker using Sentence Transformers for semantic similarity.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        """
        Initializes the Sentence Transformer re-ranker.
        
        Args:
            model_name (str): Pre-trained Sentence Transformer model name.
            device (str, optional): Device to run the model on ('cuda', 'mps', or 'cpu'). Defaults to automatic selection.
        """
        self.model = SentenceTransformer(model_name)
        
        # Set device
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'.")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Re-ranks documents based on semantic similarity to the query.
        
        Args:
            query (str): The search query.
            documents (List[str]): List of document contents to re-rank.
            top_k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
        
        Returns:
            List[Dict[str, Any]]: List of re-ranked documents with similarity scores.
        """
        if not query or not documents:
            logger.warning("Query and documents must be provided for re-ranking.")
            return []

        try:
            logger.debug("Encoding query and documents.")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities.")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)  # Adjust k to the number of available documents
            if actual_k == 0:
                logger.warning("No documents available for re-ranking.")
                return []
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}.")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            logger.info(f"Re-ranked {actual_k} documents based on semantic similarity.")
            return re_ranked_docs
        except Exception as e:
            logger.error(f"Error during re-ranking with Sentence Transformers: {e}", exc_info=True)
            return []

def rerank_documents_sentence_transformer(query: str, retrieved_docs: List[Dict[str, Any]], k: int = 20) -> List[Dict[str, Any]]:
    """
    Re-ranks the retrieved documents using Sentence Transformers.
    
    Args:
        query (str): The search query.
        retrieved_docs (List[Dict[str, Any]]): List of retrieved documents.
        k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.info(f"Starting re-ranking of {len(retrieved_docs)} documents for query: '{query}' using Sentence Transformers.")
    if not query or not retrieved_docs:
        logger.warning(f"Query and retrieved documents must be provided for re-ranking.")
        return []
    try:
        # Initialize the re-ranker
        reranker = SentenceTransformerReRanker()
        
        # Extract document contents
        documents = [doc['chunk']['original_content'] for doc in retrieved_docs]
        
        # Perform re-ranking
        re_ranked = reranker.rerank(query, documents, top_k=k)
        
        # Attach scores back to the documents
        re_ranked_docs = []
        for r, original_doc in zip(re_ranked, retrieved_docs[:len(re_ranked)]):  # Ensure matching length
            re_ranked_docs.append({
                "chunk": original_doc['chunk'],
                "score": r['score']
            })
        
        logger.info(f"Re-ranking completed using Sentence Transformers. Top {k} documents selected.")
        return re_ranked_docs
    except Exception as e:
        logger.error(f"Error during document re-ranking with Sentence Transformers: {e}", exc_info=True)
        return retrieved_docs[:k]  # Fallback to original ranking if re-ranking fails


# File: retrieval/reranking.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Example/src/reranking.py
import logging
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util
import torch
import time

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.retrieval.retrieval import hybrid_retrieval
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class SentenceTransformerReRanker:
    """
    Re-ranker using Sentence Transformers for semantic similarity.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        """
        Initializes the Sentence Transformer re-ranker.
        
        Args:
            model_name (str): Pre-trained Sentence Transformer model name.
            device (str, optional): Device to run the model on ('cuda', 'mps', or 'cpu'). Defaults to automatic selection.
        """
        self.model = SentenceTransformer(model_name)
        
        # Set device
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'.")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Re-ranks documents based on semantic similarity to the query.
        
        Args:
            query (str): The search query.
            documents (List[str]): List of document contents to re-rank.
            top_k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
        
        Returns:
            List[Dict[str, Any]]: List of re-ranked documents with similarity scores.
        """
        if not query or not documents:
            logger.warning("Query and documents must be provided for re-ranking.")
            return []

        try:
            logger.debug("Encoding query and documents.")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities.")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)  # Adjust k to the number of available documents
            if actual_k == 0:
                logger.warning("No documents available for re-ranking.")
                return []
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}.")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            logger.info(f"Re-ranked {actual_k} documents based on semantic similarity.")
            return re_ranked_docs
        except Exception as e:
            logger.error(f"Error during re-ranking with Sentence Transformers: {e}", exc_info=True)
            return []

def rerank_results(query: str, retrieved_docs: List[Dict[str, Any]], k: int = 20) -> List[Dict[str, Any]]:
    """
    Re-ranks the retrieved documents using Sentence Transformers.
    
    Args:
        query (str): The search query.
        retrieved_docs (List[Dict[str, Any]]): List of retrieved documents.
        k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.info(f"Starting re-ranking of {len(retrieved_docs)} documents for query: '{query}' using Sentence Transformers.")
    if not query or not retrieved_docs:
        logger.warning(f"Query and retrieved documents must be provided for re-ranking.")
        return []
    try:
        # Initialize the re-ranker
        reranker = SentenceTransformerReRanker()
        
        # Extract document contents
        documents = [doc['chunk']['original_content'] for doc in retrieved_docs]
        
        # Perform re-ranking
        re_ranked = reranker.rerank(query, documents, top_k=k)
        
        # Attach scores back to the documents
        re_ranked_docs = []
        for r, original_doc in zip(re_ranked, retrieved_docs[:len(re_ranked)]):  # Ensure matching length
            re_ranked_docs.append({
                "chunk": original_doc['chunk'],
                "score": r['score']
            })
        
        logger.info(f"Re-ranking completed using Sentence Transformers. Top {k} documents selected.")
        return re_ranked_docs
    except Exception as e:
        logger.error(f"Error during document re-ranking with Sentence Transformers: {e}", exc_info=True)
        return retrieved_docs[:k]  # Fallback to original ranking if re-ranking fails

def retrieve_with_reranking(query: str, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int) -> List[Dict[str, Any]]:
    """
    Retrieves documents using hybrid retrieval and re-ranks them using Sentence Transformers.
    
    Args:
        query (str): The search query.
        db (ContextualVectorDB): Contextual vector database instance.
        es_bm25 (ElasticsearchBM25): Elasticsearch BM25 instance.
        k (int): Number of top documents to retrieve.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.debug(f"Entering retrieve_with_reranking method with query='{query}' and k={k}.")
    start_time = time.time()

    try:
        logger.debug(f"Performing hybrid retrieval for query: '{query}'")
        initial_results = hybrid_retrieval(query, db, es_bm25, k=k*10)
        logger.debug(f"Initial hybrid retrieval returned {len(initial_results)} results.")

        # **Add logging for all initial chunks retrieved**
        logger.info(f"Total chunks retrieved during hybrid retrieval for query '{query}': {len(initial_results)}")
        logger.info(f"Chunk IDs retrieved during hybrid retrieval: {[res['chunk']['chunk_id'] for res in initial_results]}")

        if not initial_results:
            logger.warning(f"No initial results retrieved for query '{query}'. Skipping reranking.")
            return []

        # Re-rank the retrieved documents
        final_results = rerank_results(query, initial_results, top_k=5)  # Set top_k to 5

    except Exception as e:
        logger.error(f"Error during retrieval or re-ranking for query '{query}': {e}", exc_info=True)
        return []

    end_time = time.time()
    logger.debug(f"Exiting retrieve_with_reranking method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results


# File: retrieval/retrieval.py
#------------------------------------------------------------------------------
# File: retrieval.py
import logging
import time
from typing import List, Dict, Any
import dspy

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.retrieval.query_generator import QueryGeneratorSignature
from src.utils.utils import compute_similarity
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)


def hybrid_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    semantic_weight: float = 0.2,
    bm25_content_weight: float = 0.2,
    bm25_contextual_weight: float = 0.6,
    min_chunks: int = 1
) -> List[Dict[str, Any]]:
    """
    Performs hybrid retrieval by combining FAISS semantic search and dual BM25 contextual search using Reciprocal Rank Fusion.
    """
    logger.debug(
        f"Entering hybrid_retrieval with query='{query}', k={k}, "
        f"semantic_weight={semantic_weight}, bm25_content_weight={bm25_content_weight}, "
        f"bm25_contextual_weight={bm25_contextual_weight}, min_chunks={min_chunks}."
    )
    start_time = time.time()
    num_chunks_to_recall = k * 10  # Retrieve more to improve chances

    # Initialize reciprocal rank fusion score dictionary
    chunk_id_to_score = {}

    while True:
        # Semantic search
        logger.debug(f"Performing semantic search using FAISS for query: '{query}'")
        semantic_results = db.search(query, k=num_chunks_to_recall)
        ranked_semantic = [result['chunk_id'] for result in semantic_results]
        semantic_scores = {result['chunk_id']: result['score'] for result in semantic_results}
        logger.debug(f"Semantic search retrieved {len(ranked_semantic)} chunk IDs.")

        # BM25 search on 'content'
        logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        bm25_content_results = es_bm25.search_content(query, k=num_chunks_to_recall)
        ranked_bm25_content = [result['chunk_id'] for result in bm25_content_results]
        bm25_content_scores = {result['chunk_id']: result['score'] for result in bm25_content_results}
        logger.debug(f"BM25 'content' search retrieved {len(ranked_bm25_content)} chunk IDs.")

        # BM25 search on 'contextualized_content'
        logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        bm25_contextual_results = es_bm25.search_contextualized(query, k=num_chunks_to_recall)
        ranked_bm25_contextual = [result['chunk_id'] for result in bm25_contextual_results]
        bm25_contextual_scores = {result['chunk_id']: result['score'] for result in bm25_contextual_results}
        logger.debug(f"BM25 'contextualized_content' search retrieved {len(ranked_bm25_contextual)} chunk IDs.")

        # Combine all unique chunk IDs
        chunk_ids = list(set(ranked_semantic + ranked_bm25_content + ranked_bm25_contextual))
        logger.debug(f"Total unique chunk IDs after combining: {len(chunk_ids)}")

        # Calculate Reciprocal Rank Fusion scores
        for chunk_id in chunk_ids:
            score = 0
            if chunk_id in ranked_semantic:
                index = ranked_semantic.index(chunk_id)
                score += semantic_weight * (1 / (index + 1))
                logger.debug(
                    f"Added semantic RRF score for chunk_id {chunk_id}: "
                    f"{semantic_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_content:
                index = ranked_bm25_content.index(chunk_id)
                score += bm25_content_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_content_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_contextual:
                index = ranked_bm25_contextual.index(chunk_id)
                score += bm25_contextual_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'contextualized_content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_contextual_weight * (1 / (index + 1))}"
                )
            chunk_id_to_score[chunk_id] = score

        # Sort chunk IDs by their RRF scores in descending order
        sorted_chunk_ids = sorted(
            chunk_id_to_score.keys(),
            key=lambda x: chunk_id_to_score[x],
            reverse=True
        )
        logger.debug(f"Sorted chunk IDs based on RRF scores.")

        # Select top k chunks, ensuring at least min_chunks are returned
        final_results = []
        filtered_count = 0
        for chunk_id in sorted_chunk_ids[:k]:
            chunk_metadata = next(
                (chunk for chunk in db.metadata if chunk['chunk_id'] == chunk_id),
                None
            )
            if not chunk_metadata:
                filtered_count += 1
                logger.warning(f"Chunk metadata not found for chunk_id {chunk_id}")
                continue
            final_results.append({
                'chunk': chunk_metadata,
                'score': chunk_id_to_score[chunk_id]
            })

        logger.info(f"Filtered {filtered_count} chunks due to missing metadata.")
        logger.info(
            f"Total chunks retrieved after filtering: {len(final_results)} "
            f"(required min_chunks={min_chunks})"
        )

        if len(final_results) >= min_chunks or k >= num_chunks_to_recall:
            break
        else:
            k += 5  # Increment k to retrieve more chunks
            logger.info(
                f"Number of retrieved chunks ({len(final_results)}) is less than min_chunks ({min_chunks}). "
                f"Increasing k to {k} and retrying retrieval."
            )

    logger.debug(f"Hybrid retrieval returning {len(final_results)} chunks.")
    logger.info(
        f"Chunks used for hybrid retrieval for query '{query}': "
        f"[{', '.join([res['chunk']['chunk_id'] for res in final_results])}]"
    )
    end_time = time.time()
    logger.debug(f"Exiting hybrid_retrieval method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results


def multi_stage_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    max_hops: int = 3,
    max_results: int = 5,
    similarity_threshold: float = 0.9
) -> List[Dict[str, Any]]:
    accumulated_context = ""
    all_retrieved_chunks = {}
    current_query = query
    previous_query = ""
    query_generator = dspy.TypedChainOfThought(QueryGeneratorSignature)

    for hop in range(max_hops):
        logger.info(f"Starting hop {hop+1} with query: '{current_query}'")

        retrieved_chunks = hybrid_retrieval(current_query, db, es_bm25, k)

        # Update accumulated retrieved chunks
        for chunk in retrieved_chunks:
            chunk_id = chunk['chunk']['chunk_id']
            if chunk_id not in all_retrieved_chunks:
                all_retrieved_chunks[chunk_id] = chunk

        # Check if we have reached the desired number of results
        if len(all_retrieved_chunks) >= max_results:
            logger.info(f"Retrieved sufficient chunks ({len(all_retrieved_chunks)}). Terminating.")
            break

        # Accumulate new context from retrieved chunks
        new_context = "\n\n".join([
            chunk['chunk'].get('contextualized_content', '') or chunk['chunk'].get('original_content', '')
            for chunk in retrieved_chunks
        ])
        accumulated_context += "\n\n" + new_context

        # Generate a new query based on the accumulated context
        response = query_generator(question=query, context=accumulated_context)
        new_query = response.get('new_query', '').strip()
        if not new_query:
            logger.info("No new query generated. Terminating multi-stage retrieval.")
            break

        # Compute similarity between the new query and the previous query
        similarity = compute_similarity(current_query, new_query)
        logger.debug(f"Similarity between queries: {similarity:.4f}")
        if similarity >= similarity_threshold:
            logger.info("New query is too similar to the current query. Terminating multi-stage retrieval.")
            break

        # Update queries for the next iteration
        previous_query = current_query
        current_query = new_query

    # Sort and return the top results
    final_results = sorted(
        all_retrieved_chunks.values(),
        key=lambda x: x['score'],
        reverse=True
    )[:max_results]

    logger.info(f"Multi-stage retrieval completed with {len(final_results)} chunks.")
    return final_results



################################################################################
# Module: utils
################################################################################


# File: utils/__init__.py
#------------------------------------------------------------------------------
"""
Utility functions and helpers.
"""

from decorators import handle_exceptions
from .logger import setup_logging
from .utils import check_answer_length, compute_similarity
from .validation_functions import validate_relevance, validate_quality, validate_context_clarity

__all__ = [
    'handle_exceptions',
    'setup_logging',
    'check_answer_length',
    'compute_similarity',
    'validate_relevance',
    'validate_quality',
    'validate_context_clarity'
]

# File: utils/history_utils.py
#------------------------------------------------------------------------------
# Add this to src/utils/history_utils.py

import os
import json
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class HistoryManager:
    def __init__(self, base_dir='interaction_histories'):
        self.base_dir = base_dir
        self._ensure_directory()

    def _ensure_directory(self):
        """Ensures the history directory exists."""
        try:
            os.makedirs(self.base_dir, exist_ok=True)
            logger.info(f"Ensured history directory exists at: {self.base_dir}")
        except Exception as e:
            logger.error(f"Error creating history directory: {e}")

    def save_history(self, history, prefix='quotation'):
        """Saves a history entry with timestamp."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}_interaction_{timestamp}.json"
            filepath = os.path.join(self.base_dir, filename)

            history_data = {
                'timestamp': timestamp,
                'history': history
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2)
            
            logger.info(f"Saved interaction history to: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"Error saving history: {e}")
            return None

    def get_latest_history(self, prefix='quotation'):
        """Gets the most recent history file."""
        try:
            files = [f for f in os.listdir(self.base_dir) if f.startswith(prefix)]
            if not files:
                return None
            latest_file = max(files, key=lambda x: os.path.getctime(os.path.join(self.base_dir, x)))
            with open(os.path.join(self.base_dir, latest_file), 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error getting latest history: {e}")
            return None

# File: utils/logger.py
#------------------------------------------------------------------------------
# File: src/utils/logger.py
import logging
import logging.config
import os
import yaml

def setup_logging(default_path='config/logging_config.yaml', default_level=logging.INFO):
    """
    Setup logging configuration from a YAML file.
    Ensures that the log directory exists before configuring handlers.
    """
    if os.path.exists(default_path):
        with open(default_path, 'r') as f:
            config = yaml.safe_load(f.read())

        # Extract all file handler paths to ensure directories exist
        handlers = config.get('handlers', {})
        for handler_name, handler in handlers.items():
            if 'filename' in handler:
                log_file = handler['filename']
                log_dir = os.path.dirname(log_file)
                if log_dir and not os.path.exists(log_dir):
                    try:
                        os.makedirs(log_dir, exist_ok=True)
                        print(f"Created log directory: {log_dir}")
                    except Exception as e:
                        print(f"Failed to create log directory '{log_dir}': {e}")

        # Apply the logging configuration
        logging.config.dictConfig(config)
    else:
        # If the logging configuration file is missing, use basic configuration
        logging.basicConfig(level=default_level)
        logging.warning(f"Logging configuration file not found at '{default_path}'. Using basic configuration.")

# Initialize logging when this module is imported
setup_logging()


# File: utils/utils.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/utils.py

import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

def check_answer_length(answer: str, max_length: int = 500) -> bool:
    """
    Checks if the answer length is within the specified limit.

    Args:
        answer (str): The generated answer to evaluate.
        max_length (int, optional): The maximum allowed length for the answer. Defaults to 500.

    Returns:
        bool: True if the answer length is within the limit, False otherwise.
    """
    return len(answer) <= max_length

def compute_similarity(query1: str, query2: str) -> float:
    """
    Computes cosine similarity between two queries using TF-IDF vectorization.

    Args:
        query1 (str): The first query string.
        query2 (str): The second query string.

    Returns:
        float: Cosine similarity score between query1 and query2.
    """
    try:
        vectorizer = TfidfVectorizer().fit_transform([query1, query2])
        vectors = vectorizer.toarray()
        similarity = cosine_similarity([vectors[0]], [vectors[1]])[0][0]
        return similarity
    except Exception as e:
        logger.error(f"Error computing similarity: {e}", exc_info=True)
        return 0.0


# File: utils/validation_functions.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/validation_functions.py

import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def validate_relevance(quotations: List[str], research_objectives: str) -> bool:
    """
    Validates that each quotation is relevant to the research objectives.
    """
    try:
        for quote in quotations:
            # Simple keyword matching; can be enhanced with NLP techniques
            if not any(obj.lower() in quote.lower() for obj in research_objectives.split()):
                logger.debug(f"Quotation '{quote}' is not relevant to the research objectives.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_relevance: {e}", exc_info=True)
        return False

def validate_quality(quotations: List[Dict[str, Any]]) -> bool:
    """
    Validates the quality and representation of each quotation.
    """
    try:
        for quote in quotations:
            if len(quote["quotation"].strip()) < 10:  # Example quality check
                logger.debug(f"Quotation '{quote['QUOTE']}' is too short to be considered high quality.")
                return False
            # Additional quality checks can be added here
        return True
    except Exception as e:
        logger.error(f"Error in validate_quality: {e}", exc_info=True)
        return False

def validate_context_clarity(quotations: List[str], context: str) -> bool:
    """
    Validates that each quotation is clear and has sufficient context.
    """
    try:
        for quote in quotations:
            if quote.lower() not in context.lower():
                logger.debug(f"Quotation '{quote}' lacks sufficient context.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_context_clarity: {e}", exc_info=True)
        return False

