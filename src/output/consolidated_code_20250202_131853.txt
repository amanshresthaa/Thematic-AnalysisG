# Consolidated Source Code
# ==============================================================================



################################################################################
# Module: root
################################################################################


# File: __init__.py
#------------------------------------------------------------------------------
"""
Core functionality for the thematic analysis package.
Contains database and client implementations.
"""

from .contextual_vector_db import ContextualVectorDB
from .elasticsearch_bm25 import ElasticsearchBM25
from .openai_client import OpenAIClient

__all__ = ['ContextualVectorDB', 'ElasticsearchBM25', 'OpenAIClient']

# File: contextual_vector_db.py
#------------------------------------------------------------------------------
import os
import pickle
import numpy as np
import threading
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from dotenv import load_dotenv
import logging
import faiss
import dspy
import time

from src.core.openai_client import OpenAIClient
from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)


class SituateContextSignature(dspy.Signature):
    doc = dspy.InputField(desc="Full document content")
    chunk = dspy.InputField(desc="Specific chunk content")
    reasoning = dspy.OutputField(desc="Chain of thought reasoning")
    contextualized_content = dspy.OutputField(desc="Contextualized content for the chunk")


class SituateContext(dspy.Module):
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(SituateContextSignature)
        logger.debug("SituateContext module initialized.")

    def forward(self, doc: str, chunk: str):
        prompt = f"""
                <document>
                {doc}
                </document>
            

                CHUNK_CONTEXT_PROMPT = 
                Here is the chunk we want to situate within the whole document
                <chunk>
                {chunk}
                </chunk>

                Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
                Answer only with the succinct context and nothing else.
    """

        logger.debug("Generating contextualized content for a chunk.")
        return self.chain(doc=doc, chunk=chunk, prompt=prompt)


class ContextualVectorDB:
    def __init__(self, name: str, openai_api_key: str = None):
        if openai_api_key is None:
            load_dotenv()
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                logger.critical("OPENAI_API_KEY is not set in environment variables.")
                raise ValueError("OPENAI_API_KEY is required but not set.")

        self.name = name
        self.embeddings = []
        self.metadata = []
        self.db_path = f"./data/{name}/contextual_vector_db.pkl"
        self.faiss_index_path = f"./data/{name}/faiss_index.bin"

        self.client = OpenAIClient(api_key=openai_api_key)
        logger.debug(f"Initialized OpenAIClient for ContextualVectorDB '{self.name}'.")

        # Initialize FAISS index attribute
        self.index = None

    def situate_context(self, doc: str, chunk: str) -> Tuple[str, Any]:
        logger.debug(f"Entering situate_context with doc length={len(doc)} and chunk length={len(chunk)}.")
        try:
            if not hasattr(self, 'situate_context_module'):
                self.situate_context_module = SituateContext()
                logger.debug("Initialized SituateContext module.")

            start_time = time.time()
            response = self.situate_context_module(doc=doc, chunk=chunk)
            elapsed_time = time.time() - start_time
            logger.debug(f"Generated contextualized_content using DSPy in {elapsed_time:.2f} seconds.")

            contextualized_content = response.contextualized_content
            usage_metrics = {}  # Placeholder for actual usage metrics if available
            return contextualized_content, usage_metrics
        except Exception as e:
            logger.error(f"Error during DSPy situate_context: {e}", exc_info=True)
            return "", None

    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 8):
        logger.debug("Entering load_data method.")
        if self.embeddings and self.metadata and os.path.exists(self.faiss_index_path):
            logger.info("Vector database is already loaded. Skipping data loading.")
            return
        if os.path.exists(self.db_path) and os.path.exists(self.faiss_index_path):
            logger.info("Loading vector database and FAISS index from disk.")
            self.load_db()
            self.load_faiss_index()
            return

        texts_to_embed, metadata = self._process_dataset(dataset, parallel_threads)

        if not texts_to_embed:
            logger.warning("No texts to embed after processing the dataset.")
            return

        self._embed_and_store(texts_to_embed, metadata, max_workers=parallel_threads)
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}.")

    def _process_dataset(self, dataset: List[Dict[str, Any]], parallel_threads: int) -> Tuple[List[str], List[Dict[str, Any]]]:
        texts_to_embed = []
        metadata = []
        total_chunks = sum(len(doc.get('chunks', [])) for doc in dataset)
        logger.info(f"Total chunks to process: {total_chunks}.")

        logger.info(f"Processing {total_chunks} chunks with {parallel_threads} threads.")
        try:
            with ThreadPoolExecutor(max_workers=parallel_threads) as executor:
                futures = []
                for doc in dataset:
                    for chunk in doc.get('chunks', []):
                        futures.append(executor.submit(self._generate_contextualized_content, doc, chunk))

                for future in tqdm(as_completed(futures), total=total_chunks, desc="Processing chunks"):
                    result = future.result()
                    if result:
                        texts_to_embed.append(result['text_to_embed'])
                        metadata.append(result['metadata'])
            logger.debug(f"Completed processing all chunks.")
        except Exception as e:
            logger.error(f"Error during processing chunks: {e}", exc_info=True)
            return [], []

        return texts_to_embed, metadata

    def _generate_contextualized_content(self, doc: Dict[str, Any], chunk: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(doc, dict):
            logger.error(f"Document is not a dictionary: {doc}")
            return None

        if isinstance(chunk, dict):
            chunk_id = chunk.get('chunk_id')
            if not chunk_id:
                # Assign a unique chunk_id combining doc_id and chunk's index
                chunk_index = chunk.get('index', 0)
                chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_{chunk_index}"
                chunk['chunk_id'] = chunk_id
            content = chunk.get('content', '')
            original_index = chunk.get('original_index', chunk.get('index', 0))
        elif isinstance(chunk, str):
            # Handle case where chunk is a string
            content = chunk
            chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_0"
            original_index = 0
            logger.warning(f"Chunk is a string. Expected a dict. Assigning default values.")
        else:
            logger.error(f"Unsupported chunk type: {type(chunk)}. Skipping chunk.")
            return None

        logger.debug(f"Processing chunk_id='{chunk_id}' in doc_id='{doc.get('doc_id', 'unknown_doc_id')}'.")

        contextualized_text, usage = self.situate_context(doc.get('content', ''), content)
        if not contextualized_text:
            logger.warning(f"Contextualized content is empty for chunk_id='{chunk_id}'.")
            return None

        return {
            'text_to_embed': f"{content}\n\n{contextualized_text}",
            'metadata': {
                'doc_id': doc.get('doc_id', ''),
                'original_uuid': doc.get('original_uuid', ''),
                'chunk_id': chunk_id,
                'original_index': original_index,
                'original_content': content,
                'contextualized_content': contextualized_text
            }
        }

    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]], max_workers: int = 4):
        logger.debug("Entering _embed_and_store method.")
        batch_size = 128
        embeddings = []
        logger.info("Starting embedding generation.")

        # Define a helper function for embedding a batch
        def embed_batch(batch):
            try:
                logger.debug(f"Generating embeddings for batch of size {len(batch)}.")
                response = self.client.create_embeddings(
                    model="text-embedding-3-small",
                    input=batch
                )
                return [item['embedding'] for item in response['data']]
            except Exception as e:
                logger.error(f"Error during OpenAI embeddings for batch: {e}", exc_info=True)
                return []

        # Use ThreadPoolExecutor for parallel embedding
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i: i + batch_size]
                futures.append(executor.submit(embed_batch, batch))
            
            for future in as_completed(futures):
                embeddings_batch = future.result()
                if embeddings_batch:
                    embeddings.extend(embeddings_batch)
                    logger.debug(f"Processed a batch with {len(embeddings_batch)} embeddings.")

        if not embeddings:
            logger.warning("No embeddings were generated.")
            return

        self.embeddings = embeddings
        self.metadata = data
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Embedding generation completed. Total embeddings: {len(self.embeddings)}.")

    def _build_faiss_index(self):
        logger.debug("Entering _build_faiss_index method.")
        start_time = time.time()
        self.create_faiss_index()
        self.save_faiss_index()
        elapsed_time = time.time() - start_time
        logger.info(f"FAISS index built and saved in {elapsed_time:.2f} seconds.")

    def create_faiss_index(self):
        logger.debug("Entering create_faiss_index method.")
        try:
            if not self.embeddings:
                logger.error("No embeddings available to create FAISS index.")
                raise ValueError("Embeddings list is empty.")

            embedding_dim = len(self.embeddings[0])
            logger.info(f"Embedding dimension: {embedding_dim}.")
            embeddings_np = np.array(self.embeddings).astype('float32')
            faiss.normalize_L2(embeddings_np)
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.index.add(embeddings_np)
            logger.info(f"FAISS index created with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error creating FAISS index: {e}", exc_info=True)
            raise

    def save_faiss_index(self):
        logger.debug("Entering save_faiss_index method.")
        os.makedirs(os.path.dirname(self.faiss_index_path), exist_ok=True)
        try:
            faiss.write_index(self.index, self.faiss_index_path)
            logger.info(f"FAISS index saved to '{self.faiss_index_path}'.")
        except Exception as e:
            logger.error(f"Error saving FAISS index to '{self.faiss_index_path}': {e}", exc_info=True)

    def load_faiss_index(self):
        logger.debug("Entering load_faiss_index method.")
        if not os.path.exists(self.faiss_index_path):
            logger.error(f"FAISS index file not found at '{self.faiss_index_path}'.")
            raise ValueError("FAISS index file not found.")
        try:
            self.index = faiss.read_index(self.faiss_index_path)
            logger.info(f"FAISS index loaded from '{self.faiss_index_path}' with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error loading FAISS index from '{self.faiss_index_path}': {e}", exc_info=True)
            raise

    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        logger.debug(f"Entering search method with query='{query}' and k={k}.")
        if not self.embeddings or not self.metadata:
            logger.error("Embeddings or metadata are not loaded. Cannot perform search.")
            return []
        if not hasattr(self, 'index') or self.index is None:
            logger.error("FAISS index is not loaded.")
            return []

        try:
            start_time = time.time()
            logger.debug("Generating embedding for the query.")
            response = self.client.create_embeddings(
                model="text-embedding-3-small",
                input=[query]
            )
            query_embedding = response['data'][0]['embedding']
            elapsed_time = time.time() - start_time
            logger.debug(f"Generated embedding for query in {elapsed_time:.2f} seconds.")
        except Exception as e:
            logger.error(f"Error generating embedding for query '{query}': {e}", exc_info=True)
            return []

        query_embedding_np = np.array([query_embedding]).astype('float32')
        faiss.normalize_L2(query_embedding_np)

        logger.debug("Performing FAISS search.")
        try:
            start_time = time.time()
            distances, indices = self.index.search(query_embedding_np, k)
            elapsed_time = time.time() - start_time
            logger.debug(f"FAISS search completed in {elapsed_time:.2f} seconds.")
            indices = indices.flatten()
            distances = distances.flatten()
        except Exception as e:
            logger.error(f"Error during FAISS search: {e}", exc_info=True)
            return []

        top_results = []
        for idx, score in zip(indices, distances):
            if idx < len(self.metadata):
                meta = self.metadata[idx]
                result = {
                    "doc_id": meta['doc_id'],
                    "chunk_id": meta['chunk_id'],
                    "original_index": meta.get('original_index', 0),
                    "content": meta['original_content'],
                    "contextualized_content": meta.get('contextualized_content'),
                    "score": float(score),
                    "metadata": meta
                }
                top_results.append(result)
                logger.debug(f"Retrieved chunk_id='{meta['chunk_id']}' with score={score:.4f}.")
            else:
                logger.warning(f"Index {idx} out of bounds for metadata.")

        logger.info(f"FAISS search returned {len(top_results)} results for query: '{query}'.")
        logger.debug(f"Chunks retrieved: {[res['chunk_id'] for res in top_results]}.")
        return top_results

    def save_db(self):
        logger.debug("Entering save_db method.")
        data = {
            "metadata": self.metadata,
        }
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        try:
            with open(self.db_path, "wb") as file:
                pickle.dump(data, file)
            logger.info(f"Vector database metadata saved to '{self.db_path}'.")
        except Exception as e:
            logger.error(f"Error saving vector database metadata to '{self.db_path}': {e}", exc_info=True)

    def load_db(self):
        logger.debug("Entering load_db method.")
        if not os.path.exists(self.db_path):
            logger.error(f"Vector database file not found at '{self.db_path}'. Use load_data to create a new database.")
            raise ValueError("Vector database file not found.")
        try:
            with open(self.db_path, "rb") as file:
                data = pickle.load(file)
            self.metadata = data.get("metadata", [])
            logger.info(f"Vector database metadata loaded from '{self.db_path}' with {len(self.metadata)} entries.")
            logger.debug(f"Chunks loaded: {[meta['chunk_id'] for meta in self.metadata]}.")
        except Exception as e:
            logger.error(f"Error loading vector database metadata from '{self.db_path}': {e}", exc_info=True)
            raise


# File: elasticsearch_bm25.py
#------------------------------------------------------------------------------
# src/core/elasticsearch_bm25.py

import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk, scan
from elasticsearch.exceptions import NotFoundError, RequestError
import json
import os

from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)

class ElasticsearchBM25:
    
    def __init__(
        self, 
        index_name: str,
        es_host: str = "http://localhost:9200",
        logger: Optional[logging.Logger] = None
    ):
        self.logger = logger or logging.getLogger(__name__)
        self.index_name = index_name
        self.es_client = Elasticsearch(es_host)
        
        if not self.es_client.ping():
            raise ConnectionError(f"Failed to connect to Elasticsearch at {es_host}")
            
        self._create_index()
        
    def _create_index(self) -> None:
        index_settings = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "english"
                        },
                        "custom_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "porter_stem"]
                        }
                    }
                },
                "similarity": {
                    "custom_bm25": {
                        "type": "BM25",
                        "k1": 1.2,
                        "b": 0.75,
                    }
                },
                "index": {
                    "refresh_interval": "1s",
                    "number_of_shards": 1,
                    "number_of_replicas": 0
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "contextualized_content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "doc_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "original_index": {"type": "integer"},
                    "metadata": {"type": "object", "enabled": True}
                }
            }
        }
        
        index_data_dir = f"./data/{self.index_name}"
        os.makedirs(index_data_dir, exist_ok=True)
        self.logger.debug(f"Ensured existence of index data directory: {index_data_dir}")
        
        try:
            if self.es_client.indices.exists(index=self.index_name):
                self.logger.info(f"Index '{self.index_name}' already exists. Skipping creation.")
                # Avoid modifying immutable settings
            else:
                self.es_client.indices.create(
                    index=self.index_name,
                    body=index_settings
                )
                self.logger.info(f"Successfully created index: {self.index_name}")
        except Exception as e:
            self.logger.error(f"Failed to create/update index '{self.index_name}': {str(e)}")
            raise

    def index_documents(
        self,
        documents: List[Dict[str, Any]],
        batch_size: int = 500
    ) -> Tuple[int, List[Dict[str, Any]]]:
        if not documents:
            self.logger.warning("No documents provided for indexing")
            return 0, []

        failed_docs = []
        success_count = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            actions = [{
                "_index": self.index_name,
                "_source": {
                    "content": doc.get("original_content", ""),
                    "contextualized_content": doc.get("contextualized_content", ""),
                    "doc_id": doc.get("doc_id", ""),
                    "chunk_id": doc.get("chunk_id", ""),
                    "original_index": doc.get("original_index", 0),
                    "metadata": doc.get("metadata", {})
                }
            } for doc in batch]
            
            try:
                success, failed = bulk(
                    self.es_client,
                    actions,
                    raise_on_error=False,
                    raise_on_exception=False
                )
                success_count += success
                if failed:
                    for fail in failed:
                        failed_doc = batch[fail.get('index', 0)]
                        failed_docs.append(failed_doc)
                    self.logger.warning(f"Failed to index {len(failed)} documents in batch")
                    
            except Exception as e:
                self.logger.error(f"Batch indexing error: {str(e)}")
                failed_docs.extend(batch)
                
        self.es_client.indices.refresh(index=self.index_name)
        self.logger.info(f"Indexed {success_count}/{len(documents)} documents successfully")
        return success_count, failed_docs

    def search(
        self,
        query: str,
        k: int = 20,
        min_score: float = 0.1,
        fields: List[str] = None,
        operator: str = "or",
        minimum_should_match: str = "30%"
    ) -> List[Dict[str, Any]]:
        if not fields:
            fields = ["content^1", "contextualized_content^1.5"]
            
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "operator": operator,
                    "minimum_should_match": minimum_should_match,
                    "type": "best_fields",
                    "tie_breaker": 0.3
                }
            },
            "min_score": min_score,
            "size": k,
            "_source": True
        }

        try:
            search_body_json = json.dumps(search_body, indent=2)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")
        except Exception as e:
            self.logger.error(f"Error converting search body to JSON: {e}")
            search_body_json = str(search_body)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")

        try:
            response = self.es_client.search(
                index=self.index_name,
                body=search_body
            )
            
            hits = [{
                "doc_id": hit["_source"]["doc_id"],
                "chunk_id": hit["_source"]["chunk_id"],
                "content": hit["_source"]["content"],
                "contextualized_content": hit["_source"].get("contextualized_content"),
                "score": hit["_score"],
                "metadata": hit["_source"].get("metadata", {})
            } for hit in response["hits"]["hits"]]
            
            self.logger.debug(
                f"Search for '{query}' returned {len(hits)} results "
                f"(max_score: {response['hits'].get('max_score', 0)})"
            )
            return hits
            
        except Exception as e:
            self.logger.error(f"Search error: {str(e)}")
            raise

    def search_content(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["content^1"],
            operator="or",
            minimum_should_match="30%"
        )

    def search_contextualized(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["contextualized_content^1.5"],
            operator="or",
            minimum_should_match="30%"
        )


# File: openai_client.py
#------------------------------------------------------------------------------
import logging
import os
from typing import List, Dict, Any
from openai import OpenAI
# Initialize logger
logger = logging.getLogger(__name__)

class OpenAIClient:
    """
    Wrapper for OpenAI API interactions using the updated SDK.
    """
    def __init__(self, api_key: str):
        """
        Initializes the OpenAI client with the provided API key.
        
        Args:
            api_key (str): OpenAI API key.
        """
        if not api_key:
            logger.error("OpenAI API key is not provided.")
            raise ValueError("OpenAI API key must be provided.")
        self.client = OpenAI(api_key=api_key)
        logger.debug("OpenAI client initialized successfully.")

    def create_chat_completion(self, model: str, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> Dict[str, Any]:
        """
        Creates a chat completion using OpenAI's API.
        
        Args:
            model (str): Model name to use.
            messages (List[Dict[str, str]]): List of message dictionaries.
            max_tokens (int): Maximum number of tokens in the response.
            temperature (float): Sampling temperature.
        
        Returns:
            Dict[str, Any]: API response as a dictionary.
        """
        if not model or not messages:
            logger.error("Model and messages must be provided for chat completion.")
            raise ValueError("Model and messages must be provided for chat completion.")
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            logger.debug(f"Chat completion created successfully for model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating chat completion: {e}")
            raise

    def create_embeddings(self, model: str, input: List[str]) -> Dict[str, Any]:
        """
        Creates embeddings for the given input texts using OpenAI's API.
        
        Args:
            model (str): Embedding model to use.
            input (List[str]): List of input texts.
        
        Returns:
            Dict[str, Any]: API response containing embeddings.
        """
        if not model or not input:
            logger.error("Model and input must be provided for creating embeddings.")
            raise ValueError("Model and input must be provided for creating embeddings.")
        
        try:
            response = self.client.embeddings.create(
                model=model,
                input=input
            )
            logger.debug(f"Embeddings created successfully using model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            raise

