# Consolidated Source Code
# ==============================================================================



################################################################################
# Module: root
################################################################################


# File: __init__.py
#------------------------------------------------------------------------------



################################################################################
# Module: analysis
################################################################################


# File: analysis/__init__.py
#------------------------------------------------------------------------------
#init
"""
Analysis modules for thematic analysis.
Contains metrics and quotation selection functionality.
"""

from .metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from .select_quotation import EnhancedQuotationSignature
from .select_quotation_module import SelectQuotationModule

__all__ = [
    'comprehensive_metric',
    'is_answer_fully_correct',
    'factuality_metric',
    'EnhancedQuotationSignature',
    'SelectQuotationModule'
]

# File: analysis/base_analysis.py
#------------------------------------------------------------------------------
#analysis/base_analysis.py
import json
import logging
import re
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

class BaseAnalysisSignature:
    """
    Abstract base class for thematic analysis signatures.
    Provides common functionality for creating prompts, parsing responses,
    and validating codes, which can be extended by concrete analysis implementations.
    """
    def create_prompt(self, *args, **kwargs) -> str:
        """Abstract method to generate a prompt; to be implemented by subclass."""
        raise NotImplementedError("Subclasses must implement create_prompt.")

    def parse_response(self, response: str) -> Dict[str, Any]:
        """Extracts and parses the JSON content from the language model's response."""
        try:
            json_match = re.search(r"```json\s*(\{.*?\})\s*```", response, re.DOTALL)
            if not json_match:
                logger.error("No valid JSON found in the response.")
                logger.debug(f"Full response received: {response}")
                return {}
                
            json_string = json_match.group(1)
            return json.loads(json_string)
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}. Response: {response}")
            return {}
        except Exception as e:
            logger.error(f"Unexpected error during response parsing: {e}")
            return {}

    def validate_codes(self, codes: List[Dict[str, Any]], *args, **kwargs) -> None:
        """
        Validates the developed codes against various assertions.
        Should be extended if specific validation is needed.
        """
        # Base implementation is a no-op, subclasses should override as needed
        pass

    def handle_failed_validation(self, validation_error: Exception, *args, **kwargs) -> Dict[str, Any]:
        """
        Handles validation failures by attempting to refine the analysis.
        Should be implemented by subclasses for specific error handling.
        """
        logger.error(f"Validation failed: {validation_error}")
        return {}

    def preprocess_inputs(self, *args, **kwargs) -> Dict[str, Any]:
        """
        Preprocesses input data before analysis.
        Should be implemented by subclasses for specific preprocessing needs.
        """
        return {}

    def postprocess_outputs(self, outputs: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:
        """
        Postprocesses analysis outputs before returning.
        Should be implemented by subclasses for specific postprocessing needs.
        """
        return outputs

    def execute_analysis(self, *args, **kwargs) -> Dict[str, Any]:
        """
        Template method defining the overall analysis workflow.
        Subclasses should implement specific steps as needed.
        """
        try:
            # Preprocess inputs
            processed_inputs = self.preprocess_inputs(*args, **kwargs)
            
            # Create prompt
            prompt = self.create_prompt(**processed_inputs)
            
            # Generate response
            response = self.generate_response(prompt)
            
            # Parse response
            parsed_response = self.parse_response(response)
            
            # Validate results
            self.validate_codes(parsed_response.get("codes", []), *args, **kwargs)
            
            # Postprocess outputs
            final_output = self.postprocess_outputs(parsed_response, *args, **kwargs)
            
            return final_output
            
        except Exception as e:
            logger.error(f"Error in execute_analysis: {e}", exc_info=True)
            return {}

    def generate_response(self, prompt: str) -> str:
        """
        Abstract method to generate response from the language model.
        Should be implemented by subclasses based on their specific needs.
        """
        raise NotImplementedError("Subclasses must implement generate_response.")

# File: analysis/coding.py
#------------------------------------------------------------------------------
# analysis/coding.py
import logging
from typing import Dict, Any, List
import dspy
from dataclasses import dataclass
import json

from src.assertions_coding import (
    assert_robustness,
    assert_reflectiveness,
    assert_resplendence,
    assert_relevance,
    assert_radicality,
    assert_righteousness,
    assert_code_representation,
    assert_code_specificity,
    assert_code_relevance,
    assert_code_distinctiveness,
    run_all_coding_assertions
)
from analysis.base_analysis import BaseAnalysisSignature

logger = logging.getLogger(__name__)

@dataclass
class SixRsEvaluation:
    """Evaluation metrics for each dimension of the 6Rs framework."""
    robust: str
    reflective: str
    resplendent: str
    relevant: str
    radical: str
    righteous: str

class CodingAnalysisSignature(BaseAnalysisSignature, dspy.Signature):
    """
    Signature for comprehensive thematic coding analysis utilizing the 6Rs framework.
    Inherits common behaviors from BaseAnalysisSignature.
    """

    research_objectives: str = dspy.InputField(
        desc=(
            "A detailed statement of the study's overarching goals and specific research questions "
            "that direct the coding analysis. This should clearly articulate what the research aims to "
            "accomplish and the key questions it seeks to address, providing a foundation for the "
            "subsequent coding process."
        )
    )

    quotation: str = dspy.InputField(
        desc=(
            "The specific excerpt, passage, or segment selected from the data set for coding analysis. "
            "This quotation serves as the primary source text from which themes and codes will be derived."
        )
    )

    keywords: List[str] = dspy.InputField(
        desc="A curated list of keywords to guide the coding process."
    )

    contextualized_contents: List[str] = dspy.InputField(
        desc="Additional contextual information related to the quotation."
    )

    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc="The foundational theoretical framework that underpins the analysis."
    )

    coding_info: Dict[str, Any] = dspy.OutputField(
        desc="Comprehensive context and metadata related to the coding analysis."
    )

    codes: List[Dict[str, Any]] = dspy.OutputField(
        desc="A structured collection of developed codes with in-depth analysis."
    )

    analysis: Dict[str, Any] = dspy.OutputField(
        desc="An extensive analysis of the coding process."
    )

    def create_prompt(self, research_objectives: str, quotation: str,
                      keywords: List[str], contextualized_contents: List[str],
                      theoretical_framework: Dict[str, str]) -> str:
        # Format keywords for clarity
        keywords_formatted = "\n".join([
            f"- **{kw}**"
            for kw in keywords
        ])

        # Format contextualized contents with clear labeling
        contents_formatted = "\n\n".join([
            f"**Content {i+1}:**\n{content}"
            for i, content in enumerate([quotation] + contextualized_contents)
        ])

        # Extract theoretical framework components with default empty strings
        theory = theoretical_framework.get("theory", "N/A")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "N/A")
        rationale = theoretical_framework.get("rationale", "N/A")

        # Construct the prompt with clear sections and instructions
        prompt = (
            f"You are an experienced qualitative researcher specializing in thematic coding analysis "
            f"utilizing the 6Rs framework and grounded in {theory}. Your objective is to develop and critically analyze codes based on "
            f"the provided keywords and quotation, ensuring methodological rigor and theoretical alignment.\n\n"

            f"**Quotation for Analysis:**\n{quotation}\n\n"

            f"**Identified Keywords:**\n{keywords_formatted}\n\n"

            f"**Additional Contextualized Contents:**\n{contents_formatted}\n\n"

            f"**Research Objectives:**\n{research_objectives}\n\n"

            f"**Theoretical Framework:**\n"
            f"- **Theory:** {theory}\n"
            f"- **Philosophical Approach:** {philosophical_approach}\n"
            f"- **Rationale:** {rationale}\n\n"

            f"**Guidelines for Analysis:**\n"
            f"Your analysis should adhere to the 6Rs framework, addressing each dimension as follows:\n"
            f"1. **Robust:** Ensure that the code captures the true essence of the data in a theoretically sound manner.\n"
            f"2. **Reflective:** Demonstrate clear relationships between the data and the theoretical framework.\n"
            f"3. **Resplendent:** Provide a comprehensive understanding that encompasses all relevant aspects.\n"
            f"4. **Relevant:** Accurately represent the data, ensuring appropriateness and contextual fit.\n"
            f"5. **Radical:** Introduce unique and innovative insights that advance understanding.\n"
            f"6. **Righteous:** Maintain logical alignment with the overarching theoretical framework.\n\n"

            f"**Example Code Development:**\n"
            f"- **Code:** Economic Vulnerability\n"
            f"  - **Definition:** Victims originate from economically disadvantaged backgrounds, lacking financial stability.\n"
            f"  - **Keywords:** Poverty, Lack of Education\n"
            f"  - **6Rs Evaluation:** Robust, Relevant\n"
            f"  - **Theoretical Alignment:** Connects economic factors with victim vulnerability as per {theory}.\n"
            f"  - **Supporting Quotes:** [\"Victims of sex trafficking often come from vulnerable backgrounds, such as poverty...\"]\n"
            f"  - **Analytical Memos:** Economic hardship is a primary factor that increases susceptibility to trafficking offers.\n\n"

            f"**Instructions:**\n"
            f"- Each code should include its definition, associated keywords, 6Rs evaluation, theoretical alignment, supporting quotes, and analytical memos.\n"
            f"- Ensure that the codes are directly related to the theoretical framework and research objectives.\n"
            f"- Use the identified keywords as foundational themes for each code.\n"
            f"- Present the response in JSON format encapsulated within ```json``` blocks."
        )
        return prompt

    def validate_codes(self, codes: List[Dict[str, Any]], research_objectives: str,
                       theoretical_framework: Dict[str, str]) -> None:
        """
        Validates developed codes specific to coding analysis.
        """
        try:
            run_all_coding_assertions(
                codes=codes,
                research_objectives=research_objectives,
                theoretical_framework=theoretical_framework
            )
            logger.debug("All coding assertions passed successfully.")
        except AssertionError as ae:
            logger.error(f"Code validation failed: {ae}")
            for code in codes:
                try:
                    assert_code_relevance(code, research_objectives, theoretical_framework)
                except AssertionError as individual_ae:
                    logger.error(f"Validation failed for code '{code.get('code', 'Unknown')}': {individual_ae}")
            raise

    def forward(self, research_objectives: str, quotation: str,
                keywords: List[str], contextualized_contents: List[str],
                theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """Executes the coding analysis with retry mechanism."""
        for attempt in range(3):
            try:
                logger.debug(f"Attempt {attempt + 1} - Initiating coding analysis.")
                prompt = self.create_prompt(
                    research_objectives,
                    quotation,
                    keywords,
                    contextualized_contents,
                    theoretical_framework
                )
                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=8000,
                    temperature=0.5
                ).strip()
                logger.debug(f"Attempt {attempt + 1} - Response received from language model.")
                parsed_response = self.parse_response(response)

                if not parsed_response:
                    raise ValueError("Parsed response is empty or invalid JSON.")

                codes = parsed_response.get("codes", [])
                analysis = parsed_response.get("analysis", {})

                if not codes:
                    raise ValueError("No codes were generated. Please check the prompt and input data.")

                self.validate_codes(
                    codes=codes,
                    research_objectives=research_objectives,
                    theoretical_framework=theoretical_framework
                )

                logger.info(f"Attempt {attempt + 1} - Successfully developed and validated {len(codes)} codes.")
                return parsed_response

            except AssertionError as ae:
                logger.warning(f"Attempt {attempt + 1} - Assertion failed during coding analysis: {ae}")
                logger.debug(f"Attempt {attempt + 1} - Response causing assertion failure: {response}")
            except ValueError as ve:
                logger.warning(f"Attempt {attempt + 1} - ValueError: {ve}")
                logger.debug(f"Attempt {attempt + 1} - Response causing ValueError: {response}")
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} - Error in CodingAnalysisSignature.forward: {e}", exc_info=True)

        logger.error("Failed to develop valid codes after 3 attempts.")
        return {
            "error": "Failed to develop valid codes after 3 attempts. Please review the input data and prompt for possible improvements."
        }


# File: analysis/coding_module.py
#------------------------------------------------------------------------------
# analysis/coding_module.py
import logging
from typing import Dict, Any, List
import dspy
from analysis.coding import CodingAnalysisSignature

logger = logging.getLogger('my_logger')

class CodingAnalysisModule(dspy.Module):
    """
    DSPy module for developing and analyzing codes using the 6Rs framework.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(CodingAnalysisSignature)

    def forward(self, research_objectives: str, quotation: str,
                keywords: List[str], contextualized_contents: List[str],
                theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        logger.info("Starting coding analysis.")
        logger.debug(f"Input parameters: Research Objectives='{research_objectives[:100]}', "
                     f"Quotation='{quotation[:100]}', Keywords={keywords}, "
                     f"Contextualized Contents={contextualized_contents[:2]}, "
                     f"Theoretical Framework={theoretical_framework}")

        try:
            response = self.chain(
                research_objectives=research_objectives,
                quotation=quotation,
                keywords=keywords,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            logger.info("Successfully completed coding analysis.")
            logger.debug(f"Response: {response}")

            if not response.get("codes"):
                logger.warning("No codes were generated. Possible issue with inputs or prompt formulation.")
            
            return response
        except Exception as e:
            logger.error(f"Error during coding analysis: {e}", exc_info=True)
            return {}


# File: analysis/extract_keyword.py
#------------------------------------------------------------------------------
# src/analysis/extract_keyword.py

import logging
from typing import Dict, Any, List, Optional
import dspy
from dataclasses import dataclass
import json
import asyncio

from src.assertions_keyword import validate_keywords_dspy, AssertionConfig
from src.config.keyword_config import KeywordExtractionConfig

logger = logging.getLogger(__name__)

@dataclass
class KeywordAnalysisValue:
    """Analysis values for each of the 6Rs framework dimensions."""
    realness: str
    richness: str
    repetition: str
    rationale: str
    repartee: str
    regal: str

class KeywordExtractionSignature(dspy.Signature):
    """Signature for conducting thematic keyword extraction from quotations."""
    research_objectives: str = dspy.InputField(
        desc="Research goals and questions guiding the keyword analysis"
    )
    quotation: str = dspy.InputField(
        desc="Selected quotation for keyword extraction"
    )
    contextualized_contents: List[str] = dspy.InputField(
        desc="Additional contextual content to support interpretation"
    )
    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc="""Theoretical foundation including:
        - theory: Primary theoretical approach
        - philosophical_approach: Underlying philosophical foundation
        - rationale: Justification for chosen approach"""
    )

    quotation_info: Dict[str, Any] = dspy.OutputField(
        desc="Comprehensive context information"
    )
    keywords: List[Dict[str, Any]] = dspy.OutputField(
        desc="Extracted keywords with detailed analysis"
    )
    analysis: Dict[str, Any] = dspy.OutputField(
        desc="Comprehensive keyword analysis output"
    )

class KeywordExtractionModule(dspy.Module):
    """Enhanced DSPy module for keyword extraction with configurable validation."""
    
    def __init__(self, config: Optional[KeywordExtractionConfig] = None):
        """Initialize the module with optional configuration."""
        super().__init__()
        self.config = config or KeywordExtractionConfig()
        self.chain = dspy.ChainOfThought(KeywordExtractionSignature)
        logger.info("Initialized KeywordExtractionModule with config: %s", self.config)

    def create_prompt(self, research_objectives: str, quotation: str,
                     contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> str:
        """Creates the enhanced prompt for keyword extraction."""
        logger.debug("Creating prompt for keyword extraction")
        
        context_formatted = "\n\n".join([
            f"Context {i+1}:\n{content}"
            for i, content in enumerate(contextualized_contents)
        ])

        theory = theoretical_framework.get("theory", "")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "")
        rationale = theoretical_framework.get("rationale", "")

        prompt = (
            f"As an experienced qualitative researcher, analyze the following quotation "
            f"using the 6Rs framework to extract meaningful keywords.\n\n"

            f"Quotation:\n{quotation}\n\n"
            f"Additional Context:\n{context_formatted}\n\n"
            f"Research Objectives:\n{research_objectives}\n\n"

            f"Theoretical Framework:\n"
            f"Theory: {theory}\n"
            f"Philosophical Approach: {philosophical_approach}\n"
            f"Rationale: {rationale}\n\n"

            f"6Rs Framework Guidelines:\n"
            f"1. Realness: Select words reflecting genuine experiences\n"
            f"2. Richness: Identify words with deep meaning\n"
            f"3. Repetition: Note recurring patterns\n"
            f"4. Rationale: Connect to theoretical foundations\n"
            f"5. Repartee: Consider discussion value\n"
            f"6. Regal: Focus on centrality to topic\n\n"

            f"Requirements:\n"
            "1. Each keyword must be analyzed across all 6Rs dimensions\n"
            "2. Include theoretical alignment justification\n"
            "3. Consider contextual significance\n"
            "4. Note pattern frequency\n"
            f"5. Maximum keywords: {self.config.max_keywords}\n"
            f"6. Minimum confidence: {self.config.min_confidence}\n\n"

            "Expected Output Format:\n"
            "{\n"
            '  "quotation_info": {\n'
            '    "quotation": "...",\n'
            '    "research_objectives": "...",\n'
            '    "theoretical_framework": {...}\n'
            "  },\n"
            '  "keywords": [\n'
            "    {\n"
            '      "keyword": "...",\n'
            '      "category": "...",\n'
            '      "6Rs_framework": ["..."],\n'
            '      "analysis_value": {\n'
            '        "realness": "...",\n'
            '        "richness": "...",\n'
            '        "repetition": "...",\n'
            '        "rationale": "...",\n'
            '        "repartee": "...",\n'
            '        "regal": "..."\n'
            "      }\n"
            "    }\n"
            "  ],\n"
            '  "analysis": {\n'
            '    "patterns_identified": [...],\n'
            '    "theoretical_interpretation": "...",\n'
            '    "methodological_reflection": {...},\n'
            '    "practical_implications": "..."\n'
            "  }\n"
            "}\n"
        )
        
        logger.debug("Created prompt with length: %d characters", len(prompt))
        return prompt

    async def forward(self, 
                     research_objectives: str, 
                     quotation: str,
                     contextualized_contents: List[str], 
                     theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """Execute keyword extraction with validation and retry mechanism."""
        attempt = 0
        max_retries = self.config.max_retries
        
        while attempt < max_retries:
            attempt += 1
            logger.info("Attempt %d/%d - Starting keyword extraction", attempt, max_retries)
            
            try:
                prompt = self.create_prompt(
                    research_objectives,
                    quotation,
                    contextualized_contents,
                    theoretical_framework
                )

                start_time = asyncio.get_event_loop().time()
                response = await self.chain(
                    research_objectives=research_objectives,
                    quotation=quotation,
                    contextualized_contents=contextualized_contents,
                    theoretical_framework=theoretical_framework
                )
                generation_time = asyncio.get_event_loop().time() - start_time
                logger.debug("LLM response generated in %.2f seconds", generation_time)

                # Extract keywords and validate
                keywords = response.keywords if hasattr(response, 'keywords') else []
                if not keywords:
                    raise ValueError("No keywords found in response")

                logger.info("Extracted %d keywords", len(keywords))

                # Validate using new assertion system
                validation_result = validate_keywords_dspy(
                    keywords=keywords,
                    quotation=quotation,
                    contextualized_contents=contextualized_contents,
                    research_objectives=research_objectives,
                    theoretical_framework=theoretical_framework,
                    config=self.config.assertion_config
                )

                if not validation_result["passed"]:
                    if self.config.strict_mode:
                        raise ValueError(
                            f"Keyword validation failed: {validation_result['failed_assertions']}"
                        )
                    logger.warning(
                        "Keyword validation produced warnings: %s", 
                        validation_result["warnings"]
                    )

                # Apply configuration limits
                if len(keywords) > self.config.max_keywords:
                    logger.warning(
                        "Truncating keywords to maximum limit of %d", 
                        self.config.max_keywords
                    )
                    keywords = keywords[:self.config.max_keywords]

                # Prepare final response
                final_response = {
                    "quotation_info": {
                        "quotation": quotation,
                        "research_objectives": research_objectives,
                        "theoretical_framework": theoretical_framework
                    },
                    "keywords": keywords,
                    "analysis": response.analysis if hasattr(response, 'analysis') else {},
                    "validation_report": validation_result
                }

                logger.info("Successfully extracted and validated keywords on attempt %d", attempt)
                return final_response

            except Exception as e:
                logger.error("Error in attempt %d: %s", attempt, str(e))
                if attempt == max_retries:
                    logger.error("Max retries reached. Extraction failed.")
                    return {
                        "error": str(e),
                        "keywords": [],
                        "quotation_info": {
                            "quotation": quotation,
                            "research_objectives": research_objectives,
                            "theoretical_framework": theoretical_framework
                        },
                        "analysis": {
                            "error": "Failed to complete keyword extraction after maximum retries"
                        }
                    }
                logger.info("Retrying keyword extraction...")
                await asyncio.sleep(1)  # Brief delay before retry

        return {}

# File: analysis/extract_keyword_module.py
#------------------------------------------------------------------------------
# src/analysis/extract_keyword_module.py

import logging
from typing import Dict, Any, List
import dspy

from src.analysis.extract_keyword import KeywordExtractionSignature

logger = logging.getLogger(__name__)

class KeywordExtractionModule(dspy.Module):
    """
    DSPy module for extracting and analyzing keywords from quotations using the 6Rs framework.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(KeywordExtractionSignature)

    def forward(self, research_objectives: str, quotation: str,
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """
        Execute keyword extraction and analysis with the 6Rs framework.
        """
        try:
            logger.debug("Running KeywordExtractionModule with integrated keyword assertions.")
            response = self.chain(
                research_objectives=research_objectives,
                quotation=quotation,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            return response
        except Exception as e:
            logger.error(f"Error in KeywordExtractionModule.forward: {e}", exc_info=True)
            return {}


# File: analysis/grouping.py
#------------------------------------------------------------------------------
# src/analysis/grouping.py

import logging
from typing import Dict, Any, List
import dspy
from dataclasses import dataclass
import json
import re

logger = logging.getLogger(__name__)

@dataclass
class GroupingAnalysisSignature(dspy.Signature):
    """
    A signature for grouping similar codes into potential themes based 
    on research objectives and a theoretical framework.
    """

    # Input Fields
    research_objectives: str = dspy.InputField(
        desc="The specific goals and research questions guiding the analysis."
    )

    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc="Detailed information about the theoretical foundation supporting the analysis."
    )

    codes: List[Dict[str, Any]] = dspy.InputField(
        desc="""
        A structured collection of developed codes, each accompanied by its definition. 
        Each code entry includes:
         - code: The name or label of the developed code.
         - definition: A precise and clear explanation of the code's meaning and scope.
        """
    )

    # Output Fields
    groupings: List[Dict[str, Any]] = dspy.OutputField(
        desc="""
        A structured collection of code groupings, representing potential themes. Each grouping entry includes:
         - group_label: A tentative label that reflects the shared meaning of the grouped codes.
         - codes: A list of codes belonging to this grouping.
         - rationale: A brief explanation for grouping these codes together. 
         **Grouping Principles (4Rs Framework):
        - **Reciprocal:** Identify codes that share mutual connections or relationships, potentially leading to the formation of new concepts when grouped together. Consider how the meanings of codes interact and complement each other.
        - **Recognizable:** Ensure that the groupings are grounded in the original data. The keywords provided offer insights into the prominent concepts within the data. Utilise these keywords to guide the formation of groupings that accurately reflect the recurring patterns in the data.
        - **Responsive:** The research objectives outline the key areas of inquiry. Create groupings that directly address these objectives, ensuring that the generated themes are relevant to the research goals.
        - **Resourceful:** Focus on creating groupings that offer valuable insights and contribute to answering the research questions. Explain how the themes derived from these groupings help to understand the phenomenon under investigation.

        """
    )

    def create_prompt(
        self,
        research_objectives: str,
        theoretical_framework: Dict[str, str],
        codes: List[Dict[str, Any]]
    ) -> str:
        """
        Creates a prompt to be given to the language model, instructing it to group the provided codes into potential themes.
        """

        # Extract theory details for context
        theory = theoretical_framework.get("theory", "N/A")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "N/A")
        rationale = theoretical_framework.get("rationale", "N/A")

        # Format codes for display
        formatted_codes = "\n".join([
            f"- **{c['code']}**: {c['definition']}"
            for c in codes
        ])

        # Instructions for the LLM
        prompt = (
        f"You are an expert qualitative researcher specialising in thematic analysis. "
        f"You have a set of codes and associated keywords derived from prior coding stages. Your task is to group these codes into "
        f"higher-level themes that reflect shared meanings or patterns. The groupings should align with "
        f"the given research objectives and theoretical framework, considering the following principles:\n\n"

        f"**Research Objectives:**\n{research_objectives}\n\n"

        f"**Theoretical Framework:**\n"
        f"- Theory: {theory}\n"
        f"- Philosophical Approach: {philosophical_approach}\n"
        f"- Rationale: {rationale}\n\n"

        f"**Codes to be Grouped:**\n{formatted_codes}\n\n" 


        f"**Grouping Principles (4Rs Framework):**\n"
        f"- **Reciprocal:** Identify codes that share mutual connections or relationships, potentially leading to the formation of new concepts when grouped together. Consider how the meanings of codes interact and complement each other.\n"
        f"- **Recognizable:** Ensure that the groupings are grounded in the original data. The keywords provided offer insights into the prominent concepts within the data. Utilise these keywords to guide the formation of groupings that accurately reflect the recurring patterns in the data.\n"
        f"- **Responsive:** The research objectives outline the key areas of inquiry. Create groupings that directly address these objectives, ensuring that the generated themes are relevant to the research goals.\n"
        f"- **Resourceful:** Focus on creating groupings that offer valuable insights and contribute to answering the research questions. Explain how the themes derived from these groupings help to understand the phenomenon under investigation.\n\n" 

        f"Your response should identify meaningful themes or groups that these codes can be organised into. "
        f"For each grouping, provide:\n"
        f"- **group_label**: A tentative label describing the shared meaning of the grouped codes.\n"
        f"- **codes**: A list of the code labels in that grouping.\n"
        f"- **rationale**: A brief explanation of why these codes were grouped together, explicitly addressing how this grouping aligns with the 4Rs framework.\n\n"
        
        f"Return your final answer in JSON format inside triple backticks, e.g.:\n"
        f"```json\n{{\n \"groupings\": [\n {{\n \"group_label\": \"Label\",\n \"codes\": [\"Code A\", \"Code B\"],\n \"rationale\": \"Explanation.\"\n }}\n ]\n}}\n```\n"
    ) 

    def parse_response(self, response: str) -> Dict[str, Any]:
        """
        Extracts and parses the JSON content from the language model's response.
        """
        try:
            json_match = re.search(r"```json\s*(\{.*?\})\s*```", response, re.DOTALL)
            if not json_match:
                logger.error("No valid JSON found in the response.")
                logger.debug(f"Full response: {response}")
                return {}
            json_string = json_match.group(1)
            response_json = json.loads(json_string)
            return response_json
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}. Response: {response}")
            return {}
        except Exception as e:
            logger.error(f"Unexpected error during response parsing: {e}")
            return {}

    def forward(
        self,
        research_objectives: str,
        theoretical_framework: Dict[str, str],
        codes: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Executes the grouping analysis by generating a prompt and sending it to the language model.
        Tries multiple times in case the response is not valid.
        """
        prompt = self.create_prompt(
            research_objectives=research_objectives,
            theoretical_framework=theoretical_framework,
            codes=codes
        )

        for attempt in range(3):
            try:
                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=2000,
                    temperature=0.5
                ).strip()

                parsed_response = self.parse_response(response)

                if not parsed_response or 'groupings' not in parsed_response:
                    raise ValueError("Parsed response is empty or missing 'groupings'.")

                groupings = parsed_response.get("groupings", [])
                if not groupings:
                    raise ValueError("No groupings were generated. Check the prompt and input data.")

                logger.info(f"Successfully generated {len(groupings)} groupings.")
                return parsed_response

            except ValueError as ve:
                logger.warning(f"Attempt {attempt + 1} - ValueError: {ve}")
                logger.debug(f"Response causing ValueError: {response}")
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} - Error in GroupingAnalysisSignature.forward: {e}", exc_info=True)

        logger.error("Failed to produce valid groupings after 3 attempts.")
        return {
            "error": "Failed to develop valid groupings after multiple attempts. Please review the input data and prompt."
        }


# File: analysis/grouping_module.py
#------------------------------------------------------------------------------
# src/analysis/grouping_module.py

import logging
from typing import Dict, Any, List
import dspy
from analysis.grouping import GroupingAnalysisSignature

logger = logging.getLogger(__name__)

class GroupingAnalysisModule(dspy.Module):
    """
    DSPy module for grouping similar codes into potential themes,
    building upon code definitions, research objectives, and 
    a theoretical framework.
    """

    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(GroupingAnalysisSignature)

    def forward(
        self,
        research_objectives: str,
        theoretical_framework: Dict[str, str],
        codes: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Executes the grouping analysis using the defined signature 
        and the language model.
        """
        logger.info("Starting grouping analysis.")
        try:
            response = self.chain(
                research_objectives=research_objectives,
                theoretical_framework=theoretical_framework,
                codes=codes
            )

            if not response.get("groupings"):
                logger.warning("No groupings were generated. Possible issue with inputs or prompt formulation.")

            logger.info("Successfully completed grouping analysis.")
            return response
        except Exception as e:
            logger.error(f"Error during grouping analysis: {e}", exc_info=True)
            return {}


# File: analysis/metrics.py
#------------------------------------------------------------------------------
#metrics.py
import logging
from typing import List, Dict, Any, Callable
import dspy

from src.utils.utils import check_answer_length
from src.utils.logger import setup_logging
# Initialize logger
logger = logging.getLogger(__name__)

class BaseAssessment(dspy.Signature):
    """
    Base class for all assessment signatures.
    """
    context: str = dspy.InputField(
        desc=(
            "The contextual information provided to generate the answer. This includes all relevant "
            "documents, data chunks, or information sources that the answer is based upon."
        )
    )
    question: str = dspy.InputField(
        desc=(
            "The original question that was posed. This is used to understand the intent and scope "
            "of the answer in relation to the provided context."
        )
    )
    answer: str = dspy.InputField(
        desc=(
            "The answer generated by the system that needs to be evaluated for factual correctness "
            "based on the provided context."
        )
    )

    def generate_prompt(self, context: str, question: str, answer: str, task: str) -> str:
        return (
            f"Context: {context}\n"
            f"Question: {question}\n"
            f"Answer: {answer}\n\n"
            f"{task}"
        )

class Assess(BaseAssessment):
    """
    Assess the factual correctness of an answer based on the provided context.

    This signature evaluates whether the generated answer accurately reflects the information
    present in the given context. It leverages a language model to perform a nuanced analysis
    beyond simple keyword matching, ensuring a thorough assessment of factual accuracy.
    """
    factually_correct: str = dspy.OutputField(
        desc=(
            "Indicator of whether the answer is factually correct based on the context. "
            "Should be 'Yes' if the answer accurately reflects the information in the context, "
            "and 'No' otherwise."
        )
    )

    def forward(self, context: str, question: str, answer: str) -> Dict[str, str]:
        try:
            logger.debug(f"Assessing factual correctness for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Based on the context provided, evaluate whether the answer is factually correct.\n"
                "Respond with 'Yes' if the answer accurately reflects the information in the context.\n"
                "Respond with 'No' if the answer contains factual inaccuracies or is not supported by the context.\n"
                "Please respond with 'Yes' or 'No' only."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=3,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Factuality assessment response: '{response}'")
            if response.lower() in ['yes', 'no']:
                result = response.capitalize()
            else:
                logger.warning(f"Unexpected response from factuality assessment: '{response}'. Defaulting to 'No'.")
                result = 'No'
            logger.info(f"Factuality assessment result: {result} for question: '{question}'")
            return {"factually_correct": result}
        except Exception as e:
            logger.error(f"Error in Assess.forward: {e}", exc_info=True)
            return {"factually_correct": "No"}


class AssessRelevance(BaseAssessment):
    """
    Assess the relevance of an answer to the given question and context.
    
    This signature evaluates whether the generated answer directly and comprehensively 
    addresses the user's query, considering the provided context.
    """
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug(f"Assessing relevance for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the relevance of the answer to the question based on the context.\n"
                "Provide a relevance score between 1 (not relevant) and 5 (highly relevant)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Relevance assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected relevance score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid relevance score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Relevance assessment result: {result} for question: '{question}'")
            return {"relevance_score": result}
        except Exception as e:
            logger.error(f"Error in AssessRelevance.forward: {e}", exc_info=True)
            return {"relevance_score": 1}


class AssessCoherence(BaseAssessment):
    """
    Assess the coherence of an answer.
    
    This signature evaluates whether the generated answer is well-structured and logically consistent.
    """
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing coherence of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the coherence of the above answer.\n"
                "Provide a coherence score between 1 (not coherent) and 5 (highly coherent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Coherence assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected coherence score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid coherence score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Coherence assessment result: {result}")
            return {"coherence_score": result}
        except Exception as e:
            logger.error(f"Error in AssessCoherence.forward: {e}", exc_info=True)
            return {"coherence_score": 1}


class AssessConciseness(BaseAssessment):
    """
    Assess the conciseness of an answer.
    
    This signature evaluates whether the generated answer is succinct and free from unnecessary verbosity.
    """
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing conciseness of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the conciseness of the above answer.\n"
                "Provide a conciseness score between 1 (not concise) and 5 (highly concise)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Conciseness assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected conciseness score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid conciseness score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Conciseness assessment result: {result}")
            return {"conciseness_score": result}
        except Exception as e:
            logger.error(f"Error in AssessConciseness.forward: {e}", exc_info=True)
            return {"conciseness_score": 1}


class AssessFluency(BaseAssessment):
    """
    Assess the fluency of an answer.
    
    This signature evaluates whether the generated answer exhibits natural language flow and is free from grammatical errors.
    """
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing fluency of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the fluency of the above answer.\n"
                "Provide a fluency score between 1 (not fluent) and 5 (highly fluent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Fluency assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected fluency score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid fluency score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Fluency assessment result: {result}")
            return {"fluency_score": result}
        except Exception as e:
            logger.error(f"Error in AssessFluency.forward: {e}", exc_info=True)
            return {"fluency_score": 1}


class ComprehensiveAssessment(BaseAssessment):
    """
    Comprehensive assessment of generated answers across multiple dimensions.
    """
    factually_correct: str = dspy.OutputField(desc="Whether the answer is factually correct ('Yes'/'No').")
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, Any]:
        try:
            logger.debug(f"Performing comprehensive assessment for question: '{question}'")
            
            # Assess factual correctness
            factuality = Assess()(context=context, question=question, answer=answer)['factually_correct']
            
            # Assess relevance
            relevance = AssessRelevance()(context=context, question=question, answer=answer)['relevance_score']
            
            # Assess coherence
            coherence = AssessCoherence()(context=context, question=question, answer=answer)['coherence_score']
            
            # Assess conciseness
            conciseness = AssessConciseness()(context=context, question=question, answer=answer)['conciseness_score']
            
            # Assess fluency
            fluency = AssessFluency()(context=context, question=question, answer=answer)['fluency_score']
            
            # Aggregate scores with adjusted weights
            composite_score = {
                "factually_correct": factuality,
                "relevance_score": relevance,
                "coherence_score": coherence,
                "conciseness_score": conciseness,
                "fluency_score": fluency
            }
            logger.info(f"Comprehensive assessment result: {composite_score}")
            return composite_score
        except Exception as e:
            logger.error(f"Error in ComprehensiveAssessment.forward: {e}", exc_info=True)
            return {
                "factually_correct": "No",
                "relevance_score": 1,
                "coherence_score": 1,
                "conciseness_score": 1,
                "fluency_score": 1
            }


def comprehensive_metric(example: Dict[str, Any], pred: Dict[str, Any], trace: Any = None) -> float:
    """
    Comprehensive metric to evaluate the answer across multiple dimensions.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
        trace (Any, optional): Trace information for optimization (unused here).
    
    Returns:
        float: A combined score representing the quality of the answer.
    """
    try:
        logger.debug(f"Evaluating comprehensive metrics for question: '{example.get('question', '')}'")
        assessment = comprehensive_assessment_module(
            context=example.get('context', ''),
            question=example.get('question', ''),
            answer=pred.get('answer', '')
        )
        logger.debug(f"Comprehensive assessment: {assessment}")
        
        # Convert 'factually_correct' to binary
        factually_correct = 1 if assessment.get("factually_correct") == "Yes" else 0
        
        # Normalize scores between 0 and 1
        relevance = assessment.get("relevance_score", 1) / 5
        coherence = assessment.get("coherence_score", 1) / 5
        conciseness = assessment.get("conciseness_score", 1) / 5
        fluency = assessment.get("fluency_score", 1) / 5
        
        # Define adjusted weights for each metric
        weights = {
            "factually_correct": 0.5,
            "relevance": 0.2,
            "coherence": 0.1,
            "conciseness": 0.1,
            "fluency": 0.1
        }
        
        # Calculate the composite score
        composite_score = (
            weights["factually_correct"] * factually_correct +
            weights["relevance"] * relevance +
            weights["coherence"] * coherence +
            weights["conciseness"] * conciseness +
            weights["fluency"] * fluency
        )
        
        logger.info(f"Comprehensive metric score: {composite_score}")
        return composite_score
    except Exception as e:
        logger.error(f"Error in comprehensive_metric: {e}", exc_info=True)
        return 0.0


def is_answer_fully_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    """
    Determines if the answer meets all quality metrics.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
    
    Returns:
        bool: True if all metrics meet the desired thresholds, False otherwise.
    """
    scores = comprehensive_metric(example, pred)
    # Define threshold (e.g., composite score should be at least 0.8)
    is_factual = scores >= 0.8
    logger.debug(f"Is answer fully correct (scores >= 0.8): {is_factual}")
    return is_factual


def factuality_metric(example: Dict[str, Any], pred: Dict[str, Any]) -> int:
    """
    Metric to evaluate factual correctness of the answer.

    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.

    Returns:
        int: 1 if factually correct, 0 otherwise.
    """
    try:
        assess = Assess()
        result = assess(context=example.get('context', ''), question=example.get('question', ''), answer=pred.get('answer', ''))
        factually_correct = result.get('factually_correct', 'No')
        logger.debug(f"Factuality metric result: {factually_correct}")
        return 1 if factually_correct == 'Yes' else 0
    except Exception as e:
        logger.error(f"Error in factuality_metric: {e}", exc_info=True)
        return 0 


# Initialize the assessment modules
try:
    # Use the unoptimized module directly without caching
    comprehensive_assessment_module = dspy.ChainOfThought(ComprehensiveAssessment)
    logger.info("Comprehensive Assessment DSPy module initialized successfully.")
except Exception as e:
    logger.error(f"Error initializing Comprehensive Assessment DSPy module: {e}", exc_info=True)
    raise


# File: analysis/select_quotation.py
#------------------------------------------------------------------------------
import logging
import re
import json
from typing import List, Dict, Any

import dspy

from src.assertions import (
    assert_pattern_representation,
    assert_research_objective_alignment,
    assert_selective_transcription,
    assert_creswell_categorization,
    assert_reader_engagement
)

logger = logging.getLogger(__name__)

class EnhancedQuotationSignature(dspy.Signature):
    """
    A comprehensive signature for conducting thematic analysis of interview transcripts
    following Braun and Clarke's (2006) methodology. This signature supports systematic
    qualitative analysis with robust pattern recognition and theoretical integration.
    """
    
    research_objectives: str = dspy.InputField(
        desc="Research goals and questions guiding the thematic analysis"
    )
    
    transcript_chunk: str = dspy.InputField(
        desc="Primary transcript segment for analysis"
    )
    
    contextualized_contents: List[str] = dspy.InputField(
        desc="Additional contextual content to support interpretation"
    )
    
    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc="""Theoretical foundation including:
        - theory: Primary theoretical approach
        - philosophical_approach: Underlying philosophical foundation
        - rationale: Justification for chosen approach"""
    )
    
    transcript_info: Dict[str, Any] = dspy.OutputField(
        desc="""Comprehensive context information including:
        - transcript_chunk: Selected content
        - research_objectives: Analysis goals
        - theoretical_framework: Complete framework details"""
    )
    
    retrieved_chunks: List[str] = dspy.OutputField(
        desc="Collection of relevant transcript segments retrieved for analysis"
    )
    
    retrieved_chunks_count: int = dspy.OutputField(
        desc="Number of transcript chunks retrieved and analyzed"
    )
    
    used_chunk_ids: List[str] = dspy.OutputField(
        desc="Identifiers of transcript chunks utilized in the analysis"
    )
    
    quotations: List[Dict[str, Any]] = dspy.OutputField(
        desc="""Selected quotations with detailed analysis:
        - quotation: Exact quote text
        - creswell_category: Classification (longer/discrete/embedded)
        - classification: Content type
        - context: 
            * preceding_question
            * situation
            * pattern_representation
        - analysis_value:
            * relevance to objectives
            * pattern support
            * theoretical alignment"""
    )
    
    analysis: Dict[str, Any] = dspy.OutputField(
        desc="""Comprehensive thematic analysis including:
        - philosophical_underpinning: Analysis approach
        - patterns_identified: Key patterns discovered
        - theoretical_interpretation: Framework application
        - methodological_reflection:
            * pattern_robustness
            * theoretical_alignment
            * researcher_reflexivity
        - practical_implications: Applied insights"""
    )
    
    answer: Dict[str, Any] = dspy.OutputField(
        desc="""Analysis synthesis and contributions:
        - summary: Key findings
        - theoretical_contribution: Theory advancement
        - methodological_contribution:
            * approach
            * pattern_validity
            * theoretical_integration"""
    )

    def create_prompt(self, research_objectives: str, transcript_chunk: str, 
                     contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> str:
        chunks_formatted = "\n\n".join([
            f"Content {i+1}:\n{content}" 
            for i, content in enumerate([transcript_chunk] + contextualized_contents)
        ])

        theory = theoretical_framework.get("theory", "")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "")
        rationale = theoretical_framework.get("rationale", "")

        prompt = (
            f"You are an experienced qualitative researcher conducting a thematic analysis of "
            f"interview transcripts using Braun and Clarke's (2006) approach.\n\n"

            f"First, review the transcript chunks and contextualized_content:\n\n"
            f"{chunks_formatted}\n\n"
            
            f"Research Objectives:\n"
            f"<research_objectives>\n"
            f"{research_objectives}\n"
            f"</research_objectives>\n\n"

            f"Theoretical Framework:\n"
            f"<theoretical_framework>\n"
            f"Theory: {theory}\n"
            f"Philosophical Approach: {philosophical_approach}\n"
            f"Rationale: {rationale}\n"
            f"</theoretical_framework>\n\n"
                    
            f"Your analysis should follow these steps:\n\n"

            f"1. **Quotation Selection**:\n"
            f"   - Select quotes that demonstrate robust patterns in the data.\n"
            f"   - Classify quotes using Creswell's categories:\n"
            f"     a) Longer quotations: For complex understandings\n"
            f"     b) Discrete quotations: For diverse perspectives\n"
            f"     c) Embedded quotations: Brief phrases showing text shifts\n"
            f"   - Ensure quotes enhance reader engagement and highlight unique findings.\n"
            f"   - Provide adequate context for accurate comprehension.\n\n"

            f"2. **Pattern Recognition**:\n"
            f"   - Identify patterns emerging from data rather than predetermined categories.\n"
            f"   - Support patterns with multiple quotations.\n"
            f"   - Maintain theoretical alignment while remaining open to emerging themes.\n"
            f"   - Document methodological decisions transparently.\n\n"

            f"3. **Theoretical Integration**:\n"
            f"   - Demonstrate clear philosophical underpinning.\n"
            f"   - Show how findings connect to the theoretical framework.\n"
            f"   - Practice researcher reflexivity throughout analysis.\n"
            f"   - Balance selectivity with comprehensiveness.\n\n"

            f"Your final output should follow this JSON structure:\n\n"
            "{\n"
            "  \"transcript_info\": {\n"
            "    \"transcript_chunk\": \"\",\n"
            "    \"research_objectives\": \"\",\n"
            "    \"theoretical_framework\": {\n"
            "      \"theory\": \"\",\n"
            "      \"philosophical_approach\": \"\",\n"
            "      \"rationale\": \"\"\n"
            "    }\n"
            "  },\n"
            "  \"retrieved_chunks\": [],\n"
            "  \"retrieved_chunks_count\": 0,\n"
            "  \"used_chunk_ids\": [],\n"
            "  \"quotations\": [\n"
            "    {\n"
            "      \"quotation\": \"\",\n"
            "      \"creswell_category\": \"\",\n"
            "      \"classification\": \"\",\n"
            "      \"context\": {\n"
            "        \"preceding_question\": \"\",\n"
            "        \"situation\": \"\",\n"
            "        \"pattern_representation\": \"\"\n"
            "      },\n"
            "      \"analysis_value\": {\n"
            "        \"relevance\": \"\",\n"
            "        \"pattern_support\": \"\",\n"
            "        \"theoretical_alignment\": \"\"\n"
            "      }\n"
            "    }\n"
            "  ],\n"
            "  \"analysis\": {\n"
            "    \"philosophical_underpinning\": \"\",\n"
            "    \"patterns_identified\": [],\n"
            "    \"theoretical_interpretation\": \"\",\n"
            "    \"methodological_reflection\": {\n"
            "      \"pattern_robustness\": \"\",\n"
            "      \"theoretical_alignment\": \"\",\n"
            "      \"researcher_reflexivity\": \"\"\n"
            "    },\n"
            "    \"practical_implications\": \"\"\n"
            "  },\n"
            "  \"answer\": {\n"
            "    \"summary\": \"\",\n"
            "    \"theoretical_contribution\": \"\",\n"
            "    \"methodological_contribution\": {\n"
            "      \"approach\": \"\",\n"
            "      \"pattern_validity\": \"\",\n"
            "      \"theoretical_integration\": \"\"\n"
            "    }\n"
            "  }\n"
            "}\n"
        )
        return prompt

    def parse_response(self, response: str) -> Dict[str, Any]:
        try:
            json_match = re.search(r"```json\s*(\{.*?\})\s*```", response, re.DOTALL)
            if not json_match:
                logger.error("No valid JSON found in response.")
                logger.debug(f"Full response received: {response}")
                return {}
            json_string = json_match.group(1)

            response_json = json.loads(json_string)
            return response_json
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}, Response: {response}")
            return {}
        except Exception as e:
            logger.error(f"Error parsing response: {e}")
            return {}

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        for attempt in range(3):
            try:
                logger.debug(f"Attempt {attempt + 1} - Starting enhanced quotation selection and analysis process.")
                
                prompt = self.create_prompt(
                    research_objectives,
                    transcript_chunk,
                    contextualized_contents,
                    theoretical_framework
                )
                
                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=6000,
                    temperature=0.5
                ).strip()
                
                logger.debug(f"Attempt {attempt + 1} - Response received from language model.")
                
                parsed_response = self.parse_response(response)
                
                if not parsed_response:
                    raise ValueError("Parsed response is empty. Possibly invalid JSON.")
                
                quotations = parsed_response.get("quotations", [])
                analysis = parsed_response.get("analysis", {})
                
                assert_pattern_representation(quotations, analysis.get("patterns_identified", []))
                assert_research_objective_alignment(quotations, research_objectives)
                assert_selective_transcription(quotations, transcript_chunk)
                assert_creswell_categorization(quotations)
                assert_reader_engagement(quotations)
                
                logger.info(f"Attempt {attempt + 1} - Successfully completed analysis with {len(quotations)} quotations.")
                return parsed_response

            except AssertionError as af:
                logger.warning(f"Attempt {attempt + 1} - Assertion failed during analysis: {af}")
                logger.debug(f"Attempt {attempt + 1} - Response causing assertion failure: {response}")
                parsed_response = self.handle_failed_assertion(
                    af, research_objectives, transcript_chunk, contextualized_contents, theoretical_framework
                )
                if parsed_response:
                    return parsed_response
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} - Error in EnhancedQuotationSignature.forward: {e}", exc_info=True)
                
        logger.error(f"Failed to generate valid output after multiple attempts. Last response: {response}")
        return {}

    def handle_failed_assertion(self, assertion_failure: AssertionError,
                              research_objectives: str, transcript_chunk: str,
                              contextualized_contents: List[str],
                              theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Handling failed assertion by refining the prompt.")

            focused_prompt = self.create_prompt(
                research_objectives,
                transcript_chunk,
                contextualized_contents,
                theoretical_framework
            )
            
            focused_prompt += (
                f"\n\nThe previous attempt failed because: {assertion_failure}\n"
                f"Please ensure that your analysis addresses this specific issue while maintaining "
                f"all other requirements for thorough theoretical analysis."
            )

            response = self.language_model.generate(
                prompt=focused_prompt,
                max_tokens=2000,
                temperature=0.5
            ).strip()

            logger.debug("Response received from language model after handling assertion failure.")

            parsed_response = self.parse_response(response)
            
            if not parsed_response:
                raise ValueError("Parsed response is empty after handling assertion failure.")
            
            quotations = parsed_response.get("quotations", [])
            analysis = parsed_response.get("analysis", {})
            
            assert_pattern_representation(quotations, analysis.get("patterns_identified", []))
            assert_research_objective_alignment(quotations, research_objectives)
            assert_selective_transcription(quotations, transcript_chunk)
            assert_creswell_categorization(quotations)
            assert_reader_engagement(quotations)

            logger.info("Successfully handled failed assertion and obtained valid analysis.")
            return parsed_response

        except AssertionError as af_inner:
            logger.error(f"Refined analysis still failed assertions: {af_inner}")
            return {}
        except Exception as e:
            logger.error(f"Error in handle_failed_assertion: {e}", exc_info=True)
            return {}

class EnhancedQuotationModule(dspy.Module):
    """
    DSPy module implementing the enhanced quotation selection and theoretical analysis functionality.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(EnhancedQuotationSignature)

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Running EnhancedQuotationModule with integrated theoretical analysis.")
            response = self.chain(
                research_objectives=research_objectives,
                transcript_chunk=transcript_chunk,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            return response
        except Exception as e:
            logger.error(f"Error in EnhancedQuotationModule.forward: {e}", exc_info=True)
            return {}

# File: analysis/select_quotation_module.py
#------------------------------------------------------------------------------
import logging
from typing import Dict, Any, List
import dspy

from src.analysis.select_quotation import EnhancedQuotationModule
from src.assertions import (
    assert_pattern_representation,
    assert_research_objective_alignment,
    assert_selective_transcription,
    assert_creswell_categorization,
    assert_reader_engagement
)

logger = logging.getLogger(__name__)

class SelectQuotationModule(dspy.Module):
    """
    DSPy module to select and analyze quotations based on research objectives,
    transcript chunks, and theoretical framework.
    """
    def __init__(self):
        super().__init__()
        self.enhanced_module = EnhancedQuotationModule()

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Running SelectQuotationModule with integrated theoretical analysis.")
            response = self.enhanced_module.forward(
                research_objectives=research_objectives,
                transcript_chunk=transcript_chunk,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            
            quotations = response.get("quotations", [])
            analysis = response.get("analysis", {})
            patterns = analysis.get("patterns_identified", [])

            assert_pattern_representation(quotations, patterns)
            assert_research_objective_alignment(quotations, research_objectives)
            assert_selective_transcription(quotations, transcript_chunk)
            assert_creswell_categorization(quotations)
            assert_reader_engagement(quotations)

            return response
        except Exception as e:
            logger.error(f"Error in SelectQuotationModule.forward: {e}", exc_info=True)
            return {}

# File: analysis/theme_development.py
#------------------------------------------------------------------------------
# src/analysis/theme_development.py

import logging
from typing import Dict, Any, List
import dspy
import json
import re
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ThemeDimensionsEvaluation:
    integrated: str
    abstracted: str
    coherent: str
    theoretically_aligned: str

class ThemedevelopmentAnalysisSignature(dspy.Signature):
    """
    Signature for elevating coded data into broader, more abstract themes.
    """

    # Input Fields
    research_objectives: str = dspy.InputField(
        desc=(
            "A statement of the research goals and questions. This guides the theme development by "
            "ensuring that all identified themes remain relevant and aligned with what the study "
            "aims to explore."
        )
    )

    quotation: str = dspy.InputField(
        desc=(
            "The original quotation from the data that was analyzed during the coding stage. "
            "Serves as a reference point for developing higher-level themes that go beyond this "
            "specific excerpt."
        )
    )

    keywords: List[str] = dspy.InputField(
        desc=(
            "Keywords extracted from the original quotation, providing anchors for thematic development. "
            "These terms help ensure that the emerging themes remain grounded in the data."
        )
    )

    codes: List[Dict[str, Any]] = dspy.InputField(
        desc=(
            "A set of previously developed and validated codes derived from the quotation. "
            "Each code contains a label, definition, and an evaluation along various dimensions. "
            "Themes should build upon these codes, grouping and abstracting them into higher-level concepts."
        )
    )

    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc=(
            "The theoretical foundation guiding the analysis. Themes should reflect and engage with "
            "the specified theory, philosophical approach, and rationale. This ensures that the generated "
            "themes are not only data-driven but also theory-informed."
        )
    )

    transcript_chunk: str = dspy.InputField(
        desc=(
            "The original transcript chunk or contextual segment associated with the quotation. "
            "Provides a broader context for understanding how the identified codes and emerging themes "
            "fit into the larger narrative or dataset."
        )
    )

    # Output Fields
    themes: List[Dict[str, Any]] = dspy.OutputField(
        desc=(
            "A list of identified themes. Each theme includes:\n"
            " - **name**: The name or label of the emerging theme.\n"
            " - **description**: A detailed explanation of the theme’s meaning and how it relates to the data.\n"
            " - **associatedCodes**: Which codes contribute to this theme, demonstrating how multiple codes are synthesized.\n"
            " - **evaluation**: An evaluation of how well the theme integrates, abstracts, and aligns theoretically.\n"
            " - **theoreticalAlignment**: How this theme fits within the specified theoretical framework.\n"
        )
    )

    analysis: Dict[str, Any] = dspy.OutputField(
        desc=(
            "An analysis of the theme development process, including:\n"
            " - **methodology**: Insights on the process of moving from codes to themes.\n"
            " - **objectiveAlignment**: An evaluation of how well the themes address the original research aims.\n"
            " - **futureImplications**: How these themes might inform subsequent steps in the research, "
            "further analysis, or practical applications."
        )
    )

    def create_prompt(self, research_objectives: str, quotation: str, keywords: List[str],
                      codes: List[Dict[str, Any]], theoretical_framework: Dict[str, str],
                      transcript_chunk: str) -> str:
        # Format keywords
        keywords_formatted = "\n".join([f"- {kw}" for kw in keywords])

        # Format codes for display
        codes_formatted = "\n".join([
            f"- Code: {code.get('code', 'N/A')}\n  Definition: {code.get('definition', 'N/A')}\n"
            f"  6Rs_framework: {code.get('6Rs_framework', [])}\n" 
            for code in codes
        ])

        theory = theoretical_framework.get("theory", "N/A")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "N/A")
        rationale = theoretical_framework.get("rationale", "N/A")

        prompt = (
            f"You are a qualitative researcher skilled in thematic analysis. Your goal is to synthesize these codes "
            f"into higher-level themes that capture deeper patterns and theoretical insights.\n\n"

            f"**Quotation:**\n{quotation}\n\n"
            f"**Keywords:**\n{keywords_formatted}\n\n"
            f"**Codes Derived from Quotation:**\n{codes_formatted}\n\n"
            f"**Transcript Context:**\n{transcript_chunk}\n\n"
            f"**Research Objectives:**\n{research_objectives}\n\n"
            f"**Theoretical Framework:**\n"
            f"- Theory: {theory}\n"
            f"- Philosophical Approach: {philosophical_approach}\n"
            f"- Rationale: {rationale}\n\n"

            f"**Instructions:**\n"
            f"- Identify overarching themes that group and abstract the codes into more general concepts.\n"
            f"- Each theme should integrate multiple codes, showing how they collectively represent a larger concept.\n"
            f"- Evaluate each theme along dimensions of integration, abstraction, coherence, and theoretical alignment.\n"
            f"- Describe how each theme fits within the theoretical framework and addresses the research objectives.\n"
            f"- Present your response as a JSON object encapsulated in ```json``` code blocks.\n\n"

            f"Your final output should contain:\n"
            f" - A 'themes' array, with each theme having 'name', 'description', 'associatedCodes', "
            f"   'evaluation', and 'theoreticalAlignment'.\n"
            f" - An 'analysis' object with 'methodology', 'objectiveAlignment', "
            f"   and 'futureImplications'."
        )
        return prompt

    def parse_response(self, response: str) -> Dict[str, Any]:
        try:
            json_match = re.search(r"```json\s*(\{.*?\})\s*```", response, re.DOTALL)
            if not json_match:
                logger.error("No valid JSON found in the response.")
                logger.debug(f"Full response: {response}")
                return {}
            json_string = json_match.group(1)
            response_json = json.loads(json_string)
            return response_json
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}. Response: {response}")
            return {}
        except Exception as e:
            logger.error(f"Unexpected error during response parsing: {e}")
            return {}

    def forward(self, research_objectives: str, quotation: str, keywords: List[str], 
                codes: List[Dict[str, Any]], theoretical_framework: Dict[str, str], 
                transcript_chunk: str) -> Dict[str, Any]:
        # Attempt up to 3 times to get a valid response
        for attempt in range(3):
            try:
                logger.debug(f"Theme development attempt {attempt + 1}")
                prompt = self.create_prompt(
                    research_objectives,
                    quotation,
                    keywords,
                    codes,
                    theoretical_framework,
                    transcript_chunk
                )

                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=8000,
                    temperature=0.5
                ).strip()

                logger.debug(f"Attempt {attempt + 1} - Response received from language model.")

                parsed_response = self.parse_response(response)

                if not parsed_response:
                    raise ValueError("Parsed response is empty or invalid.")
                if not parsed_response.get("themes"):
                    raise ValueError("No themes generated. Check inputs or prompt.")

                logger.info(f"Attempt {attempt + 1} - Successfully developed {len(parsed_response.get('themes', []))} themes.")
                return parsed_response

            except ValueError as ve:
                logger.warning(f"Attempt {attempt + 1} - {ve}")
                logger.debug(f"Response: {response}")
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} - Error in ThemedevelopmentAnalysisSignature.forward: {e}", exc_info=True)

        logger.error("Failed to develop valid themes after 3 attempts.")
        return {
            "error": "Failed to develop valid themes after multiple attempts. Please review inputs and prompt."
        }


# File: analysis/theme_development_module.py
#------------------------------------------------------------------------------
# src/analysis/theme_development_module.py

import logging
from typing import Dict, Any, List
import dspy
from .theme_development import ThemedevelopmentAnalysisSignature

logger = logging.getLogger(__name__)

class ThemedevelopmentAnalysisModule(dspy.Module):
    """
    DSPy module for developing and refining themes from previously derived codes.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(ThemedevelopmentAnalysisSignature)  # Updated to use ChainOfThought

    def forward(self,
                research_objectives: str,
                quotation: str,
                keywords: List[str],
                codes: List[Dict[str, Any]],
                theoretical_framework: Dict[str, str],
                transcript_chunk: str) -> Dict[str, Any]:
        """
        Execute theme development analysis to transform codes into higher-order themes.

        Args:
            research_objectives (str): The research aims and guiding questions.
            quotation (str): The original excerpt or passage analyzed.
            keywords (List[str]): The extracted keywords from the quotation.
            codes (List[Dict[str, Any]]): The previously developed and validated codes.
            theoretical_framework (Dict[str, str]): The theoretical foundation and rationale.
            transcript_chunk (str): The contextual transcript segment associated with the quotation.

        Returns:
            Dict[str, Any]: Thematic analysis results including identified themes and their analysis.
        """
        logger.info("Starting theme development analysis.")
        logger.debug(f"Inputs: Research Objectives='{research_objectives[:100]}', Quotation='{quotation[:100]}', "
                     f"Keywords={keywords}, Codes={len(codes)}, Theoretical Framework={theoretical_framework}, "
                     f"Transcript Chunk Length={len(transcript_chunk)}")

        try:
            response = self.chain(
                research_objectives=research_objectives,
                quotation=quotation,
                keywords=keywords,
                codes=codes,
                theoretical_framework=theoretical_framework,
                transcript_chunk=transcript_chunk
            )

            logger.info("Successfully completed theme development analysis.")
            logger.debug(f"Response: {response}")

            if not response.get("themes"):
                logger.warning("No themes were generated. Possible issue with inputs or prompt formulation.")
            
            return response
        except Exception as e:
            logger.error(f"Error during theme development analysis: {e}", exc_info=True)
            return {}



################################################################################
# Module: root
################################################################################


# File: assertions.py
#------------------------------------------------------------------------------

#src/assertions_quotation.py
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def assert_pattern_representation(quotations: List[Dict[str, Any]], patterns: List[str]) -> None:
    """
    Ensure quotations represent robust patterns in the data.
    According to the paper: "Quotations should symbolize robust patterns within the data"
    and "Select quotes that demonstrate robust patterns in the data."

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata
        patterns (List[str]): Identified patterns in the data

    Raises:
        AssertionError: If quotations don't demonstrate robust patterns
    """
    if not patterns:
        raise AssertionError("No patterns provided for analysis")

    pattern_support = {pattern: [] for pattern in patterns}

    for quote in quotations:
        pattern_representation = quote.get("context", {}).get("pattern_representation", "")
        for pattern in patterns:
            if pattern.lower() in pattern_representation.lower():
                pattern_support[pattern].append(quote["quotation"])

    # Each pattern should be supported by multiple quotations for robustness
    for pattern, supporting_quotes in pattern_support.items():
        if len(supporting_quotes) < 2:
            error_msg = f"Pattern '{pattern}' is not robustly supported by multiple quotations"
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_research_objective_alignment(quotations: List[Dict[str, Any]], research_objectives: str) -> None:
    """
    Ensure quotations align with research objectives.
    According to the paper: "The evaluation objectives provide a focus or domain of relevance
    for conducting the analysis"

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata
        research_objectives (str): Research objectives guiding the analysis

    Raises:
        AssertionError: If quotations don't align with objectives
    """
    for quote in quotations:
        relevance = quote.get('analysis_value', {}).get('relevance', '')
        if not relevance or not any(obj.lower() in relevance.lower() for obj in research_objectives.split('.')):
            error_msg = f"Quotation does not align with research objectives: '{quote.get('quotation', '')[:50]}...'"
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_selective_transcription(quotations: List[Dict[str, Any]], transcript: str) -> None:
    """
    Ensure quotations are selectively chosen for relevance.
    According to the paper: "A more useful transcript is a more selective one" and
    "selecting parts relevant to the evaluation objectives"

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata
        transcript (str): Original transcript text

    Raises:
        AssertionError: If quotation selection isn't properly selective
    """
    total_words = len(transcript.split())
    quoted_words = sum(len(quote.get('quotation', '').split()) for quote in quotations)

    # Check if quotations are too verbose (should be selective)
    if quoted_words > total_words * 0.3:  # Maximum 30% of original text
        error_msg = "Quotation selection is not selective enough"
        logger.error(error_msg)
        raise AssertionError(error_msg)

def assert_creswell_categorization(quotations: List[Dict[str, Any]]) -> None:
    """
    Verify proper use of Creswell's quotation categories.
    According to the paper: "Creswell (2012) classified quotations into three types:
    discrete, embedded, and longer quotations"

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata

    Raises:
        AssertionError: If quotations don't follow Creswell's guidelines
    """
    categories = {'longer': 0, 'discrete': 0, 'embedded': 0}

    for quote in quotations:
        category = quote.get("creswell_category", "").lower()
        if category not in categories:
            error_msg = f"Invalid Creswell category '{category}'"
            logger.error(error_msg)
            raise AssertionError(error_msg)

        quote_length = len(quote.get("quotation", "").split())

        # Length guidelines based on Creswell's classifications
        if category == "longer" and quote_length < 40:
            error_msg = f"'Longer' quotation is too short ({quote_length} words)"
            logger.error(error_msg)
            raise AssertionError(error_msg)
        elif category == "discrete" and quote_length > 30:
            error_msg = f"'Discrete' quotation is too long ({quote_length} words)"
            logger.error(error_msg)
            raise AssertionError(error_msg)
        elif category == "embedded" and quote_length > 10:
            error_msg = f"'Embedded' quotation is too long ({quote_length} words)"
            logger.error(error_msg)
            raise AssertionError(error_msg)

        categories[category] += 1

def assert_reader_engagement(quotations: List[Dict[str, Any]]) -> None:
    """
    Ensure quotations enhance reader engagement.
    According to the paper: "Quotations can enhance the readers' engagement with the text"
    and should not be chosen merely to "incite controversy"

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata

    Raises:
        AssertionError: If quotations don't promote proper engagement
    """
    for quote in quotations:
        # Check for essential engagement elements
        has_context = bool(quote.get("context", {}).get("situation"))
        has_interpretation = bool(quote.get("analysis_value", {}).get("relevance"))
        has_pattern = bool(quote.get("context", {}).get("pattern_representation"))

        if not all([has_context, has_interpretation, has_pattern]):
            error_msg = "Quotation lacks essential engagement elements (context, interpretation, or pattern connection)"
            logger.error(error_msg)
            raise AssertionError(error_msg)

        # Check against controversial selection without substance
        quote_text = quote.get("quotation", "")
        if "!" in quote_text and not has_interpretation:
            error_msg = "Quotation appears selected for controversy without substantive contribution"
            logger.error(error_msg)
            raise AssertionError(error_msg)


# File: assertions_alt.py
#------------------------------------------------------------------------------
# src/assertions.py
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def assert_relevant_quotations(quotations: List[Dict[str, Any]], themes: List[str]) -> None:
    """
    Ensure that each quotation is relevant to at least one identified theme.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        themes (List[str]): List of identified themes in the analysis.
    
    Raises:
        AssertionError: If a quotation does not align with any theme.
    """
    for quote in quotations:
        if not any(theme.lower() in [t.lower() for t in quote.get('themes', [])] for theme in themes):
            error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with any identified theme."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_confidentiality(quotations: List[Dict[str, Any]], sensitive_keywords: List[str]) -> None:
    """
    Ensure that no quotations contain sensitive or identifiable information.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        sensitive_keywords (List[str]): List of keywords that should not appear in quotations.
    
    Raises:
        AssertionError: If a quotation contains sensitive information.
    """
    for quote in quotations:
        quote_text = quote.get("quotation", "").lower()
        for keyword in sensitive_keywords:
            if keyword.lower() in quote_text:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' contains sensitive keyword '{keyword}'."
                logger.error(error_msg)
                raise AssertionError(error_msg)

def assert_diversity_of_quotations(quotations: List[Dict[str, Any]], min_participants: int = 3) -> None:
    """
    Ensure that quotations represent a diverse set of participants.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        min_participants (int): Minimum number of different participants required.
    
    Raises:
        AssertionError: If quotations do not represent the required diversity.
    """
    participant_ids = {q.get("participant_id") for q in quotations}
    if len(participant_ids) < min_participants:
        error_msg = f"Only {len(participant_ids)} unique participants are represented in quotations; minimum required is {min_participants}."
        logger.error(error_msg)
        raise AssertionError(error_msg)

def assert_contextual_adequacy(quotations: List[Dict[str, Any]], transcript_chunks: List[str]) -> None:
    """
    Ensure that each quotation has adequate context.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        transcript_chunks (List[str]): List of transcript chunks.
    
    Raises:
        AssertionError: If a quotation lacks necessary context or is not found in transcript.
    """
    for quote in quotations:
        quote_text = quote.get("quotation", "")
        context = quote.get("context", "").strip()
        # Check if the quote exists in any of the transcript chunks
        in_transcript = any(quote_text in chunk for chunk in transcript_chunks)
        if not in_transcript:
            error_msg = f"Quotation '{quote_text[:50]}...' not found in any transcript chunk."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        if not context:
            error_msg = f"Quotation '{quote_text[:50]}...' lacks contextual information."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_philosophical_alignment(quotations: List[Dict[str, Any]], theoretical_framework: str) -> None:
    """
    Ensure that quotations align with the researcher's philosophical stance.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        theoretical_framework (str): The theoretical and philosophical framework guiding the analysis.
    
    Raises:
        AssertionError: If any quotation does not align with the specified orientation.
    """
    framework_lower = theoretical_framework.lower()
    for quote in quotations:
        alignment_score = float(quote.get("alignment_score", 0.0))
        analysis_notes = quote.get("analysis_notes", "").lower()
        if "constructivist" in framework_lower:
            if alignment_score < 0.7 or "subjective" not in analysis_notes:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with constructivist orientation."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        elif "critical realism" in framework_lower:
            if alignment_score < 0.7 or "objective" not in analysis_notes:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with critical realism orientation."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        else:
            if alignment_score < 0.7:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' has insufficient alignment score."
                logger.error(error_msg)
                raise AssertionError(error_msg)


# File: assertions_coding.py
#------------------------------------------------------------------------------
# src/assertions_coding.py

import logging
from typing import List, Dict, Any, Set

logger = logging.getLogger(__name__)

def assert_robustness(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure coding accurately and comprehensively captures the essence of the data.

    Raises:
        AssertionError: If any code does not comprehensively represent the data.
    """
    for code in codes:
        data_extracts = code.get("data_extracts", [])
        if not data_extracts:
            error_msg = f"Code '{code.get('code_name', '')}' has no data extracts."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        # Example metric: keyword diversity (assuming each extract has 'keywords')
        keyword_set: Set[str] = set()
        for extract in data_extracts:
            keywords = extract.get("keywords", [])
            keyword_set.update([kw.lower() for kw in keywords])
        
        if len(keyword_set) < 2:
            error_msg = f"Code '{code.get('code_name', '')}' lacks keyword diversity."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code.get('code_name', '')}' passes robustness check.")

def assert_reflectiveness(codes: List[Dict[str, Any]], theoretical_framework: Dict[str, str]) -> None:
    """
    Ensure codes reflect the relationship between data and the theoretical framework.

    Raises:
        AssertionError: If any code does not align with the theoretical framework.
    """
    framework_concepts = set(k.lower() for k in theoretical_framework.keys())
    
    for code in codes:
        data_extracts = code.get("data_extracts", [])
        aligned = False
        for extract in data_extracts:
            keywords = extract.get("keywords", [])
            if any(kw.lower() in framework_concepts for kw in keywords):
                aligned = True
                break
        if not aligned:
            error_msg = f"Code '{code.get('code_name', '')}' does not align with the theoretical framework."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code.get('code_name', '')}' passes reflectiveness check.")

def assert_resplendence(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes offer rich and comprehensive explanations of the context.

    Raises:
        AssertionError: If any code lacks comprehensive contextual explanations.
    """
    for code in codes:
        analysis_notes = code.get("analysis_notes", "")
        if not analysis_notes or len(analysis_notes.split()) < 50:
            error_msg = f"Code '{code.get('code_name', '')}' lacks comprehensive analysis notes."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code.get('code_name', '')}' passes resplendence check.")

def assert_relevance(codes: List[Dict[str, Any]], research_objectives: str) -> None:
    """
    Ensure codes accurately represent the data and align with research objectives.

    Raises:
        AssertionError: If any code does not align with research objectives.
    """
    objectives = [obj.strip().lower() for obj in research_objectives.split('.') if obj.strip()]
    if not objectives:
        error_msg = "No research objectives provided for relevance check."
        logger.error(error_msg)
        raise AssertionError(error_msg)
    
    for code in codes:
        code_name = code.get("code_name", "")
        aligns = False
        data_extracts = code.get("data_extracts", [])
        for extract in data_extracts:
            relevance = extract.get("relevance", "")
            if any(obj in relevance.lower() for obj in objectives):
                aligns = True
                break
        if not aligns:
            error_msg = f"Code '{code_name}' does not align with any research objectives."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes relevance check.")

def assert_radicality(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes provide unique insights and may challenge dominant narratives.

    Raises:
        AssertionError: If any code lacks uniqueness or does not offer fresh perspectives.
    """
    seen_codes = set()
    for code in codes:
        code_name = code.get("code_name", "").lower()
        if code_name in seen_codes:
            error_msg = f"Code '{code.get('code_name', '')}' is not unique."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        seen_codes.add(code_name)
        
        data_extracts = code.get("data_extracts", [])
        unique_insights = set()
        for extract in data_extracts:
            insight = extract.get("insight", "").lower()
            if insight:
                unique_insights.add(insight)
        
        if len(unique_insights) < 1:
            error_msg = f"Code '{code.get('code_name', '')}' does not offer unique insights."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code.get('code_name', '')}' passes radicality check.")

def assert_righteousness(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes fit logically within the coding framework and avoid overlaps.

    Raises:
        AssertionError: If any code overlaps conceptually with others or lacks logical consistency.
    """
    code_definitions = {code.get("code_name", "").lower(): code for code in codes}
    
    for code_name, code in code_definitions.items():
        related_codes = code.get("related_codes", [])
        for related_code in related_codes:
            related_code_lower = related_code.lower()
            if related_code_lower not in code_definitions:
                error_msg = f"Related code '{related_code}' for code '{code_name}' does not exist."
                logger.error(error_msg)
                raise AssertionError(error_msg)
            
            # Example overlap check: shared keywords
            code_keywords = set(kw.lower() for extract in code.get("data_extracts", []) for kw in extract.get("keywords", []))
            related_keywords = set(kw.lower() for extract in code_definitions[related_code_lower].get("data_extracts", []) for kw in extract.get("keywords", []))
            overlap = code_keywords.intersection(related_keywords)
            if len(overlap) > 5:  # Threshold for overlap
                error_msg = f"Code '{code_name}' overlaps significantly with related code '{related_code}'."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes righteousness check.")

def assert_code_representation(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure each code accurately encapsulates underlying concepts and meanings.

    Raises:
        AssertionError: If any code does not consistently reflect its definition.
    """
    for code in codes:
        code_name = code.get("code_name", "")
        definition = code.get("definition", "").lower()
        data_extracts = code.get("data_extracts", [])
        
        for extract in data_extracts:
            quotation = extract.get("quotation", "").lower()
            if definition not in quotation:
                error_msg = f"Data extract in code '{code_name}' does not reflect the code's definition."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes code representation check.")

def assert_code_specificity(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes are specific enough to convey clear meanings within the theoretical framework.

    Raises:
        AssertionError: If any code is too vague or broad.
    """
    for code in codes:
        code_name = code.get("code_name", "")
        if len(code_name.split()) < 2:
            error_msg = f"Code name '{code_name}' is too broad or vague."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        data_extracts = code.get("data_extracts", [])
        for extract in data_extracts:
            keywords = extract.get("keywords", [])
            for kw in keywords:
                if len(kw.split()) < 2:
                    error_msg = f"Keyword '{kw}' in code '{code_name}' is too broad or vague."
                    logger.error(error_msg)
                    raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes specificity check.")

def assert_code_relevance(codes: List[Dict[str, Any]], research_objectives: str) -> None:
    """
    Ensure each code aligns with research objectives and contributes to answering research questions.

    Raises:
        AssertionError: If any code does not contribute to the research objectives.
    """
    objectives = [obj.strip().lower() for obj in research_objectives.split('.') if obj.strip()]
    if not objectives:
        error_msg = "No research objectives provided for code relevance check."
        logger.error(error_msg)
        raise AssertionError(error_msg)
    
    for code in codes:
        code_name = code.get("code_name", "")
        aligns = False
        data_extracts = code.get("data_extracts", [])
        for extract in data_extracts:
            relevance = extract.get("relevance", "")
            if any(obj in relevance.lower() for obj in objectives):
                aligns = True
                break
        if not aligns:
            error_msg = f"Code '{code_name}' does not contribute to any research objectives."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes code relevance check.")

def assert_code_distinctiveness(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes are distinct and do not overlap or duplicate each other.

    Raises:
        AssertionError: If any codes are too similar or duplicated.
    """
    seen_definitions = {}
    for code in codes:
        code_name = code.get("code_name", "").lower()
        definition = code.get("definition", "").lower()
        if definition in seen_definitions:
            error_msg = f"Code '{code_name}' has a duplicate definition with code '{seen_definitions[definition]}'."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        seen_definitions[definition] = code_name
        logger.debug(f"Code '{code_name}' is distinct.")

def run_all_coding_assertions(
    codes: List[Dict[str, Any]],
    research_objectives: str,
    theoretical_framework: Dict[str, str]
) -> None:
    """
    Run all coding assertions to validate the coding framework.

    Args:
        codes (List[Dict[str, Any]]): List of codes with metadata.
        research_objectives (str): Research objectives guiding the analysis.
        theoretical_framework (Dict[str, str]): Theoretical framework for context.

    Raises:
        AssertionError: If any of the assertions fail.
    """
    assert_robustness(codes)
    assert_reflectiveness(codes, theoretical_framework)
    assert_resplendence(codes)
    assert_relevance(codes, research_objectives)
    assert_radicality(codes)
    assert_righteousness(codes)
    assert_code_representation(codes)
    assert_code_specificity(codes)
    assert_code_relevance(codes, research_objectives)
    assert_code_distinctiveness(codes)
    logger.info("All coding assertions passed successfully.")


# File: assertions_keyword.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
import dspy
from functools import lru_cache

logger = logging.getLogger(__name__)

@dataclass
class AssertionThresholds:
    """Configuration thresholds for keyword assertions."""
    min_analysis_words: int = 10
    min_keywords_per_r: int = 2
    min_theoretical_alignment: float = 0.7
    min_repetition_count: int = 2
    min_framework_aspects: int = 2
    min_total_keywords: int = 3

@dataclass
class AssertionConfig:
    """Complete configuration for keyword assertions."""
    enabled_assertions: List[str] = None
    required_passes: int = 3
    thresholds: AssertionThresholds = None
    strict_mode: bool = False
    detailed_logging: bool = True
    
    def __post_init__(self):
        if self.enabled_assertions is None:
            self.enabled_assertions = ["all"]
        if self.thresholds is None:
            self.thresholds = AssertionThresholds()

class KeywordAssertions:
    """Enhanced DSPy assertions for granular keyword validation in thematic analysis."""
    
    def __init__(self, config: Optional[AssertionConfig] = None):
        """Initialize with optional configuration."""
        self.config = config or AssertionConfig()
        logger.info("Initialized KeywordAssertions with config: %s", self.config)
    
    @lru_cache(maxsize=128)
    def _tokenize_text(self, text: str) -> Set[str]:
        """Tokenize text into words, with caching for performance."""
        return set(text.lower().strip().split())
    
    def _validate_realness_keyword(self, keyword: Dict[str, Any]) -> bool:
        """Validate a single keyword's realness analysis."""
        kw_text = keyword.get("keyword", "")
        realness_value = keyword.get("analysis_value", {}).get("realness", "").strip()
        
        if not realness_value:
            logger.error(f"Keyword '{kw_text}' lacks Realness analysis")
            return False
        
        word_count = len(realness_value.split())
        if word_count < self.config.thresholds.min_analysis_words:
            logger.warning(
                f"Realness analysis for '{kw_text}' has only {word_count} words "
                f"(min: {self.config.thresholds.min_analysis_words})"
            )
            return False
        
        return True
    
    def assert_realness_analysis(self, keywords: List[Dict[str, Any]], quotation: str) -> bool:
        """Validates the Realness dimension analysis for each keyword."""
        logger.debug("Starting Realness Analysis assertion")
        realness_keywords = [k for k in keywords if "Realness" in k.get("6Rs_framework", [])]
        
        if len(realness_keywords) < self.config.thresholds.min_keywords_per_r:
            logger.warning(
                f"Found only {len(realness_keywords)} Realness keywords, "
                f"minimum required: {self.config.thresholds.min_keywords_per_r}"
            )
            return False
        
        for keyword in realness_keywords:
            if not self._validate_realness_keyword(keyword):
                return False
        
        logger.debug("Completed Realness Analysis assertion")
        return True
    
    def _calculate_theoretical_alignment(self, analysis_text: str, theoretical_framework: Dict[str, str]) -> float:
        """Calculate theoretical alignment score."""
        theory = theoretical_framework.get("theory", "").lower()
        philosophical_approach = theoretical_framework.get("philosophical_approach", "").lower()
        
        theory_words = self._tokenize_text(theory + " " + philosophical_approach)
        analysis_words = self._tokenize_text(analysis_text)
        
        if not theory_words:
            return 0.0
            
        return len(theory_words.intersection(analysis_words)) / len(theory_words)
    
    def assert_richness_analysis(self, keywords: List[Dict[str, Any]], 
                               theoretical_framework: Dict[str, str]) -> bool:
        """Validates the Richness dimension analysis with theoretical alignment."""
        logger.debug("Starting Richness Analysis assertion")
        richness_keywords = [k for k in keywords if "Richness" in k.get("6Rs_framework", [])]
        
        if len(richness_keywords) < self.config.thresholds.min_keywords_per_r:
            logger.warning(
                f"Found only {len(richness_keywords)} Richness keywords, "
                f"minimum required: {self.config.thresholds.min_keywords_per_r}"
            )
            return False
        
        for keyword in richness_keywords:
            kw_text = keyword.get("keyword", "")
            richness_value = keyword.get("analysis_value", {}).get("richness", "").strip()
            
            if not richness_value:
                logger.error(f"Keyword '{kw_text}' lacks Richness analysis")
                return False
            
            alignment_score = self._calculate_theoretical_alignment(richness_value, theoretical_framework)
            
            if alignment_score < self.config.thresholds.min_theoretical_alignment:
                logger.warning(
                    f"Low theoretical alignment (score: {alignment_score:.2f}) for '{kw_text}'"
                )
                return False
        
        logger.debug("Completed Richness Analysis assertion")
        return True
    
    def _count_keyword_occurrences(self, keyword: str, contents: List[str]) -> int:
        """Count occurrences of a keyword in contextualized contents."""
        return sum(keyword.lower() in content.lower() for content in contents)
    
    def assert_repetition_analysis(self, keywords: List[Dict[str, Any]], 
                                 contextualized_contents: List[str]) -> bool:
        """Validates the Repetition dimension with contextual evidence."""
        logger.debug("Starting Repetition Analysis assertion")
        
        repetition_keywords = [k for k in keywords if "Repetition" in k.get("6Rs_framework", [])]
        
        if len(repetition_keywords) < self.config.thresholds.min_keywords_per_r:
            logger.warning(
                f"Found only {len(repetition_keywords)} Repetition keywords, "
                f"minimum required: {self.config.thresholds.min_keywords_per_r}"
            )
            return False
        
        for keyword in repetition_keywords:
            kw_text = keyword.get("keyword", "").lower()
            repetition_value = keyword.get("analysis_value", {}).get("repetition", "").strip()
            
            if not repetition_value:
                logger.error(f"Keyword '{kw_text}' lacks Repetition analysis")
                return False
            
            occurrences = self._count_keyword_occurrences(kw_text, contextualized_contents)
            
            if occurrences < self.config.thresholds.min_repetition_count:
                logger.warning(
                    f"Keyword '{kw_text}' appears only {occurrences} times "
                    f"(min: {self.config.thresholds.min_repetition_count})"
                )
                return False
        
        logger.debug("Completed Repetition Analysis assertion")
        return True
    
    def _check_framework_coverage(self, keywords: List[Dict[str, Any]]) -> bool:
        """Check if keywords have sufficient framework aspect coverage."""
        aspects_coverage = [len(kw.get("6Rs_framework", [])) for kw in keywords]
        return all(ac >= self.config.thresholds.min_framework_aspects for ac in aspects_coverage)

def validate_keywords_dspy(
    keywords: List[Dict[str, Any]], 
    quotation: str,
    contextualized_contents: List[str], 
    research_objectives: str,
    theoretical_framework: Dict[str, str],
    config: Optional[AssertionConfig] = None
) -> Dict[str, Any]:
    """
    Comprehensive keyword validation with configurable assertions and detailed reporting.
    
    Args:
        keywords: List of keyword dictionaries to validate
        quotation: Original quotation text
        contextualized_contents: List of contextual content strings
        research_objectives: Research objectives text
        theoretical_framework: Dictionary containing theoretical framework details
        config: Optional assertion configuration
    
    Returns:
        Dict containing validation results and any suggestions/warnings
    """
    logger.info("Starting keyword validation with DSPy assertions")
    config = config or AssertionConfig()
    assertions = KeywordAssertions(config)
    
    validation_report = {
        "passed": True,
        "assertions_run": [],
        "failed_assertions": [],
        "suggestions": [],
        "warnings": []
    }
    
    try:
        passes = 0
        assertion_methods = {
            "realness": lambda: assertions.assert_realness_analysis(keywords, quotation),
            "richness": lambda: assertions.assert_richness_analysis(keywords, theoretical_framework),
            "repetition": lambda: assertions.assert_repetition_analysis(keywords, contextualized_contents)
        }
        
        # Run enabled assertions
        enabled = config.enabled_assertions
        for assertion_name, assertion_method in assertion_methods.items():
            if "all" in enabled or assertion_name in enabled:
                try:
                    logger.debug(f"Running {assertion_name} assertion")
                    if assertion_method():
                        validation_report["assertions_run"].append(assertion_name)
                        passes += 1
                    else:
                        validation_report["failed_assertions"].append({
                            "type": assertion_name,
                            "error": f"{assertion_name} assertion failed"
                        })
                        validation_report["passed"] = False
                except Exception as e:
                    logger.error(f"{assertion_name} assertion failed with error: {str(e)}")
                    validation_report["failed_assertions"].append({
                        "type": assertion_name,
                        "error": str(e)
                    })
                    validation_report["passed"] = False
        
        # Framework coverage check
        if not assertions._check_framework_coverage(keywords):
            message = (
                f"Some keywords cover fewer than "
                f"{config.thresholds.min_framework_aspects} framework aspects"
            )
            validation_report["warnings"].append(message)
            dspy.Suggest(False, message)
        
        # Total keywords check
        if len(keywords) < config.thresholds.min_total_keywords:
            message = (
                f"Total keywords ({len(keywords)}) below minimum threshold "
                f"({config.thresholds.min_total_keywords})"
            )
            validation_report["warnings"].append(message)
            dspy.Suggest(False, message)
        
        # Required passes check
        if passes < config.required_passes:
            message = f"Insufficient assertion passes: {passes}/{config.required_passes}"
            validation_report["warnings"].append(message)
            if config.strict_mode:
                validation_report["passed"] = False
                validation_report["failed_assertions"].append({
                    "type": "required_passes",
                    "error": message
                })
        
        if config.detailed_logging:
            logger.debug("Validation report: %s", validation_report)
        
        return validation_report
    
    except Exception as e:
        error_msg = f"Unexpected error during keyword validation: {str(e)}"
        logger.error(error_msg, exc_info=True)
        validation_report["passed"] = False
        validation_report["failed_assertions"].append({
            "type": "unknown",
            "error": error_msg
        })
        return validation_report

# File: clearcache.py
#------------------------------------------------------------------------------
import os
import shutil
import logging
from elasticsearch import Elasticsearch
from typing import List, Optional
import json

def setup_logging() -> logging.Logger:
    """Configure and return a logger instance."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def clear_elasticsearch_cache(logger: logging.Logger, es_host: str = "http://localhost:9200") -> None:
    """Clear Elasticsearch indices."""
    try:
        es = Elasticsearch(es_host)
        if es.ping():
            indices_to_delete = ["contextual_bm25_index"]
            for index in indices_to_delete:
                if es.indices.exists(index=index):
                    es.indices.delete(index=index)
                    logger.info(f"Deleted Elasticsearch index: {index}")
            logger.info("Elasticsearch cache cleared successfully")
    except Exception as e:
        logger.error(f"Error clearing Elasticsearch cache: {e}")

def remove_file_or_directory(path: str, logger: logging.Logger) -> None:
    """Remove a file or directory with proper error handling."""
    try:
        if os.path.isfile(path):
            os.remove(path)
            logger.info(f"Removed file: {path}")
        elif os.path.isdir(path):
            shutil.rmtree(path, ignore_errors=True)
            logger.info(f"Removed directory: {path}")
    except Exception as e:
        logger.error(f"Error removing {path}: {e}")

def clear_pickle_files(data_dir: str, logger: logging.Logger) -> None:
    """Remove all pickle files in the specified directory and its subdirectories."""
    try:
        for root, _, files in os.walk(data_dir):
            for file in files:
                if file.endswith((".pkl", ".pickle")):
                    file_path = os.path.join(root, file)
                    os.remove(file_path)
                    logger.info(f"Removed pickle file: {file_path}")
    except Exception as e:
        logger.error(f"Error clearing pickle files: {e}")

def clear_temporary_files(data_dir: str, logger: logging.Logger) -> None:
    """Remove temporary files like .pyc, .pyo, and __pycache__ directories."""
    try:
        # First, collect all paths to remove
        cache_dirs = []
        compiled_files = []
        
        for root, dirs, files in os.walk(data_dir, topdown=True):
            # Collect __pycache__ directories
            if "__pycache__" in dirs:
                cache_dir = os.path.join(root, "__pycache__")
                cache_dirs.append(cache_dir)
                dirs.remove("__pycache__")  # Prevent recursing into __pycache__
            
            # Collect .pyc and .pyo files
            for file in files:
                if file.endswith((".pyc", ".pyo")):
                    file_path = os.path.join(root, file)
                    compiled_files.append(file_path)
        
        # Remove collected paths
        for cache_dir in cache_dirs:
            if os.path.exists(cache_dir):  # Check again in case it was already removed
                shutil.rmtree(cache_dir, ignore_errors=True)
                logger.info(f"Removed __pycache__ directory: {cache_dir}")
        
        for file_path in compiled_files:
            if os.path.exists(file_path):  # Check again in case it was already removed
                os.remove(file_path)
                logger.info(f"Removed compiled Python file: {file_path}")
                
    except Exception as e:
        logger.error(f"Error clearing temporary files: {e}")

def create_directories(directories: List[str], logger: logging.Logger) -> None:
    """Create necessary directories."""
    for directory in directories:
        try:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"Created directory: {directory}")
        except Exception as e:
            logger.error(f"Error creating directory {directory}: {e}")

def clear_model_cache(cache_dir: str, logger: logging.Logger) -> None:
    """Clear any cached model files or artifacts."""
    try:
        model_paths = [
            os.path.join(cache_dir, "models"),
            os.path.join(cache_dir, "optimized_models"),
            os.path.join(cache_dir, "checkpoints")
        ]
        for path in model_paths:
            if os.path.exists(path):
                shutil.rmtree(path, ignore_errors=True)
                logger.info(f"Cleared model cache: {path}")
    except Exception as e:
        logger.error(f"Error clearing model cache: {e}")

def clear_output_files(output_dir: str, logger: logging.Logger) -> None:
    """Clear generated output files."""
    try:
        # Clear JSON output files
        for root, _, files in os.walk(output_dir):
            for file in files:
                if file.endswith((".json", ".jsonl")):
                    file_path = os.path.join(root, file)
                    os.remove(file_path)
                    logger.info(f"Removed output file: {file_path}")
    except Exception as e:
        logger.error(f"Error clearing output files: {e}")

def clear_all_cache(base_dir: Optional[str] = None) -> None:
    """Main function to clear all cache and temporary files."""
    logger = setup_logging()
    
    if base_dir is None:
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    logger.info(f"Starting cache clearing process in: {base_dir}")
    
    # Define paths relative to base directory
    data_dir = os.path.join(base_dir, "data")
    cache_dir = os.path.join(base_dir, "cache")
    logs_dir = os.path.join(base_dir, "logs")
    output_dir = os.path.join(base_dir, "output")
    
    # 1. Clear Elasticsearch cache
    clear_elasticsearch_cache(logger)
    
    # 2. Clear data directories
    data_paths = [
        os.path.join(data_dir, "contextual_db"),
        os.path.join(data_dir, "optimized_program.json"),
        logs_dir,
        cache_dir,
        os.path.join(data_dir, "contextual_db", "faiss_index.bin"),
        output_dir
    ]
    
    for path in data_paths:
        remove_file_or_directory(path, logger)
    
    # 3. Clear pickle files
    clear_pickle_files(data_dir, logger)
    
    # 4. Clear temporary Python files from the entire project directory
    clear_temporary_files(base_dir, logger)
    
    # 5. Clear model cache
    clear_model_cache(cache_dir, logger)
    
    # 6. Clear output files
    clear_output_files(output_dir, logger)
    
    # 7. Recreate necessary directories
    required_dirs = [
        data_dir,
        os.path.join(data_dir, "contextual_db"),
        logs_dir,
        cache_dir,
        output_dir
    ]
    create_directories(required_dirs, logger)
    
    logger.info("Cache clearing completed successfully")

if __name__ == "__main__":
    clear_all_cache()


################################################################################
# Module: config
################################################################################


# File: config/keyword_config.py
#------------------------------------------------------------------------------
# src/config/keyword_config.py
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

@dataclass
class AssertionThresholds:
    """Configuration thresholds for keyword assertions."""
    min_analysis_words: int = 10
    min_keywords_per_r: int = 2
    min_theoretical_alignment: float = 0.7
    min_repetition_count: int = 2
    min_framework_aspects: int = 2
    min_total_keywords: int = 3

@dataclass
class AssertionConfig:
    """Complete configuration for keyword assertions."""
    enabled_assertions: List[str]
    required_passes: int = 3
    thresholds: AssertionThresholds = None
    strict_mode: bool = False
    detailed_logging: bool = True

@dataclass
class KeywordExtractionConfig:
    """Configuration for keyword extraction module."""
    max_keywords: int = 10
    min_confidence: float = 0.7
    batch_size: int = 100
    assertion_config: AssertionConfig = None

@dataclass
class PipelineConfig:
    """Overall pipeline configuration."""
    keyword_config: KeywordExtractionConfig
    optimization_enabled: bool = True
    parallel_processing: bool = True
    cache_results: bool = True
    max_retries: int = 3

# File: config/model_config.py
#------------------------------------------------------------------------------
from enum import Enum
from typing import Optional
from dataclasses import dataclass

class ModelProvider(Enum):
    OPENAI = "openai"
    GOOGLE = "google"
    DEEPSEEK = "deepseek"

@dataclass
class ModelConfig:
    provider: ModelProvider
    model_name: str
    api_key_env: str
    max_tokens: int = 8192
    temperature: float = 0.7

DEFAULT_MODELS = {
    ModelProvider.OPENAI: ModelConfig(
        provider=ModelProvider.OPENAI,
        model_name="gpt-4o-mini",
        api_key_env="OPENAI_API_KEY",
    ),
    ModelProvider.GOOGLE: ModelConfig(
        provider=ModelProvider.GOOGLE,
        model_name="gemini-2.0-flash-thinking-exp-01-21",
        api_key_env="GOOGLE_API_KEY",
    ),
    ModelProvider.DEEPSEEK: ModelConfig(
        provider=ModelProvider.DEEPSEEK,
        model_name="deepseek/deepseek-reasoner",
        api_key_env="DEEPSEEK_API_KEY",
    ),
}

def get_model_config(provider: Optional[str] = None) -> ModelConfig:
    """Get model configuration based on provider name."""
    if not provider:
        return DEFAULT_MODELS[ModelProvider.OPENAI]
    
    try:
        provider_enum = ModelProvider(provider.lower())
        return DEFAULT_MODELS[provider_enum]
    except (ValueError, KeyError):
        raise ValueError(f"Unsupported provider: {provider}. Available providers: {[p.value for p in ModelProvider]}")


################################################################################
# Module: convert
################################################################################


# File: convert/__init__.py
#------------------------------------------------------------------------------


# File: convert/convertcodingforgrouping.py
#------------------------------------------------------------------------------
#convertcodingforgrouping.py
import json
import os
from typing import List, Dict, Any

def convert_query_results(input_file: str, output_dir: str, output_file: str):
    """
    Convert query results to only extract the code arrays from all entries and write them to an output JSON file.
    If multiple entries have the same 'code' name, only the first instance is included in the final output.
    The final output is structured as a list containing a single dictionary with a 'codes' key.
    """
    
    # Ensure the output directory exists
    os.makedirs(os.path.join(output_dir, 'input'), exist_ok=True)

    try:
        # Read the input JSON file
        with open(input_file, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
        
        # Ensure we are dealing with a list of entries
        if not isinstance(input_data, list):
            input_data = [input_data]

        # A dictionary to keep track of codes we've already seen to avoid duplicates
        seen_codes = {}
        final_codes = []

        # Iterate over each entry
        for entry in input_data:
            codes = entry.get('codes', [])
            for code_entry in codes:
                # code_entry should be a dictionary containing 'code' and other keys
                code_name = code_entry.get('code', '').strip()
                if code_name and code_name not in seen_codes:
                    seen_codes[code_name] = True
                    final_codes.append(code_entry)

        # Structure the final output as a list containing one dictionary with 'codes' key
        structured_output = {"codes": final_codes}
        output_list = [structured_output]

        # Write the structured output to the output JSON file
        output_path = os.path.join(output_dir, 'input', output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_list, f, indent=4, ensure_ascii=False)

        print(f"Successfully extracted and saved unique codes to {output_path}")

    except FileNotFoundError:
        print(f"Error: The file {input_file} was not found.")
    except json.JSONDecodeError as jde:
        print(f"Error decoding JSON: {jde}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    # Example usage; adjust paths as needed
    convert_query_results(
        input_file='src/convert/query_results_coding_analysis.json',  # Replace with your input file path
        output_dir='data/',                                           # Replace with your desired output directory
        output_file='queries_themes.json'                            # Replace with your desired output file name
    )


# File: convert/convertcodingfortheme.py
#------------------------------------------------------------------------------
# src/convert/convertcodingfortheme.py

import json
import os
from typing import List, Dict, Any, Optional

def extract_coding_info(entry: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extracts and simplifies coding information from a single JSON entry.
    This revised version keeps codes in dictionary format instead of just extracting their 'code' strings.
    """
    try:
        coding_info = entry.get('coding_info', {})
        quotation = coding_info.get('quotation', '').strip()
        keywords = coding_info.get('keywords', [])
        research_objectives = coding_info.get('research_objectives', '').strip()
        theoretical_framework = coding_info.get('theoretical_framework', {})

        # Extract codes as dictionaries if possible
        codes = entry.get('codes', [])
        processed_codes = []
        for code_entry in codes:
            # Ensure the code_entry is a dictionary and has at least the 'code' key
            if isinstance(code_entry, dict) and 'code' in code_entry:
                processed_codes.append(code_entry)
            else:
                # If the code entry is not in the expected format, skip it or handle as needed
                print(f"Warning: Code entry is not a valid dictionary or missing 'code' key: {code_entry}")

        # Combine transcript chunks
        retrieved_chunks = entry.get('retrieved_chunks', [])
        transcript_contents = [chunk.get('original_content', '').strip() for chunk in retrieved_chunks]
        transcript_chunk = ' '.join([quotation] + transcript_contents)

        simplified_entry = {
            "quotation": quotation,
            "keywords": keywords,
            "codes": processed_codes,  # Keep codes as dictionaries
            "research_objectives": research_objectives,
            "theoretical_framework": theoretical_framework,
            "transcript_chunk": transcript_chunk
        }

        return simplified_entry

    except Exception as e:
        print(f"Error processing entry: {e}")
        return {}

def convert_query_results(input_file: str, output_dir: str, output_file: str, sub_dir: Optional[str] = 'input'):
    """
    Convert query results to a simplified format and save them in the specified directory.
    
    The simplified format includes:
    - quotation: The actual quotation text.
    - keywords: List of keywords associated with the quotation.
    - codes: List of code dictionaries associated with the quotation.
    - research_objectives: The research objectives.
    - theoretical_framework: The theoretical framework details.
    - transcript_chunk: The combined transcript chunk for context.
    """
    # Ensure the output directory exists
    if sub_dir:
        target_dir = os.path.join(output_dir, sub_dir)
    else:
        target_dir = output_dir
    os.makedirs(target_dir, exist_ok=True)

    try:
        # Read the input JSON file
        with open(input_file, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
        
        # Check if input_data is a list; if not, make it a list
        if not isinstance(input_data, list):
            input_data = [input_data]

        simplified_data: List[Dict[str, Any]] = []

        for idx, entry in enumerate(input_data):
            simplified_entry = extract_coding_info(entry)
            if simplified_entry:
                simplified_data.append(simplified_entry)
            else:
                print(f"Entry at index {idx} could not be processed and was skipped.")

        # Define the output path
        output_path = os.path.join(target_dir, output_file)

        # Write the simplified data to the output JSON file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(simplified_data, f, indent=4, ensure_ascii=False)

        print(f"Successfully converted and saved to {output_path}")

    except FileNotFoundError:
        print(f"Error: The file {input_file} was not found.")
    except json.JSONDecodeError as jde:
        print(f"Error decoding JSON: {jde}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    # Example usage; this will only run when the script is executed directly
    convert_query_results(
        input_file='data/output/query_results_coding_analysis.json',  # Replace with your input file path
        output_dir='data',                                           # Set to 'data' to avoid double 'input' subdirectory
        output_file='queries_theme.json'                            # Replace with your desired output file name
        # sub_dir='input'  # Optional: Specify if needed
    )


# File: convert/convertgroupingfortheme.py
#------------------------------------------------------------------------------
# src/convert/convertgroupingfortheme.py

import json
import os
import logging

logger = logging.getLogger(__name__)

def convert_query_results(input_file: str, output_dir: str, output_file: str):
    """
    Converts query_results_grouping.json to queries_theme.json.

    Args:
        input_file (str): Path to query_results_grouping.json.
        output_dir (str): Directory where queries_theme.json will be saved.
        output_file (str): Name of the output file (e.g., queries_theme.json).
    """
    try:
        logger.info(f"Starting conversion from {input_file} to {os.path.join(output_dir, output_file)}")

        if not os.path.exists(input_file):
            logger.error(f"Input file {input_file} does not exist.")
            return

        with open(input_file, 'r', encoding='utf-8') as f:
            grouping_results = json.load(f)

        if not grouping_results:
            logger.error(f"Input file {input_file} is empty.")
            return

        first_result = grouping_results[0]

        # Assuming info.json has already been handled in main.py's generate_theme_input
        # Here, we're just copying necessary fields
        codes = first_result.get("grouping_info", {}).get("codes", [])
        groupings = first_result.get("groupings", [])

        # Placeholders; actual implementation should retrieve these from the pipeline
        quotation = "Original quotation from previous step."
        keywords = ["improvement", "reasoning", "innovation"]
        transcript_chunk = "A relevant transcript chunk providing context."

        queries_theme = [
            {
                "quotation": quotation,
                "keywords": keywords,
                "codes": codes,
                "research_objectives": "To develop advanced reasoning capabilities in AI models...",
                "theoretical_framework": {
                    "theory": "Scaling laws of deep learning suggest...",
                    "philosophical_approach": "A pragmatic, engineering-driven approach...",
                    "rationale": "Reasoning is identified as a crucial bottleneck..."
                },
                "transcript_chunk": transcript_chunk,
                "groupings": groupings
            }
        ]

        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(queries_theme, f, indent=4)

        logger.info(f"Successfully converted and saved to {output_path}")
    except Exception as e:
        logger.error(f"Error during conversion: {e}", exc_info=True)


# File: convert/convertkeywordforcoding.py
#------------------------------------------------------------------------------
# src/convert/convertkeywordforcoding.py

import json
import os
import re
from typing import List, Dict, Any

def split_into_sentences(text: str) -> List[str]:
    """
    Splits a given text into sentences using regular expressions.
    """
    sentence_endings = re.compile(r'(?<=[.!?]) +')
    sentences = sentence_endings.split(text.strip())
    return [sentence.strip() for sentence in sentences if sentence.strip()]

def extract_keywords(keywords_list: List[Dict[str, Any]]) -> List[str]:
    """
    Extracts the keyword strings from the keywords list.
    """
    return [keyword_entry.get('keyword', '') for keyword_entry in keywords_list if 'keyword' in keyword_entry]

def convert_query_results(input_file: str, output_dir: str, output_file: str):
    """
    Convert query results to a simplified format and save them in the specified directory.
    """
    # Ensure the output directory exists
    os.makedirs(os.path.join(output_dir, 'input'), exist_ok=True)

    try:
        # Read the input JSON file
        with open(input_file, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
        
        simplified_data: List[Dict[str, Any]] = []

        for entry in input_data:
            quotation_info = entry.get('quotation_info', {})
            full_quotation = quotation_info.get('quotation', '')
            research_objectives = quotation_info.get('research_objectives', '')
            theoretical_framework = quotation_info.get('theoretical_framework', {})
            transcript_chunk = full_quotation

            # Split the full quotation into sentences
            sentences = split_into_sentences(full_quotation)

            # Extract keywords
            keywords = extract_keywords(entry.get('keywords', []))

            for sentence in sentences:
                simplified_entry = {
                    "quotation": sentence,
                    "keywords": keywords,
                    "research_objectives": research_objectives,
                    "theoretical_framework": theoretical_framework,
                    "transcript_chunk": transcript_chunk
                }
                simplified_data.append(simplified_entry)

        # Define the output path with '_standard' suffix
        output_path = os.path.join(output_dir, 'input', output_file)

        # Write the simplified data to the output JSON file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(simplified_data, f, indent=4, ensure_ascii=False)

        print(f"Successfully converted and saved to {output_path}")

    except FileNotFoundError:
        print(f"Error: The file {input_file} was not found.")
    except json.JSONDecodeError as jde:
        print(f"Error decoding JSON: {jde}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    # Example usage; this will only run when the script is executed directly
    convert_query_results(
        input_file='data/output/query_results_keyword_extraction.json',  # Ensure this path is correct
        output_dir='data',
        output_file='queries_coding_standard.json'  # Modified filename
    )


# File: convert/convertquotationforkeyword.py
#------------------------------------------------------------------------------
# src/convert/convertquotationforkeyword.py

import json
import os
from typing import List, Dict, Any

def convert_query_results(input_file: str, output_dir: str, output_file: str):
    """
    Convert query results to a simplified format and save them in the specified directory.

    The simplified format includes:
    - quotation: The actual quotation text.
    - research_objectives: The research objectives from transcript_info.
    - theoretical_framework: The theoretical framework details from transcript_info (preserved as a nested dictionary).
    - transcript_chunk: (Optional) The full transcript chunk for context.

    If a result contains multiple quotations, each quotation will be a separate entry in the simplified data.
    """
    # Create output directory if it doesn't exist
    os.makedirs(os.path.join(output_dir, 'input'), exist_ok=True)

    try:
        # Read the query results file
        with open(input_file, 'r', encoding='utf-8') as f:
            results = json.load(f)

        # Convert to simplified format
        simplified_data: List[Dict[str, Any]] = []
        for result in results:
            transcript_info = result.get('transcript_info', {})
            research_objectives = transcript_info.get('research_objectives', '')
            theoretical_framework = transcript_info.get('theoretical_framework', {})
            transcript_chunk = transcript_info.get('transcript_chunk', '')

            quotations = result.get('quotations', [])
            for quotation_entry in quotations:
                quotation = quotation_entry.get('quotation', '')
                simplified_entry = {
                    "quotation": quotation,
                    "research_objectives": research_objectives,
                    "theoretical_framework": theoretical_framework,  # Preserved as a nested dictionary
                    "transcript_chunk": transcript_chunk  # Optional: Include for additional context
                }
                simplified_data.append(simplified_entry)

        # Save to output file with '_standard' suffix
        output_path = os.path.join(output_dir, 'input', output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(simplified_data, f, indent=4, ensure_ascii=False)

        print(f"Successfully converted and saved to {output_path}")

    except Exception as e:
        print(f"Error converting file: {e}")

if __name__ == "__main__":
    # Example usage; this will only run when the script is executed directly
    convert_query_results(
        input_file='data/output/query_results_quotation.json',  # Ensure this path is correct
        output_dir='data',
        output_file='queries_keyword_standard.json'  # Modified filename
    )



################################################################################
# Module: root
################################################################################


# File: copymd.py
#------------------------------------------------------------------------------
import os
import shutil
from pathlib import Path
import sys
from datetime import datetime

def setup_backup_directory() -> Path:
    """
    Create a new backup directory with timestamp in the same folder as the script.
    
    Returns:
        Path: Path to the created backup directory
    """
    script_dir = Path(__file__).parent
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = script_dir / f"md_backup_{timestamp}"
    backup_dir.mkdir(parents=True, exist_ok=True)
    print(f"Created directory: {backup_dir}")
    return backup_dir

def get_flattened_name(file_path: Path) -> str:
    """
    Generate a flattened filename based on the file's path.
    Example: docs/models/arch.md becomes models_arch.md
    
    Args:
        file_path (Path): Original file path
    
    Returns:
        str: New flattened filename
    """
    # Get the path relative to the docs directory
    rel_parts = file_path.parent.parts[file_path.parent.parts.index('docs')+1:]
    if rel_parts:
        # Join all directory names with underscore and append the filename
        return f"{'_'.join(rel_parts)}_{file_path.name}"
    return file_path.name

def copy_markdown_files(dest_dir: Path) -> tuple[int, list[str]]:
    """
    Copy all Markdown files from docs directory to destination directory with flattened names.
    
    Args:
        dest_dir (Path): Path to destination directory
    
    Returns:
        tuple[int, list[str]]: Number of files copied and list of any errors encountered
    """
    script_dir = Path(__file__).parent
    docs_path = script_dir / 'docs'
    
    if not docs_path.exists():
        return 0, [f"Source docs directory '{docs_path}' does not exist"]
    
    files_copied = 0
    errors = []
    
    try:
        # Track used names to handle potential duplicates
        used_names = set()
        
        for md_file in docs_path.rglob("*.md"):
            try:
                # Generate new flattened name
                new_name = get_flattened_name(md_file)
                
                # Handle potential name collisions
                base_name = new_name
                counter = 1
                while new_name in used_names:
                    name_parts = base_name.rsplit('.', 1)
                    new_name = f"{name_parts[0]}_{counter}.{name_parts[1]}"
                    counter += 1
                
                used_names.add(new_name)
                
                # Copy the file with new name
                dest_file = dest_dir / new_name
                shutil.copy2(md_file, dest_file)
                
                # Print original path -> new name
                rel_path = md_file.relative_to(docs_path)
                print(f"Copied: {rel_path} -> {new_name}")
                files_copied += 1
                
            except Exception as e:
                errors.append(f"Error copying {md_file.name}: {str(e)}")
    
    except Exception as e:
        errors.append(f"Error accessing directory: {str(e)}")
    
    return files_copied, errors

def main():
    # Create destination directory with timestamp
    dest_dir = setup_backup_directory()
    
    # Copy files and handle results
    files_copied, errors = copy_markdown_files(dest_dir)
    
    # Print summary
    print(f"\nFiles copied: {files_copied}")
    if errors:
        print("\nErrors encountered:")
        for error in errors:
            print(f"- {error}")
        
        if files_copied == 0:
            try:
                dest_dir.rmdir()
                print(f"\nRemoved empty directory: {dest_dir}")
            except:
                pass
        sys.exit(1)
    
    if files_copied == 0:
        print("\nNo markdown files found in docs directory.")
        try:
            dest_dir.rmdir()
            print(f"Removed empty directory: {dest_dir}")
        except:
            pass
        sys.exit(1)
    
    print(f"\nFiles successfully copied to: {dest_dir}")
    sys.exit(0)

if __name__ == "__main__":
    main()


################################################################################
# Module: core
################################################################################


# File: core/__init__.py
#------------------------------------------------------------------------------
# File: __init__.py
# ------------------------------------------------------------------------------
"""
Core functionality for the thematic analysis package.
Contains database and client implementations.
"""

from .contextual_vector_db import ContextualVectorDB
from .elasticsearch_bm25 import ElasticsearchBM25
from .openai_client import OpenAIClient

__all__ = ['ContextualVectorDB', 'ElasticsearchBM25', 'OpenAIClient']

# File: core/contextual_vector_db.py
#------------------------------------------------------------------------------

# File: contextual_vector_db.py
# ------------------------------------------------------------------------------
"""
Module: contextual_vector_db

This module contains the ContextualVectorDB class which manages the creation,
storage, and search functionalities for a vector database enhanced with contextual
information. It leverages external APIs (OpenAI) and libraries (FAISS, DSPy) while
implementing robust error handling and detailed logging.
"""

import os
import pickle
import numpy as np
import threading
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from dotenv import load_dotenv
import logging
import faiss
import dspy
import time

from src.core.openai_client import OpenAIClient
from src.utils.logger import setup_logging, log_execution_time, get_logger

setup_logging()
logger = logging.getLogger(__name__)


def log_exception(logger_obj: logging.Logger, message: str, e: Exception, context: Dict[str, Any] = None) -> None:
    """
    Helper function for logging exceptions with additional context.
    
    Args:
        logger_obj (logging.Logger): Logger instance.
        message (str): Custom error message.
        e (Exception): The caught exception.
        context (Dict[str, Any], optional): Additional contextual information.
    """
    context_str = ", ".join(f"{k}={v}" for k, v in (context or {}).items())
    logger_obj.error(f"{message}. Context: {context_str}. Exception: {e}", exc_info=True)


class SituateContextSignature(dspy.Signature):
    doc = dspy.InputField(desc="Full document content")
    chunk = dspy.InputField(desc="Specific chunk content")
    reasoning = dspy.OutputField(desc="Chain of thought reasoning")
    contextualized_content = dspy.OutputField(desc="Contextualized content for the chunk")


class SituateContext(dspy.Module):
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(SituateContextSignature)
        logger.debug("SituateContext module initialized.")

    def forward(self, doc: str, chunk: str):
        prompt = f"""
                <document>
                {doc}
                </document>
            
                CHUNK_CONTEXT_PROMPT = 
                Here is the chunk we want to situate within the whole document
                <chunk>
                {chunk}
                </chunk>

                Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
                Answer only with the succinct context and nothing else.
        """
        logger.debug("Generating contextualized content for a chunk.")
        return self.chain(doc=doc, chunk=chunk, prompt=prompt)


class ContextualVectorDB:
    def __init__(self, name: str, openai_api_key: str = None):
        if openai_api_key is None:
            load_dotenv()
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                logger.critical("OPENAI_API_KEY is not set in environment variables.")
                raise ValueError("OPENAI_API_KEY is required but not set.")

        self.name = name
        self.embeddings = []
        self.metadata = []
        self.db_path = f"./data/{name}/contextual_vector_db.pkl"
        self.faiss_index_path = f"./data/{name}/faiss_index.bin"

        try:
            self.client = OpenAIClient(api_key=openai_api_key)
            logger.debug(f"Initialized OpenAIClient for ContextualVectorDB '{self.name}'.")
        except Exception as e:
            log_exception(logger, "Failed to initialize OpenAIClient", e, {"name": self.name})
            raise

        self.index = None  # FAISS index

    @log_execution_time(logger)
    def situate_context(self, doc: str, chunk: str) -> Tuple[str, Any]:
        logger.debug(f"Entering situate_context with doc length={len(doc)} and chunk length={len(chunk)}.")
        try:
            if not hasattr(self, 'situate_context_module'):
                self.situate_context_module = SituateContext()
                logger.debug("Initialized SituateContext module.")

            start_time = time.time()
            response = self.situate_context_module(doc=doc, chunk=chunk)
            elapsed_time = time.time() - start_time
            logger.debug(f"Generated contextualized_content using DSPy in {elapsed_time:.2f} seconds.")

            contextualized_content = response.contextualized_content
            usage_metrics = {}  # Actual usage metrics can be populated here if desired.
            return contextualized_content, usage_metrics
        except Exception as e:
            log_exception(
                logger,
                "Error during DSPy situate_context",
                e,
                {"doc_length": len(doc), "chunk_length": len(chunk)}
            )
            return "", None

    @log_execution_time(logger)
    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 8):
        logger.debug("Entering load_data method.")
        if self.embeddings and self.metadata and os.path.exists(self.faiss_index_path):
            logger.info("Vector database is already loaded. Skipping data loading.")
            return

        if os.path.exists(self.db_path) and os.path.exists(self.faiss_index_path):
            logger.info("Loading vector database and FAISS index from disk.")
            self.load_db()
            self.load_faiss_index()
            return

        texts_to_embed, metadata = self._process_dataset(dataset, parallel_threads)

        if not texts_to_embed:
            logger.warning("No texts to embed after processing the dataset.")
            return

        self._embed_and_store(texts_to_embed, metadata, max_workers=parallel_threads)
        self.save_db()
        self._build_faiss_index()
        logger.info(f"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}.")

    @log_execution_time(logger)
    def _process_dataset(self, dataset: List[Dict[str, Any]], parallel_threads: int) -> Tuple[List[str], List[Dict[str, Any]]]:
        texts_to_embed = []
        metadata = []
        total_chunks = sum(len(doc.get('chunks', [])) for doc in dataset)
        logger.info(f"Total chunks to process: {total_chunks}.")

        logger.info(f"Processing {total_chunks} chunks with {parallel_threads} threads.")
        try:
            with ThreadPoolExecutor(max_workers=parallel_threads) as executor:
                futures = []
                for doc in dataset:
                    for chunk in doc.get('chunks', []):
                        futures.append(executor.submit(self._generate_contextualized_content, doc, chunk))

                for future in tqdm(as_completed(futures), total=total_chunks, desc="Processing chunks"):
                    result = future.result()
                    if result:
                        texts_to_embed.append(result['text_to_embed'])
                        metadata.append(result['metadata'])
            logger.debug("Completed processing all chunks.")
        except Exception as e:
            log_exception(logger, "Error during processing chunks", e)
            return [], []

        return texts_to_embed, metadata

    @log_execution_time(logger)
    def _generate_contextualized_content(self, doc: Dict[str, Any], chunk: Any) -> Dict[str, Any]:
        """
        Generates contextualized content for a single doc-chunk pair.
        """
        try:
            if not isinstance(doc, dict):
                logger.error(f"Document is not a dictionary: {doc}")
                return None

            if isinstance(chunk, dict):
                chunk_id = chunk.get('chunk_id')
                if not chunk_id:
                    # Assign a unique chunk_id if missing
                    chunk_index = chunk.get('index', 0)
                    chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_{chunk_index}"
                    chunk['chunk_id'] = chunk_id
                content = chunk.get('content', '')
                original_index = chunk.get('original_index', chunk.get('index', 0))
            elif isinstance(chunk, str):
                # Handle case where chunk is a string
                content = chunk
                chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_0"
                original_index = 0
                logger.warning("Chunk is a string. Expected a dict. Assigning default values.")
            else:
                logger.error(f"Unsupported chunk type: {type(chunk)}. Skipping chunk.")
                return None

            logger.debug(f"Processing chunk_id='{chunk_id}' in doc_id='{doc.get('doc_id', 'unknown_doc_id')}'.")
            contextualized_text, usage = self.situate_context(doc.get('content', ''), content)
            if not contextualized_text:
                logger.warning(f"Contextualized content is empty for chunk_id='{chunk_id}'.")
                return None

            return {
                'text_to_embed': f"{content}\n\n{contextualized_text}",
                'metadata': {
                    'doc_id': doc.get('doc_id', ''),
                    'original_uuid': doc.get('original_uuid', ''),
                    'chunk_id': chunk_id,
                    'original_index': original_index,
                    'original_content': content,
                    'contextualized_content': contextualized_text
                }
            }
        except Exception as e:
            log_exception(logger, "Error generating contextualized content", e, {
                "doc_id": doc.get('doc_id', 'unknown'), 
                "chunk": chunk
            })
            return None

    @log_execution_time(logger)
    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]], max_workers: int = 4):
        logger.debug("Entering _embed_and_store method.")
        batch_size = 128
        embeddings = []
        logger.info("Starting embedding generation.")

        def embed_batch(batch: List[str]) -> List[List[float]]:
            try:
                logger.debug(f"Generating embeddings for batch of size {len(batch)}.")
                response = self.client.create_embeddings(
                    model="text-embedding-3-small",
                    input=batch
                )
                return [item['embedding'] for item in response['data']]
            except Exception as e:
                log_exception(logger, "Error during OpenAI embeddings for batch", e, {"batch_size": len(batch)})
                return []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i: i + batch_size]
                futures.append(executor.submit(embed_batch, batch))

            for future in as_completed(futures):
                try:
                    embeddings_batch = future.result()
                    if embeddings_batch:
                        embeddings.extend(embeddings_batch)
                        logger.debug(f"Processed a batch with {len(embeddings_batch)} embeddings.")
                except Exception as e:
                    log_exception(logger, "Error retrieving embeddings from future", e)

        if not embeddings:
            logger.warning("No embeddings were generated.")
            return

        self.embeddings = embeddings
        self.metadata = data
        self.save_db()
        self._build_faiss_index()
        logger.info(f"Embedding generation completed. Total embeddings: {len(self.embeddings)}.")

    @log_execution_time(logger)
    def _build_faiss_index(self):
        logger.debug("Entering _build_faiss_index method.")
        start_time = time.time()
        try:
            self.create_faiss_index()
            self.save_faiss_index()
        except Exception as e:
            log_exception(logger, "Error building FAISS index", e)
            raise
        elapsed_time = time.time() - start_time
        logger.info(f"FAISS index built and saved in {elapsed_time:.2f} seconds.")

    @log_execution_time(logger)
    def create_faiss_index(self):
        logger.debug("Entering create_faiss_index method.")
        try:
            if not self.embeddings:
                logger.error("No embeddings available to create FAISS index.")
                raise ValueError("Embeddings list is empty.")

            embedding_dim = len(self.embeddings[0])
            logger.info(f"Embedding dimension: {embedding_dim}.")
            embeddings_np = np.array(self.embeddings).astype('float32')
            faiss.normalize_L2(embeddings_np)
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.index.add(embeddings_np)
            logger.info(f"FAISS index created with {self.index.ntotal} vectors.")
        except Exception as e:
            log_exception(logger, "Error creating FAISS index", e)
            raise

    @log_execution_time(logger)
    def save_faiss_index(self):
        logger.debug("Entering save_faiss_index method.")
        os.makedirs(os.path.dirname(self.faiss_index_path), exist_ok=True)
        try:
            faiss.write_index(self.index, self.faiss_index_path)
            logger.info(f"FAISS index saved to '{self.faiss_index_path}'.")
        except Exception as e:
            log_exception(logger, "Error saving FAISS index", e, {"faiss_index_path": self.faiss_index_path})

    @log_execution_time(logger)
    def load_faiss_index(self):
        logger.debug("Entering load_faiss_index method.")
        if not os.path.exists(self.faiss_index_path):
            logger.error(f"FAISS index file not found at '{self.faiss_index_path}'.")
            raise ValueError("FAISS index file not found.")
        try:
            self.index = faiss.read_index(self.faiss_index_path)
            logger.info(f"FAISS index loaded from '{self.faiss_index_path}' with {self.index.ntotal} vectors.")
        except Exception as e:
            log_exception(logger, "Error loading FAISS index", e, {"faiss_index_path": self.faiss_index_path})
            raise

    @log_execution_time(logger)
    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        logger.debug(f"Entering search method with query='{query}' and k={k}.")
        if not self.embeddings or not self.metadata:
            logger.error("Embeddings or metadata are not loaded. Cannot perform search.")
            return []
        if not hasattr(self, 'index') or self.index is None:
            logger.error("FAISS index is not loaded.")
            return []

        try:
            start_time = time.time()
            logger.debug("Generating embedding for the query.")
            response = self.client.create_embeddings(
                model="text-embedding-3-small",
                input=[query]
            )
            query_embedding = response['data'][0]['embedding']
            elapsed_time = time.time() - start_time
            logger.debug(f"Generated embedding for query in {elapsed_time:.2f} seconds.")
        except Exception as e:
            log_exception(logger, "Error generating embedding for query", e, {"query": query})
            return []

        query_embedding_np = np.array([query_embedding]).astype('float32')
        faiss.normalize_L2(query_embedding_np)

        logger.debug("Performing FAISS search.")
        try:
            start_time = time.time()
            distances, indices = self.index.search(query_embedding_np, k)
            elapsed_time = time.time() - start_time
            logger.debug(f"FAISS search completed in {elapsed_time:.2f} seconds.")
            indices = indices.flatten()
            distances = distances.flatten()
        except Exception as e:
            log_exception(logger, "Error during FAISS search", e, {"query": query, "k": k})
            return []

        top_results = []
        for idx, score in zip(indices, distances):
            if idx < len(self.metadata):
                meta = self.metadata[idx]
                result = {
                    "doc_id": meta['doc_id'],
                    "chunk_id": meta['chunk_id'],
                    "original_index": meta.get('original_index', 0),
                    "content": meta['original_content'],
                    "contextualized_content": meta.get('contextualized_content'),
                    "score": float(score),
                    "metadata": meta
                }
                top_results.append(result)
                logger.debug(f"Retrieved chunk_id='{meta['chunk_id']}' with score={score:.4f}.")
            else:
                logger.warning(f"Index {idx} out of bounds for metadata.")

        logger.info(f"FAISS search returned {len(top_results)} results for query: '{query}'.")
        logger.debug(f"Chunks retrieved: {[res['chunk_id'] for res in top_results]}.")
        return top_results

    @log_execution_time(logger)
    def save_db(self):
        logger.debug("Entering save_db method.")
        data = {
            "metadata": self.metadata,
        }
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        try:
            with open(self.db_path, "wb") as file:
                pickle.dump(data, file)
            logger.info(f"Vector database metadata saved to '{self.db_path}'.")
        except Exception as e:
            log_exception(logger, "Error saving vector database metadata", e, {"db_path": self.db_path})

    @log_execution_time(logger)
    def load_db(self):
        logger.debug("Entering load_db method.")
        if not os.path.exists(self.db_path):
            logger.error(f"Vector database file not found at '{self.db_path}'. Use load_data to create a new database.")
            raise ValueError("Vector database file not found.")
        try:
            with open(self.db_path, "rb") as file:
                data = pickle.load(file)
            self.metadata = data.get("metadata", [])
            logger.info(f"Vector database metadata loaded from '{self.db_path}' with {len(self.metadata)} entries.")
            logger.debug(f"Chunks loaded: {[meta['chunk_id'] for meta in self.metadata]}.")
        except Exception as e:
            log_exception(logger, "Error loading vector database metadata", e, {"db_path": self.db_path})
            raise


# File: core/elasticsearch_bm25.py
#------------------------------------------------------------------------------

# File: elasticsearch_bm25.py
# ------------------------------------------------------------------------------
# src/core/elasticsearch_bm25.py

import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk, scan
from elasticsearch.exceptions import NotFoundError, RequestError
import json
import os

from src.utils.logger import setup_logging, log_execution_time

setup_logging()
logger = logging.getLogger(__name__)

class ElasticsearchBM25:
    
    @log_execution_time(logger)
    def __init__(
        self, 
        index_name: str,
        es_host: str = "http://localhost:9200",
        logger: Optional[logging.Logger] = None
    ):
        self.logger = logger or logging.getLogger(__name__)
        self.index_name = index_name
        self.es_client = Elasticsearch(es_host)
        
        if not self.es_client.ping():
            raise ConnectionError(f"Failed to connect to Elasticsearch at {es_host}")
            
        self._create_index()
        
    @log_execution_time(logger)
    def _create_index(self) -> None:
        index_settings = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "english"
                        },
                        "custom_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "porter_stem"]
                        }
                    }
                },
                "similarity": {
                    "custom_bm25": {
                        "type": "BM25",
                        "k1": 1.2,
                        "b": 0.75,
                    }
                },
                "index": {
                    "refresh_interval": "1s",
                    "number_of_shards": 1,
                    "number_of_replicas": 0
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "contextualized_content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "doc_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "original_index": {"type": "integer"},
                    "metadata": {"type": "object", "enabled": True}
                }
            }
        }
        
        index_data_dir = f"./data/{self.index_name}"
        os.makedirs(index_data_dir, exist_ok=True)
        self.logger.debug(f"Ensured existence of index data directory: {index_data_dir}")
        
        try:
            if self.es_client.indices.exists(index=self.index_name):
                self.logger.info(f"Index '{self.index_name}' already exists. Skipping creation.")
            else:
                self.es_client.indices.create(
                    index=self.index_name,
                    body=index_settings
                )
                self.logger.info(f"Successfully created index: {self.index_name}")
        except Exception as e:
            self.logger.error(f"Failed to create/update index '{self.index_name}': {str(e)}")
            raise

    @log_execution_time(logger)
    def index_documents(
        self,
        documents: List[Dict[str, Any]],
        batch_size: int = 500
    ) -> Tuple[int, List[Dict[str, Any]]]:
        if not documents:
            self.logger.warning("No documents provided for indexing")
            return 0, []

        failed_docs = []
        success_count = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            actions = [{
                "_index": self.index_name,
                "_source": {
                    "content": doc.get("original_content", ""),
                    "contextualized_content": doc.get("contextualized_content", ""),
                    "doc_id": doc.get("doc_id", ""),
                    "chunk_id": doc.get("chunk_id", ""),
                    "original_index": doc.get("original_index", 0),
                    "metadata": doc.get("metadata", {})
                }
            } for doc in batch]
            
            try:
                success, failed = bulk(
                    self.es_client,
                    actions,
                    raise_on_error=False,
                    raise_on_exception=False
                )
                success_count += success
                if failed:
                    for fail in failed:
                        failed_doc = batch[fail.get('index', 0)]
                        failed_docs.append(failed_doc)
                    self.logger.warning(f"Failed to index {len(failed)} documents in batch")
                    
            except Exception as e:
                self.logger.error(f"Batch indexing error: {str(e)}")
                failed_docs.extend(batch)
                
        self.es_client.indices.refresh(index=self.index_name)
        self.logger.info(f"Indexed {success_count}/{len(documents)} documents successfully")
        return success_count, failed_docs

    @log_execution_time(logger)
    def search(
        self,
        query: str,
        k: int = 20,
        min_score: float = 0.1,
        fields: List[str] = None,
        operator: str = "or",
        minimum_should_match: str = "30%"
    ) -> List[Dict[str, Any]]:
        if not fields:
            fields = ["content^1", "contextualized_content^1.5"]
            
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "operator": operator,
                    "minimum_should_match": minimum_should_match,
                    "type": "best_fields",
                    "tie_breaker": 0.3
                }
            },
            "min_score": min_score,
            "size": k,
            "_source": True
        }

        try:
            search_body_json = json.dumps(search_body, indent=2)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")
        except Exception as e:
            self.logger.error(f"Error converting search body to JSON: {e}")
            search_body_json = str(search_body)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")

        try:
            response = self.es_client.search(
                index=self.index_name,
                body=search_body
            )
            
            hits = [{
                "doc_id": hit["_source"]["doc_id"],
                "chunk_id": hit["_source"]["chunk_id"],
                "content": hit["_source"]["content"],
                "contextualized_content": hit["_source"].get("contextualized_content"),
                "score": hit["_score"],
                "metadata": hit["_source"].get("metadata", {})
            } for hit in response["hits"]["hits"]]
            
            self.logger.debug(
                f"Search for '{query}' returned {len(hits)} results "
                f"(max_score: {response['hits'].get('max_score', 0)})"
            )
            return hits
            
        except Exception as e:
            self.logger.error(f"Search error: {str(e)}")
            raise

    @log_execution_time(logger)
    def search_content(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["content^1"],
            operator="or",
            minimum_should_match="30%"
        )

    @log_execution_time(logger)
    def search_contextualized(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["contextualized_content^1.5"],
            operator="or",
            minimum_should_match="30%"
        )



# File: core/openai_client.py
#------------------------------------------------------------------------------

# File: openai_client.py
# ------------------------------------------------------------------------------
import logging
import os
from typing import List, Dict, Any
from openai import OpenAI

logger = logging.getLogger(__name__)

class OpenAIClient:
    """
    Wrapper for OpenAI API interactions using the updated SDK.
    """
    def __init__(self, api_key: str):
        """
        Initializes the OpenAI client with the provided API key.
        
        Args:
            api_key (str): OpenAI API key.
        """
        if not api_key:
            logger.error("OpenAI API key is not provided.")
            raise ValueError("OpenAI API key must be provided.")
        self.client = OpenAI(api_key=api_key)
        logger.debug("OpenAI client initialized successfully.")

    def create_chat_completion(self, model: str, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> Dict[str, Any]:
        """
        Creates a chat completion using OpenAI's API.
        
        Args:
            model (str): Model name to use.
            messages (List[Dict[str, str]]): List of message dictionaries.
            max_tokens (int): Maximum number of tokens in the response.
            temperature (float): Sampling temperature.
        
        Returns:
            Dict[str, Any]: API response as a dictionary.
        """
        if not model or not messages:
            logger.error("Model and messages must be provided for chat completion.")
            raise ValueError("Model and messages must be provided for chat completion.")
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            logger.debug(f"Chat completion created successfully for model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating chat completion: {e}", exc_info=True)
            raise

    def create_embeddings(self, model: str, input: List[str]) -> Dict[str, Any]:
        """
        Creates embeddings for the given input texts using OpenAI's API.
        
        Args:
            model (str): Embedding model to use.
            input (List[str]): List of input texts.
        
        Returns:
            Dict[str, Any]: API response containing embeddings.
        """
        if not model or not input:
            logger.error("Model and input must be provided for creating embeddings.")
            raise ValueError("Model and input must be provided for creating embeddings.")
        
        try:
            response = self.client.embeddings.create(
                model=model,
                input=input
            )
            logger.debug(f"Embeddings created successfully using model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}", exc_info=True)
            raise



# File: core/retrieval/__init__.py
#------------------------------------------------------------------------------

################################################################################
# Module: retrieval
################################################################################

# File: retrieval/__init__.py
# ------------------------------------------------------------------------------
"""
Retrieval functionality for searching and ranking content.
"""

from .query_generator import QueryGeneratorSignature
from .retrieval import hybrid_retrieval, multi_stage_retrieval
from .reranking import retrieve_with_reranking

__all__ = [
    'QueryGeneratorSignature',
    'hybrid_retrieval',
    'multi_stage_retrieval',
    'retrieve_with_reranking',
    'SentenceTransformerReRanker'
]


# File: core/retrieval/query_generator.py
#------------------------------------------------------------------------------

# File: retrieval/query_generator.py
# ------------------------------------------------------------------------------
import dspy
import logging
from typing import Dict

from src.utils.logger import setup_logging

logger = logging.getLogger(__name__)

class QueryGeneratorSignature(dspy.Signature):
    question: str = dspy.InputField(desc="The original user question.")
    context: str = dspy.InputField(desc="The accumulated context from previous retrievals.")
    new_query: str = dspy.OutputField(desc="The generated query for the next retrieval hop.")

    def forward(self, question: str, context: str) -> Dict[str, str]:
        try:
            if not question or not context:
                raise ValueError("Both 'question' and 'context' must be provided and non-empty.")
            
            prompt = (
                f"Given the question: '{question}'\n"
                f"and the context retrieved so far:\n{context}\n"
                "Generate a search query that will help find additional information needed to answer the question."
            )
            new_query = self.language_model.generate(
                prompt=prompt,
                max_tokens=50,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=["\n"]
            ).strip()
            logger.info(f"Generated new query: '{new_query}'")
            return {"new_query": new_query}
        except ValueError as ve:
            logger.error(f"ValueError in QueryGeneratorSignature.forward: {ve}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question
        except Exception as e:
            logger.error(f"Error in QueryGeneratorSignature.forward: {e}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question


# File: core/retrieval/reranking.py
#------------------------------------------------------------------------------


# File: retrieval/reranking.py
# ------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer, util
import torch
import time
import cohere
import os
from enum import Enum

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging, log_execution_time
from src.core.retrieval.retrieval import hybrid_retrieval

setup_logging()
logger = logging.getLogger(__name__)

class RerankerType(str, Enum):
    SENTENCE_TRANSFORMER = "sentence_transformer"
    COHERE = "cohere"
    COMBINED = "combined"

class RerankerConfig:
    def __init__(self,
                 reranker_type: RerankerType = RerankerType.SENTENCE_TRANSFORMER,
                 st_model_name: str = 'all-MiniLM-L6-v2',
                 device: Optional[str] = None,
                 cohere_api_key: Optional[str] = None,
                 st_weight: float = 0.5):
        self.reranker_type = reranker_type
        self.st_model_name = st_model_name
        self.device = device
        self.cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")
        self.st_weight = st_weight

class SentenceTransformerReRanker:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        self.model = SentenceTransformer(model_name)
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')

        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'")

    @log_execution_time(logger)
    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        if not query or not documents:
            logger.warning("Query and documents must be provided for re-ranking.")
            return []

        try:
            start_time = time.time()
            logger.info(f"Starting ST reranking for query: '{query[:100]}...' with {len(documents)} documents")
            
            logger.debug("Encoding query and documents with SentenceTransformer")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            elapsed_time = time.time() - start_time
            if re_ranked_docs:
                logger.info(
                    f"ST reranking completed in {elapsed_time:.2f}s. "
                    f"Top score: {re_ranked_docs[0]['score']:.4f}, "
                    f"Bottom score: {re_ranked_docs[-1]['score']:.4f}"
                )
            else:
                logger.info(f"ST reranking completed in {elapsed_time:.2f}s with no documents returned.")
            return re_ranked_docs
            
        except Exception as e:
            logger.error(f"Error during ST reranking: {e}", exc_info=True)
            return []

class CohereReRanker:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("COHERE_API_KEY")
        if not self.api_key:
            raise ValueError("Cohere API key must be provided or set in COHERE_API_KEY environment variable")
        self.client = cohere.Client(self.api_key)
        logger.info("CohereReRanker initialized successfully")

    @log_execution_time(logger)
    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        if not query or not documents:
            logger.warning("Query and documents must be provided for Cohere re-ranking.")
            return []

        try:
            start_time = time.time()
            logger.info(f"Starting Cohere reranking for query: '{query[:100]}...' with {len(documents)} documents")
            
            logger.debug("Calling Cohere rerank API")
            response = self.client.rerank(
                model="rerank-english-v3.0",
                query=query,
                documents=documents,
                top_n=min(top_k, len(documents))
            )
            
            re_ranked_docs = []
            for result in response.results:
                re_ranked_docs.append({
                    "document": documents[result.index],
                    "score": result.relevance_score
                })
            
            elapsed_time = time.time() - start_time
            if re_ranked_docs:
                logger.info(
                    f"Cohere reranking completed in {elapsed_time:.2f}s. "
                    f"Top score: {re_ranked_docs[0]['score']:.4f}, "
                    f"Bottom score: {re_ranked_docs[-1]['score']:.4f}"
                )
            else:
                logger.info(f"Cohere reranking completed in {elapsed_time:.2f}s with no documents returned.")
            return re_ranked_docs
            
        except Exception as e:
            logger.error(f"Error during Cohere reranking: {e}", exc_info=True)
            return []

class CombinedReRanker:
    def __init__(self, 
                 st_model_name: str = 'all-MiniLM-L6-v2',
                 device: Optional[str] = None,
                 cohere_api_key: Optional[str] = None,
                 st_weight: float = 0.5):
        self.st_reranker = SentenceTransformerReRanker(st_model_name, device)
        self.cohere_reranker = CohereReRanker(cohere_api_key)
        self.st_weight = st_weight
        logger.info(f"CombinedReRanker initialized with ST weight: {st_weight}")

    @log_execution_time(logger)
    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        try:
            start_time = time.time()
            logger.info(f"Starting combined reranking for query: '{query[:100]}...' with {len(documents)} documents")
            
            logger.debug("Getting ST rankings")
            st_results = self.st_reranker.rerank(query, documents, top_k=len(documents))
            logger.debug("Getting Cohere rankings")
            cohere_results = self.cohere_reranker.rerank(query, documents, top_k=len(documents))
            
            logger.debug("Creating score maps and combining results")
            st_scores = {doc['document']: doc['score'] for doc in st_results}
            cohere_scores = {doc['document']: doc['score'] for doc in cohere_results}
            
            combined_scores = {}
            for doc in documents:
                st_score = st_scores.get(doc, 0.0)
                cohere_score = cohere_scores.get(doc, 0.0)
                combined_score = (st_score * self.st_weight) + (cohere_score * (1 - self.st_weight))
                combined_scores[doc] = combined_score
                logger.debug(
                    f"Document scores - ST: {st_score:.4f}, "
                    f"Cohere: {cohere_score:.4f}, "
                    f"Combined: {combined_score:.4f}"
                )
            
            sorted_docs = sorted(
                combined_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )[:top_k]
            final_results = [{"document": doc, "score": score} for doc, score in sorted_docs]
            
            elapsed_time = time.time() - start_time
            if final_results:
                logger.info(
                    f"Combined reranking completed in {elapsed_time:.2f}s. "
                    f"Top score: {final_results[0]['score']:.4f}, "
                    f"Bottom score: {final_results[-1]['score']:.4f}"
                )
            else:
                logger.info(f"Combined reranking completed in {elapsed_time:.2f}s with no documents returned.")
            return final_results
            
        except Exception as e:
            logger.error(f"Error in combined reranking: {e}", exc_info=True)
            logger.debug("Falling back to ST reranker")
            return self.st_reranker.rerank(query, documents, top_k=top_k)

class RerankerFactory:
    @staticmethod
    def create_reranker(config: RerankerConfig):
        logger.debug(f"Creating reranker of type: {config.reranker_type}")
        if config.reranker_type == RerankerType.SENTENCE_TRANSFORMER:
            return SentenceTransformerReRanker(model_name=config.st_model_name, device=config.device)
        elif config.reranker_type == RerankerType.COHERE:
            return CohereReRanker(api_key=config.cohere_api_key)
        elif config.reranker_type == RerankerType.COMBINED:
            return CombinedReRanker(
                st_model_name=config.st_model_name,
                device=config.device,
                cohere_api_key=config.cohere_api_key,
                st_weight=config.st_weight
            )
        else:
            error_msg = f"Unknown reranker type: {config.reranker_type}"
            logger.error(error_msg)
            raise ValueError(error_msg)

@log_execution_time(logger)
def retrieve_with_reranking(query: str,
                            db: ContextualVectorDB,
                            es_bm25: ElasticsearchBM25,
                            k: int,
                            reranker_config: Optional[RerankerConfig] = None) -> List[Dict[str, Any]]:
    start_time = time.time()
    logger.info(f"Starting retrieval and reranking for query: '{query[:100]}...'")

    if reranker_config is None:
        logger.debug("No reranker config provided, defaulting to SentenceTransformer")
        reranker_config = RerankerConfig(reranker_type=RerankerType.SENTENCE_TRANSFORMER)
    
    logger.debug(f"Using reranker type: {reranker_config.reranker_type}")

    try:
        logger.debug(f"Performing initial hybrid retrieval with k={k*10}")
        initial_results = hybrid_retrieval(query, db, es_bm25, k=k*10)
        logger.info(f"Initial retrieval returned {len(initial_results)} results in {time.time() - start_time:.2f}s")

        if not initial_results:
            logger.warning(f"No initial results retrieved for query '{query[:100]}...'. Skipping reranking.")
            return []

        documents = [doc['chunk']['original_content'] for doc in initial_results]
        
        logger.debug(f"Creating {reranker_config.reranker_type} reranker")
        reranker = RerankerFactory.create_reranker(reranker_config)
        
        logger.debug(f"Performing reranking with k={k}")
        re_ranked = reranker.rerank(query, documents, top_k=k)

        final_results = []
        for r in re_ranked:
            doc_index = documents.index(r['document'])
            final_results.append({
                "chunk": initial_results[doc_index]['chunk'],
                "score": r['score']
            })

        elapsed_time = time.time() - start_time
        logger.info(f"Retrieval and reranking completed in {elapsed_time:.2f}s. Returned {len(final_results)} results")
        if final_results:
            logger.debug(f"Top score: {final_results[0]['score']:.4f}, Bottom score: {final_results[-1]['score']:.4f}")
        return final_results

    except Exception as e:
        logger.error(f"Error during retrieval/reranking for query '{query[:100]}...': {e}", exc_info=True)
        return []



# File: core/retrieval/retrieval.py
#------------------------------------------------------------------------------

# File: retrieval/retrieval.py
# ------------------------------------------------------------------------------
import logging
import time
from typing import List, Dict, Any
import dspy

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging, log_execution_time
from src.core.retrieval.query_generator import QueryGeneratorSignature
from src.utils.utils import compute_similarity

setup_logging()
logger = logging.getLogger(__name__)

@log_execution_time(logger)
def hybrid_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    semantic_weight: float = 0.2,
    bm25_content_weight: float = 0.2,
    bm25_contextual_weight: float = 0.6,
    min_chunks: int = 1
) -> List[Dict[str, Any]]:
    """
    Performs hybrid retrieval by combining FAISS semantic search and dual BM25
    contextual search using Reciprocal Rank Fusion.
    """
    logger.debug(
        f"Entering hybrid_retrieval with query='{query}', k={k}, "
        f"semantic_weight={semantic_weight}, bm25_content_weight={bm25_content_weight}, "
        f"bm25_contextual_weight={bm25_contextual_weight}, min_chunks={min_chunks}."
    )
    start_time = time.time()
    num_chunks_to_recall = k * 10  # Retrieve more to improve chances

    # Initialize reciprocal rank fusion score dictionary
    chunk_id_to_score = {}

    while True:
        # Semantic search
        logger.debug(f"Performing semantic search using FAISS for query: '{query}'")
        semantic_results = db.search(query, k=num_chunks_to_recall)
        ranked_semantic = [result['chunk_id'] for result in semantic_results]
        semantic_scores = {result['chunk_id']: result['score'] for result in semantic_results}
        logger.debug(f"Semantic search retrieved {len(ranked_semantic)} chunk IDs.")

        # BM25 search on 'content'
        logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        bm25_content_results = es_bm25.search_content(query, k=num_chunks_to_recall)
        ranked_bm25_content = [result['chunk_id'] for result in bm25_content_results]
        bm25_content_scores = {result['chunk_id']: result['score'] for result in bm25_content_results}
        logger.debug(f"BM25 'content' search retrieved {len(ranked_bm25_content)} chunk IDs.")

        # BM25 search on 'contextualized_content'
        logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        bm25_contextual_results = es_bm25.search_contextualized(query, k=num_chunks_to_recall)
        ranked_bm25_contextual = [result['chunk_id'] for result in bm25_contextual_results]
        bm25_contextual_scores = {result['chunk_id']: result['score'] for result in bm25_contextual_results}
        logger.debug(f"BM25 'contextualized_content' search retrieved {len(ranked_bm25_contextual)} chunk IDs.")

        # Combine all unique chunk IDs
        chunk_ids = list(set(ranked_semantic + ranked_bm25_content + ranked_bm25_contextual))
        logger.debug(f"Total unique chunk IDs after combining: {len(chunk_ids)}")

        # Calculate Reciprocal Rank Fusion scores
        for chunk_id in chunk_ids:
            score = 0
            if chunk_id in ranked_semantic:
                index = ranked_semantic.index(chunk_id)
                score += semantic_weight * (1 / (index + 1))
                logger.debug(
                    f"Added semantic RRF score for chunk_id {chunk_id}: "
                    f"{semantic_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_content:
                index = ranked_bm25_content.index(chunk_id)
                score += bm25_content_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_content_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_contextual:
                index = ranked_bm25_contextual.index(chunk_id)
                score += bm25_contextual_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'contextualized_content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_contextual_weight * (1 / (index + 1))}"
                )
            chunk_id_to_score[chunk_id] = score

        # Sort chunk IDs by their RRF scores in descending order
        sorted_chunk_ids = sorted(
            chunk_id_to_score.keys(),
            key=lambda x: chunk_id_to_score[x],
            reverse=True
        )
        logger.debug(f"Sorted chunk IDs based on RRF scores.")

        # Select top k chunks, ensuring at least min_chunks are returned
        final_results = []
        filtered_count = 0
        for chunk_id in sorted_chunk_ids[:k]:
            chunk_metadata = next(
                (chunk for chunk in db.metadata if chunk['chunk_id'] == chunk_id),
                None
            )
            if not chunk_metadata:
                filtered_count += 1
                logger.warning(f"Chunk metadata not found for chunk_id {chunk_id}")
                continue
            final_results.append({
                'chunk': chunk_metadata,
                'score': chunk_id_to_score[chunk_id]
            })

        logger.info(f"Filtered {filtered_count} chunks due to missing metadata.")
        logger.info(
            f"Total chunks retrieved after filtering: {len(final_results)} "
            f"(required min_chunks={min_chunks})"
        )

        if len(final_results) >= min_chunks or k >= num_chunks_to_recall:
            break
        else:
            k += 5  # Increment k to retrieve more chunks
            logger.info(
                f"Number of retrieved chunks ({len(final_results)}) is less than min_chunks ({min_chunks}). "
                f"Increasing k to {k} and retrying retrieval."
            )

    logger.debug(f"Hybrid retrieval returning {len(final_results)} chunks.")
    logger.info(
        f"Chunks used for hybrid retrieval for query '{query}': "
        f"[{', '.join([res['chunk']['chunk_id'] for res in final_results])}]"
    )
    end_time = time.time()
    logger.debug(f"Exiting hybrid_retrieval method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results

@log_execution_time(logger)
def multi_stage_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    max_hops: int = 3,
    max_results: int = 5,
    similarity_threshold: float = 0.9
) -> List[Dict[str, Any]]:
    accumulated_context = ""
    all_retrieved_chunks = {}
    current_query = query
    previous_query = ""
    query_generator = dspy.ChainOfThought(QueryGeneratorSignature)

    for hop in range(max_hops):
        logger.info(f"Starting hop {hop+1} with query: '{current_query}'")

        retrieved_chunks = hybrid_retrieval(current_query, db, es_bm25, k)

        # Update accumulated retrieved chunks
        for chunk in retrieved_chunks:
            chunk_id = chunk['chunk']['chunk_id']
            if chunk_id not in all_retrieved_chunks:
                all_retrieved_chunks[chunk_id] = chunk

        # Check if we have reached the desired number of results
        if len(all_retrieved_chunks) >= max_results:
            logger.info(f"Retrieved sufficient chunks ({len(all_retrieved_chunks)}). Terminating.")
            break

        # Accumulate new context from retrieved chunks
        new_context = "\n\n".join([
            chunk['chunk'].get('contextualized_content', '') or chunk['chunk'].get('original_content', '')
            for chunk in retrieved_chunks
        ])
        accumulated_context += "\n\n" + new_context

        # Generate a new query based on the accumulated context
        response = query_generator(question=query, context=accumulated_context)
        new_query = response.get('new_query', '').strip()
        if not new_query:
            logger.info("No new query generated. Terminating multi-stage retrieval.")
            break

        # Compute similarity between the new query and the previous query
        similarity = compute_similarity(current_query, new_query)
        logger.debug(f"Similarity between queries: {similarity:.4f}")
        if similarity >= similarity_threshold:
            logger.info("New query is too similar to the current query. Terminating multi-stage retrieval.")
            break

        # Update queries for the next iteration
        previous_query = current_query
        current_query = new_query

    # Sort and return the top results
    final_results = sorted(
        all_retrieved_chunks.values(),
        key=lambda x: x['score'],
        reverse=True
    )[:max_results]

    logger.info(f"Multi-stage retrieval completed with {len(final_results)} chunks.")
    return final_results




################################################################################
# Module: data
################################################################################


# File: data/__init__.py
#------------------------------------------------------------------------------
"""
Data handling modules for loading and processing data.
"""

from .data_loader import load_codebase_chunks, load_queries
from .copycode import CodeFileHandler

__all__ = ['load_codebase_chunks', 'load_queries', 'CodeFileHandler']

# File: data/copycode.py
#------------------------------------------------------------------------------
import logging
import os
import asyncio
import aiofiles
from typing import List, Set, Tuple
import time
from pathlib import Path

# Initialize logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CodeFileHandler:
    def __init__(self, project_root: str, extensions: Set[str] = None):
        """
        Initialize the CodeFileHandler.
        
        Args:
            project_root (str): Root directory of the project
            extensions (Set[str]): Set of file extensions to include
        """
        self.project_root = project_root
        self.src_dir = os.path.join(project_root, '')
        self.extensions = extensions or {'.py', '.yaml', '.yml'}
        self.ignored_dirs = {'.venv', 'venv', 'env', '.env', 'myenv', '__pycache__', '.git', 'node_modules'}
        self.output_dir = os.path.join(project_root, 'output')

    async def get_code_files(self) -> List[Tuple[str, str]]:
        """
        Get all code files from the src directory.
        
        Returns:
            List[Tuple[str, str]]: List of (file_path, relative_path) tuples
        """
        logger.info(f"Scanning for code files in: {self.src_dir}")
        start_time = time.time()
        code_files = []
        
        try:
            for root, dirs, files in os.walk(self.src_dir):
                # Remove ignored directories
                dirs[:] = [d for d in dirs if d not in self.ignored_dirs]
                
                # Process files
                for file in files:
                    if file.endswith(tuple(self.extensions)):
                        file_path = os.path.join(root, file)
                        relative_path = os.path.relpath(file_path, self.src_dir)
                        code_files.append((file_path, relative_path))
                        logger.debug(f"Found code file: {relative_path}")
        
        except Exception as e:
            logger.error(f"Error scanning code files: {e}", exc_info=True)
            return []

        end_time = time.time()
        logger.info(f"Found {len(code_files)} code files in {end_time - start_time:.2f} seconds")
        return sorted(code_files, key=lambda x: x[1])  # Sort by relative path

    async def copy_code_to_file(self, code_files: List[Tuple[str, str]], output_file: str):
        """
        Copy code files to a single output file with proper organization.
        
        Args:
            code_files (List[Tuple[str, str]]): List of (file_path, relative_path) tuples
            output_file (str): Output file path
        """
        logger.info(f"Writing consolidated code to: {output_file}")
        start_time = time.time()
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        try:
            async with aiofiles.open(output_file, 'w', encoding='utf-8') as outfile:
                # Write header
                await outfile.write("# Consolidated Source Code\n")
                await outfile.write("# " + "=" * 78 + "\n\n")

                current_module = None
                for file_path, relative_path in code_files:
                    # Get module name (first directory in relative path)
                    module = relative_path.split(os.sep)[0] if os.sep in relative_path else 'root'
                    
                    # Write module header if changed
                    if module != current_module:
                        await outfile.write(f"\n\n{'#' * 80}\n")
                        await outfile.write(f"# Module: {module}\n")
                        await outfile.write(f"{'#' * 80}\n\n")
                        current_module = module

                    # Write file header
                    await outfile.write(f"\n# File: {relative_path}\n")
                    await outfile.write("#" + "-" * 78 + "\n")

                    try:
                        async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                            content = await infile.read()
                            await outfile.write(content)
                            await outfile.write("\n")
                            logger.debug(f"Copied content from {relative_path}")
                    except Exception as e:
                        error_msg = f"# Error reading file {relative_path}: {str(e)}\n"
                        await outfile.write(error_msg)
                        logger.error(f"Error reading file {relative_path}: {e}", exc_info=True)

        except Exception as e:
            logger.error(f"Error writing to output file: {e}", exc_info=True)
            return

        end_time = time.time()
        logger.info(f"Code consolidation completed in {end_time - start_time:.2f} seconds")

async def main_async():
    """
    Main async function to handle code consolidation.
    """
    try:
        # Get project root (parent of src directory)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        logger.info(f"Project root: {project_root}")

        # Initialize handler
        handler = CodeFileHandler(
            project_root=project_root,
            extensions={'.py', '.yaml', '.yml'}
        )

        # Create output directory if it doesn't exist
        output_dir = os.path.join(project_root, 'output')
        os.makedirs(output_dir, exist_ok=True)

        # Get code files
        code_files = await handler.get_code_files()
        
        if not code_files:
            logger.error("No code files found to process")
            return

        # Generate output file path
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f'consolidated_code_{timestamp}.txt')

        # Copy code files
        await handler.copy_code_to_file(code_files, output_file)
        logger.info(f"Code consolidated successfully to: {output_file}")

    except Exception as e:
        logger.error(f"Error in main_async: {e}", exc_info=True)

def main():
    """
    Main entry point of the script.
    """
    try:
        asyncio.run(main_async())
        logger.info("Code consolidation completed successfully")
    except KeyboardInterrupt:
        logger.warning("Process interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)

if __name__ == "__main__":
    main()

# File: data/data_loader.py
#------------------------------------------------------------------------------
import json
import logging
from typing import List, Dict, Any

from src.utils.logger import setup_logging
logger = logging.getLogger(__name__)

class JSONLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Dict[str, Any]]:
        data = []
        try:
            if self.file_path.endswith('.jsonl'):
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    for line_number, line in enumerate(f, start=1):
                        if line.strip():
                            try:
                                obj = json.loads(line)
                                data.append(obj)
                            except json.JSONDecodeError as e:
                                logger.error(f"JSON parsing error in file '{self.file_path}' at line {line_number}: {e}")
                logger.info(f"Loaded JSONL file '{self.file_path}' with {len(data)} entries successfully.")
            else:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logger.info(f"Loaded JSON file '{self.file_path}' successfully with {len(data)} entries.")
            return data
        except FileNotFoundError:
            logger.error(f"Error loading JSON file '{self.file_path}': File does not exist.")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"Error loading JSON file '{self.file_path}': {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error loading JSON file '{self.file_path}': {e}")
            return []

def load_codebase_chunks(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading codebase chunks from '{file_path}'.")
    loader = JSONLoader(file_path)
    return loader.load()

def load_queries(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading queries from '{file_path}'.")
    loader = JSONLoader(file_path)
    return loader.load()



################################################################################
# Module: root
################################################################################


# File: decorators.py
#------------------------------------------------------------------------------
import logging
import functools

logger = logging.getLogger(__name__)

def handle_exceptions(func):
    """
    Decorator to handle exceptions in functions.
    
    Args:
        func (callable): The function to wrap with exception handling.
    
    Returns:
        callable: The wrapped function with exception handling.
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}", exc_info=True)
            return {"error": "An error occurred. Please try again later."}
    return wrapper



################################################################################
# Module: docs
################################################################################


# File: docs/mkdocs.yml
#------------------------------------------------------------------------------
site_name: DSPy Documentation
site_description: The framework for programming—rather than prompting—language models.
site_url: https://dspy.ai

repo_url: https://github.com/stanfordnlp/dspy
repo_name: stanfordnlp/dspy

edit_uri: blob/main/docs/docs/
docs_dir: "docs/"
nav:
  - Home: index.md
  - Learn DSPy:
      - Learning DSPy: learn/index.md
      - DSPy Programming:
          - Programming Overview: learn/programming/overview.md
          - Language Models: learn/programming/language_models.md
          - Signatures: learn/programming/signatures.md
          - Modules: learn/programming/modules.md
      - DSPy Evaluation:
          - Evaluation Overview: learn/evaluation/overview.md
          - Data Handling: learn/evaluation/data.md
          - Metrics: learn/evaluation/metrics.md
      - DSPy Optimization:
          - Optimization Overview: learn/optimization/overview.md
          - Optimizers: learn/optimization/optimizers.md
      - Other References:
          - Retrieval Clients:
              - Azure: deep-dive/retrieval_models_clients/Azure.md
              - ChromadbRM: deep-dive/retrieval_models_clients/ChromadbRM.md
              - ClarifaiRM: deep-dive/retrieval_models_clients/ClarifaiRM.md
              - ColBERTv2: deep-dive/retrieval_models_clients/ColBERTv2.md
              - Custom RM Client: deep-dive/retrieval_models_clients/custom-rm-client.md
              - DatabricksRM: deep-dive/retrieval_models_clients/DatabricksRM.md
              - FaissRM: deep-dive/retrieval_models_clients/FaissRM.md
              - LancedbRM: deep-dive/retrieval_models_clients/LancedbRM.md
              - MilvusRM: deep-dive/retrieval_models_clients/MilvusRM.md
              - MyScaleRM: deep-dive/retrieval_models_clients/MyScaleRM.md
              - Neo4jRM: deep-dive/retrieval_models_clients/Neo4jRM.md
              - QdrantRM: deep-dive/retrieval_models_clients/QdrantRM.md
              - RAGatouilleRM: deep-dive/retrieval_models_clients/RAGatouilleRM.md
              - SnowflakeRM: deep-dive/retrieval_models_clients/SnowflakeRM.md
              - WatsonDiscovery: deep-dive/retrieval_models_clients/WatsonDiscovery.md
              - WeaviateRM: deep-dive/retrieval_models_clients/WeaviateRM.md
              - YouRM: deep-dive/retrieval_models_clients/YouRM.md
  - Tutorials:
      - Tutorials Overview: tutorials/index.md
      - Retrieval-Augmented Generation: tutorials/rag/index.ipynb
      - Entity Extraction: tutorials/entity_extraction/index.ipynb
      - Deployment: tutorials/deployment/index.md
  - Community:
      - Community Resources: community/community-resources.md
      - Use Cases: community/use-cases.md
      - Roadmap: roadmap.md
      - Contributing: community/how-to-contribute.md
  - FAQ:
      - FAQ: faqs.md
      - Cheatsheet: cheatsheet.md

theme:
  name: material
  custom_dir: overrides
  features:
    - navigation.tabs
    - navigation.path
    - navigation.indexes
    - toc.follow
    - navigation.top
    - search.suggest
    - search.highlight
    - content.tabs.link
    - content.code.annotation
    - content.code.copy
    - navigation.footer
    - content.action.edit
  language: en
  palette:
    - scheme: default
      toggle:
        icon: material/weather-night
        name: Switch to dark mode
      primary: white
      accent: black
    - scheme: slate
      toggle:
        icon: material/weather-sunny
        name: Switch to light mode
      primary: black
      accent: lime
  icon:
    repo: fontawesome/brands/git-alt
    edit: material/pencil
    view: material/eye
  logo: static/img/dspy_logo.png
  favicon: static/img/logo.png

extra_css:
  - stylesheets/extra.css

plugins:
  - social
  - search
  - mkdocstrings
  # - blog
  - mkdocs-jupyter:
      ignore_h1_titles: True
  - redirects:
      redirect_maps:
        # Redirect /intro/ to the main page
        "intro/index.md": "index.md"
        "intro.md": "index.md"

        "docs/quick-start/getting-started-01.md": "tutorials/rag/index.ipynb"
        "docs/quick-start/getting-started-02.md": "tutorials/rag/index.ipynb"
        "quick-start/getting-started-01.md": "tutorials/rag/index.ipynb"
        "quick-start/getting-started-02.md": "tutorials/rag/index.ipynb"

extra:
  social:
    - icon: fontawesome/brands/github
      link: https://github.com/stanfordnlp/dspy
    - icon: fontawesome/brands/discord
      link: https://discord.gg/XCGy2WDCQB

extra_javascript:
  - "js/runllm-widget.js"

markdown_extensions:
  - pymdownx.tabbed:
      alternate_style: true
  - pymdownx.highlight:
      anchor_linenums: true
  - pymdownx.inlinehilite
  - pymdownx.snippets
  - admonition
  - pymdownx.arithmatex:
      generic: true
  - footnotes
  - pymdownx.details
  - pymdownx.superfences
  - pymdownx.mark
  - attr_list
  - pymdownx.emoji:
      emoji_index: !!python/name:material.extensions.emoji.twemoji
      emoji_generator: !!python/name:materialx.emoji.to_svg

copyright: |
  &copy; 2024 <a href="https://github.com/stanfordnlp"  target="_blank" rel="noopener">Stanford NLP</a>



################################################################################
# Module: evaluation
################################################################################


# File: evaluation/__init__.py
#------------------------------------------------------------------------------
"""
Evaluation modules for assessing model performance.
"""

from .evaluation import PipelineEvaluator
from .evaluator import PipelineEvaluator as Evaluator

__all__ = ['PipelineEvaluator', 'Evaluator']

# File: evaluation/evaluation.py
#------------------------------------------------------------------------------
import logging
import time
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.analysis.metrics import comprehensive_metric, is_answer_fully_correct

setup_logging()
logger = logging.getLogger(__name__)

class PipelineEvaluator:
    def __init__(self, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, retrieval_function: Callable):
        self.db = db
        self.es_bm25 = es_bm25
        self.retrieval_function = retrieval_function

    def evaluate_pipeline(self, queries: List[Dict[str, Any]], k: int = 20) -> Dict[str, float]:
        total_score = 0
        total_queries = len(queries)
        queries_with_golden = 0
        queries_without_golden = 0

        semantic_success = 0
        bm25_contextual_success = 0
        total_semantic_hits = 0
        total_bm25_contextual_hits = 0

        logger.info(f"Starting evaluation of {total_queries} queries.")

        for query_item in tqdm(queries, desc="Evaluating retrieval"):
            query = query_item.get('query', '').strip()

            has_golden_data = all([
                'golden_doc_uuids' in query_item,
                'golden_chunk_uuids' in query_item,
                'golden_documents' in query_item
            ])

            if has_golden_data:
                queries_with_golden += 1
                golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
                golden_contents = []

                for doc_uuid, chunk_index in golden_chunk_uuids:
                    golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                    if not golden_doc:
                        logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                    if not golden_chunk:
                        logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_contents.append(golden_chunk.get('content', '').strip())

                if not golden_contents:
                    logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                    continue

                retrieved_docs = self.retrieval_function(query, self.db, self.es_bm25, k)

                chunks_found = 0
                semantic_hits = 0
                bm25_contextual_hits = 0

                for golden_content in golden_contents:
                    for doc in retrieved_docs[:k]:
                        retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                        contextualized_content = doc.get('chunk', {}).get('contextualized_content', '').strip()
                        if retrieved_content == golden_content:
                            chunks_found += 1
                            semantic_hits += 1
                            break
                        elif contextualized_content == golden_content:
                            chunks_found += 1
                            bm25_contextual_hits += 1
                            break

                query_score = chunks_found / len(golden_contents)
                total_score += query_score
                logger.debug(f"Query '{query}' score: {query_score}")

                semantic_success += semantic_hits
                bm25_contextual_success += bm25_contextual_hits
                total_semantic_hits += semantic_hits
                total_bm25_contextual_hits += bm25_contextual_hits
            else:
                queries_without_golden += 1
                logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
                continue

        average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
        pass_at_n = average_score * 100

        semantic_percentage = (semantic_success / total_semantic_hits) * 100 if total_semantic_hits > 0 else 0
        bm25_contextual_percentage = (bm25_contextual_success / total_bm25_contextual_hits) * 100 if total_bm25_contextual_hits > 0 else 0

        logger.info(f"Evaluation completed.")
        logger.info(f"Total Queries: {total_queries}")
        logger.info(f"Queries with Golden Data: {queries_with_golden}")
        logger.info(f"Queries without Golden Data: {queries_without_golden}")
        logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")
        logger.info(f"Semantic Hits: {semantic_percentage:.2f}%")
        logger.info(f"BM25 Contextual Hits: {bm25_contextual_percentage:.2f}%")

        return {
            "pass_at_n": pass_at_n,
            "average_score": average_score,
            "semantic_hit_percentage": semantic_percentage,
            "bm25_contextual_hit_percentage": bm25_contextual_percentage,
            "total_queries": total_queries,
            "queries_with_golden": queries_with_golden,
            "queries_without_golden": queries_without_golden
        }

    def evaluate_complete_pipeline(self, k_values: List[int], evaluation_set: List[Dict[str, Any]]):
        for k in k_values:
            logger.info(f"Starting evaluation for Pass@{k}")
            results = self.evaluate_pipeline(evaluation_set, k)
            logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
            logger.info(f"Average Score: {results['average_score']:.4f}")
            logger.info(f"Semantic Hit Percentage: {results['semantic_hit_percentage']:.2f}%")
            logger.info(f"BM25 Contextual Hit Percentage: {results['bm25_contextual_hit_percentage']:.2f}%")
            logger.info(f"Total Queries: {results['total_queries']}")
            logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
            logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")


# File: evaluation/evaluator.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25

logger = logging.getLogger(__name__)

class PipelineEvaluator:
    def __init__(self, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, retrieval_function: Callable):
        self.db = db
        self.es_bm25 = es_bm25
        self.retrieval_function = retrieval_function

    def evaluate_pipeline(self, queries: List[Dict[str, Any]], k: int = 20) -> Dict[str, float]:
        total_score = 0
        total_queries = len(queries)
        queries_with_golden = 0
        queries_without_golden = 0

        logger.info(f"Starting evaluation of {total_queries} queries.")

        for query_item in tqdm(queries, desc="Evaluating retrieval"):
            query = query_item.get('query', '').strip()

            has_golden_data = all([
                'golden_doc_uuids' in query_item,
                'golden_chunk_uuids' in query_item,
                'golden_documents' in query_item
            ])

            if has_golden_data:
                queries_with_golden += 1
                golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
                golden_contents = []

                for doc_uuid, chunk_index in golden_chunk_uuids:
                    golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                    if not golden_doc:
                        logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                    if not golden_chunk:
                        logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_contents.append(golden_chunk.get('content', '').strip())

                if not golden_contents:
                    logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                    continue

                retrieved_docs = self.retrieval_function(query, self.db, self.es_bm25, k)

                chunks_found = 0
                for golden_content in golden_contents:
                    for doc in retrieved_docs[:k]:
                        retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                        if retrieved_content == golden_content:
                            chunks_found += 1
                            break

                query_score = chunks_found / len(golden_contents)
                total_score += query_score
                logger.debug(f"Query '{query}' score: {query_score}")
            else:
                queries_without_golden += 1
                logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
                continue

        average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
        pass_at_n = average_score * 100

        logger.info(f"Evaluation completed.")
        logger.info(f"Total Queries: {total_queries}")
        logger.info(f"Queries with Golden Data: {queries_with_golden}")
        logger.info(f"Queries without Golden Data: {queries_without_golden}")
        logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")

        return {
            "pass_at_n": pass_at_n,
            "average_score": average_score,
            "total_queries": total_queries,
            "queries_with_golden": queries_with_golden,
            "queries_without_golden": queries_without_golden
        }

    def evaluate_complete_pipeline(self, k_values: List[int], evaluation_set: List[Dict[str, Any]]):
        for k in k_values:
            logger.info(f"Starting evaluation for Pass@{k}")
            results = self.evaluate_pipeline(evaluation_set, k)
            logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
            logger.info(f"Average Score: {results['average_score']:.4f}")
            logger.info(f"Total Queries: {results['total_queries']}")
            logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
            logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")



################################################################################
# Module: root
################################################################################


# File: main.py
#------------------------------------------------------------------------------
# src/main.py

import asyncio
import logging
import sys
import os

# Add the parent directory to the Python path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pipeline.pipeline_configs import ModuleConfig
from pipeline.pipeline_runner import ThematicAnalysisPipeline

from analysis.select_quotation_module import EnhancedQuotationModule as EnhancedQuotationModuleStandard
from analysis.extract_keyword_module import KeywordExtractionModule
from analysis.coding_module import CodingAnalysisModule
from analysis.grouping_module import GroupingAnalysisModule
from analysis.theme_development_module import ThemedevelopmentAnalysisModule

from convert.convertquotationforkeyword import convert_query_results as convert_quotation_to_keyword
from convert.convertkeywordforcoding import convert_query_results as convert_keyword_to_coding
from convert.convertcodingforgrouping import convert_query_results as convert_coding_to_grouping
from convert.convertgroupingfortheme import convert_query_results as convert_grouping_to_theme

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.info("Launching Refactored Thematic Analysis Pipeline")

    configs = [
        ModuleConfig(
            index_name='contextual_bm25_index_standard_quotation',
            codebase_chunks_file='data/codebase_chunks/codebase_chunks.json',
            queries_file_standard='data/input/queries_quotation.json',
            evaluation_set_file='data/evaluation/evaluation_set_quotation.jsonl',
            output_filename_primary='data/output/query_results_quotation.json',
            training_data='data/training/quotation_training_data.csv',
            optimized_program_path='data/optimized/optimized_quotation_program.json',
            module_class=EnhancedQuotationModuleStandard,
            conversion_func=convert_quotation_to_keyword
        ),
        ModuleConfig(
            index_name='contextual_bm25_index_keyword_extraction',
            codebase_chunks_file='data/codebase_chunks/codebase_chunks.json',
            queries_file_standard='data/input/queries_keyword_standard.json',
            evaluation_set_file='data/evaluation/evaluation_set_keyword.jsonl',
            output_filename_primary='data/output/query_results_keyword_extraction.json',
            training_data='data/training/keyword_training_data.csv',
            optimized_program_path='data/optimized/optimized_keyword_program.json',
            module_class=KeywordExtractionModule,
            conversion_func=convert_keyword_to_coding
        ),
        ModuleConfig(
            index_name='contextual_bm25_index_coding_analysis',
            codebase_chunks_file='data/codebase_chunks/codebase_chunks.json',
            queries_file_standard='data/input/queries_coding_standard.json',
            evaluation_set_file='data/evaluation/evaluation_set_coding.jsonl',
            output_filename_primary='data/output/query_results_coding_analysis.json',
            training_data='data/training/coding_training_data.csv',
            optimized_program_path='data/optimized/optimized_coding_program.json',
            module_class=CodingAnalysisModule,
            conversion_func=convert_coding_to_grouping
        ),
        ModuleConfig(
            index_name='contextual_bm25_index_grouping',
            codebase_chunks_file='data/codebase_chunks/codebase_chunks.json',
            queries_file_standard='data/input/queries_grouping.json',
            evaluation_set_file='data/evaluation/evaluation_set_grouping.jsonl',
            output_filename_primary='data/output/query_results_grouping.json',
            training_data='data/training/grouping_training_data.csv',
            optimized_program_path='data/optimized/optimized_grouping_program.json',
            module_class=GroupingAnalysisModule,
            conversion_func=convert_grouping_to_theme
        ),
        ModuleConfig(
            index_name='contextual_bm25_index_theme_development',
            codebase_chunks_file='data/codebase_chunks/codebase_chunks.json',
            queries_file_standard='data/input/queries_theme.json',
            evaluation_set_file='data/evaluation/evaluation_set_theme.jsonl',
            output_filename_primary='data/output/query_results_theme_development.json',
            training_data='data/training/theme_training_data.csv',
            optimized_program_path='data/optimized/optimized_theme_program.json',
            module_class=ThemedevelopmentAnalysisModule,
            conversion_func=None
        )
    ]

    pipeline = ThematicAnalysisPipeline()

    try:
        asyncio.run(pipeline.run_pipeline(configs))
        logger.info("Thematic Analysis Pipeline completed successfully.")
    except Exception as e:
        logger.critical(f"Pipeline execution failed: {e}", exc_info=True)


################################################################################
# Module: pipeline
################################################################################


# File: pipeline/__init__.py
#------------------------------------------------------------------------------


# File: pipeline/pipeline_configs.py
#------------------------------------------------------------------------------
from dataclasses import dataclass
from typing import Any, Callable, Type, Optional

@dataclass
class OptimizerConfig:
    max_bootstrapped_demos: int = 4
    max_labeled_demos: int = 4
    num_candidate_programs: int = 10
    num_threads: int = 1
    temperature: float = 0.7
    max_tokens: int = 8192

@dataclass
class ModelConfig:
    """Configuration for the language model to be used."""
    provider: str = "openai"  # Default to OpenAI
    model_name: Optional[str] = None  # If None, will use provider's default model
    api_key_env: Optional[str] = None  # Environment variable name for API key
    
    def __post_init__(self):
        # Set default values based on provider
        if self.provider == "openai":
            self.model_name = self.model_name or "gpt-4"
            self.api_key_env = self.api_key_env or "OPENAI_API_KEY"
        elif self.provider == "google":
            self.model_name = self.model_name or "gemini-2.0-flash-thinking-exp-01-21"
            self.api_key_env = self.api_key_env or "GOOGLE_API_KEY"
        elif self.provider == "deepseek":
            self.model_name = self.model_name or "deepseek/deepseek-reasoner"
            self.api_key_env = self.api_key_env or "DEEPSEEK_API_KEY"
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

@dataclass
class ModuleConfig:
    index_name: str
    codebase_chunks_file: str
    queries_file_standard: str
    evaluation_set_file: str
    output_filename_primary: str
    training_data: str
    optimized_program_path: str
    module_class: Type[Any]
    conversion_func: Optional[Callable[[str, str, str], None]] = None
    model_config: Optional[ModelConfig] = None  # Added field for model configuration

    def __post_init__(self):
        # If no model config is provided, use default OpenAI configuration
        if self.model_config is None:
            self.model_config = ModelConfig()

# File: pipeline/pipeline_data.py
#------------------------------------------------------------------------------
# src/pipeline/pipeline_data.py

import logging
import os
import json
from typing import List

def create_directories(dir_paths: List[str]) -> None:
    """
    Creates directories if they do not exist.
    """
    logger = logging.getLogger(__name__)
    for path in dir_paths:
        try:
            os.makedirs(path, exist_ok=True)
            logger.info(f"Directory ensured: {path}")
        except Exception as e:
            logger.error(f"Failed to create directory {path}: {e}", exc_info=True)
            raise

def generate_theme_input(info_path: str, grouping_path: str, output_path: str) -> None:
    """
    Generates queries_theme.json from info.json and query_results_grouping.json.
    """
    logger = logging.getLogger(__name__)
    logger.info("Generating queries_theme.json...")

    if not os.path.exists(info_path):
        logger.error("info.json not found.")
        return

    if not os.path.exists(grouping_path):
        logger.error("query_results_grouping.json not found.")
        return

    with open(info_path, 'r', encoding='utf-8') as f:
        info = json.load(f)

    with open(grouping_path, 'r', encoding='utf-8') as f:
        grouping_results = json.load(f)

    if not grouping_results:
        logger.error("query_results_grouping.json is empty.")
        return

    first_result = grouping_results[0]
    research_objectives = info.get("research_objectives", "")
    theoretical_framework = info.get("theoretical_framework", {})

    codes = first_result.get("grouping_info", {}).get("codes", [])
    groupings = first_result.get("groupings", [])

    # Assume placeholders for demonstration
    quotation = "Original quotation from previous step."
    keywords = ["improvement", "reasoning", "innovation"]
    transcript_chunk = "Some relevant transcript chunk."

    queries_theme = [
        {
            "quotation": quotation,
            "keywords": keywords,
            "codes": codes,
            "research_objectives": research_objectives,
            "theoretical_framework": theoretical_framework,
            "transcript_chunk": transcript_chunk,
            "groupings": groupings
        }
    ]

    create_directories([os.path.dirname(output_path)])
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(queries_theme, f, indent=4)
    logger.info(f"Generated {output_path} successfully.")

# File: pipeline/pipeline_optimizer.py
#------------------------------------------------------------------------------
# src/pipeline/pipeline_optimizer.py

import logging
import time
import dspy
from dspy.datasets import DataLoader
from dspy.teleprompt import BootstrapFewShotWithRandomSearch
from dspy.primitives.assertions import backtrack_handler
from src.analysis.metrics import comprehensive_metric
from src.processing.answer_generator import QuestionAnswerSignature
from src.pipeline.pipeline_configs import OptimizerConfig, ModuleConfig

logger = logging.getLogger(__name__)

async def initialize_optimizer(config: ModuleConfig, optimizer_config: OptimizerConfig, optimized_programs: dict):
    """
    Initializes and trains a teleprompt-based optimizer for the specified module.
    """
    module_name = config.module_class.__name__.replace("Module", "").lower()
    logger.info(f"Initializing {module_name} optimizer")
    start_time = time.time()

    try:
        dl = DataLoader()
        train_dataset = dl.from_csv(
            config.training_data,
            fields=("input", "output"),
            input_keys=("input",)
        )
        logger.info(f"Loaded {len(train_dataset)} samples for {module_name} training data")

        qa_module = dspy.ChainOfThought(QuestionAnswerSignature)
        teleprompter = BootstrapFewShotWithRandomSearch(
            metric=comprehensive_metric,
            max_bootstrapped_demos=optimizer_config.max_bootstrapped_demos,
            max_labeled_demos=optimizer_config.max_labeled_demos,
            num_candidate_programs=optimizer_config.num_candidate_programs,
            num_threads=optimizer_config.num_threads
        )

        compile_start = time.time()
        optimized_program = teleprompter.compile(
            student=qa_module,
            teacher=qa_module,
            trainset=train_dataset
        )
        compile_time = time.time() - compile_start
        logger.info(f"Compiled optimized {module_name} program in {compile_time:.2f}s")

        optimized_program.save(config.optimized_program_path)
        logger.info(f"Saved optimized {module_name} program to {config.optimized_program_path}")

        optimized_programs[module_name] = optimized_program
    except Exception as e:
        logger.error(f"Error initializing {module_name} optimizer: {e}", exc_info=True)
        raise

    total_time = time.time() - start_time
    logger.info(f"{module_name.capitalize()} optimizer initialization completed in {total_time:.2f}s")

# File: pipeline/pipeline_runner.py
#------------------------------------------------------------------------------
#pipeline_runner.py
import asyncio
import logging
import time
import gc
import os
from typing import Optional, Callable
from dotenv import load_dotenv

import dspy
from dspy.primitives.assertions import assert_transform_module, backtrack_handler
from src.config.model_config import get_model_config, ModelProvider
from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.data.data_loader import load_codebase_chunks, load_queries
from src.decorators import handle_exceptions
from src.evaluation.evaluation import PipelineEvaluator
from src.pipeline.pipeline_configs import OptimizerConfig, ModuleConfig
from src.pipeline.pipeline_data import create_directories, generate_theme_input
from src.pipeline.pipeline_optimizer import initialize_optimizer
from src.processing.query_processor import process_queries

from src.core.retrieval.reranking import retrieve_with_reranking, RerankerConfig, RerankerType
from src.utils.logger import setup_logging

logger = logging.getLogger(__name__)

class ThematicAnalysisPipeline:
    def __init__(self):
        load_dotenv()
        setup_logging()
        logger.info("Initializing ThematicAnalysisPipeline")

        create_directories([
            'data/input',
            'data/output',
            'data/codebase_chunks',
            'data/optimized',
            'data/training',
            'data/evaluation'
        ])

        self.contextual_db = ContextualVectorDB("contextual_db")
        self.es_bm25: Optional[ElasticsearchBM25] = None
        self.optimized_programs = {}

    def _setup_model_environment(self, model_config):
        """Set up environment variables for the specified model."""
        api_key = os.getenv(model_config.api_key_env)
        if not api_key:
            raise ValueError(f"{model_config.api_key_env} environment variable is not set")

        # Set provider-specific environment variables
        if model_config.provider == ModelProvider.OPENAI:
            os.environ['OPENAI_API_KEY'] = api_key
        elif model_config.provider == ModelProvider.GOOGLE:
            os.environ['GOOGLE_API_KEY'] = api_key
        elif model_config.provider == ModelProvider.DEEPSEEK:
            os.environ['DEEPSEEK_API_KEY'] = api_key

    def create_elasticsearch_bm25_index(self, index_name: str) -> ElasticsearchBM25:
        logger.info(f"Creating Elasticsearch BM25 index: {index_name}")
        start_time = time.time()
        try:
            es_bm25 = ElasticsearchBM25(index_name=index_name)
            success_count, failed_docs = es_bm25.index_documents(self.contextual_db.metadata)
            if failed_docs:
                logger.warning(f"Failed to index {len(failed_docs)} documents")
            total_time = time.time() - start_time
            logger.info(f"Elasticsearch BM25 index creation completed in {total_time:.2f}s")
            return es_bm25
        except Exception as e:
            logger.error(f"Error creating Elasticsearch BM25 index '{index_name}': {e}", exc_info=True)
            raise

    def _configure_language_model(self, model_config):
        """Configure the language model based on provider."""
        logger.info(f"Configuring DSPy Language Model for provider: {model_config.provider.value}")
        
        if model_config.provider == ModelProvider.OPENAI:
            model_name = model_config.model_name
        elif model_config.provider == ModelProvider.GOOGLE:
            model_name = f"google/{model_config.model_name}"
        elif model_config.provider == ModelProvider.DEEPSEEK:
            model_name = model_config.model_name
        else:
            raise ValueError(f"Unsupported provider: {model_config.provider}")

        lm = dspy.LM(
            model_name,
            max_tokens=model_config.max_tokens,
            temperature=model_config.temperature
        )
        dspy.configure(lm=lm)
        dspy.Cache = False
        return lm

    @handle_exceptions
    async def run_pipeline_with_config(self, config: ModuleConfig, optimizer_config: OptimizerConfig):
        module_name = config.module_class.__name__.replace("Module", "").lower()
        logger.info(f"Starting pipeline stage for {module_name.capitalize()}")
        pipeline_start_time = time.time()

        try:
            # Get model configuration from ModuleConfig or use default
            model_config = get_model_config(getattr(config, 'model_provider', None))
            
            # Setup environment variables for the selected model
            self._setup_model_environment(model_config)
            
            # Configure the language model
            lm = self._configure_language_model(model_config)

            logger.info(f"Loading codebase chunks from {config.codebase_chunks_file}")
            codebase_chunks = load_codebase_chunks(config.codebase_chunks_file)

            logger.info("Loading data into ContextualVectorDB")
            self.contextual_db.load_data(codebase_chunks, parallel_threads=4)

            logger.info(f"Creating Elasticsearch BM25 index: {config.index_name}")
            self.es_bm25 = self.create_elasticsearch_bm25_index(config.index_name)

            logger.info(f"Loading queries from {config.queries_file_standard}")
            standard_queries = load_queries(config.queries_file_standard)

            logger.info(f"Initializing optimizer for {module_name.capitalize()}")
            await initialize_optimizer(config, optimizer_config, self.optimized_programs)

            module_instance = config.module_class()
            logger.debug(f"Module instance created: {type(module_instance).__name__}")
            module_instance = assert_transform_module(module_instance, backtrack_handler)

            optimized_program = self.optimized_programs.get(module_name)
            if not optimized_program:
                logger.error(f"Optimized program for {module_name} not found.")
                return

            logger.info(f"Processing queries for {module_name.capitalize()}")
            await process_queries(
                transcripts=standard_queries,
                db=self.contextual_db,
                es_bm25=self.es_bm25,
                k=20,
                output_file=config.output_filename_primary,
                optimized_program=optimized_program,
                module=module_instance
            )

            logger.info(f"Starting evaluation for {module_name.capitalize()}")
            evaluator = PipelineEvaluator(
                db=self.contextual_db,
                es_bm25=self.es_bm25,
                retrieval_function=lambda query, db, es_bm25, k: retrieve_with_reranking(
                    query, db, es_bm25, k, RerankerConfig(
                        reranker_type=RerankerType.COHERE,
                        cohere_api_key=os.getenv("COHERE_API_KEY"),
                        st_weight=0.5
                    )
                )
            )
            evaluation_set = load_queries(config.evaluation_set_file)
            evaluator.evaluate_complete_pipeline(k_values=[5, 10, 20], evaluation_set=evaluation_set)

            total_time = time.time() - pipeline_start_time
            logger.info(f"Pipeline stage for {module_name.capitalize()} completed in {total_time:.2f}s")

        except Exception as e:
            logger.error(f"Error in pipeline execution for {module_name}: {e}", exc_info=True)
            raise

    async def convert_results(
        self,
        conversion_func: Optional[Callable[[str, str, str], None]],
        input_file: str,
        output_dir: str,
        output_file: str
    ):
        if not conversion_func:
            logger.info("No conversion function provided; skipping.")
            return

        create_directories([output_dir])
        logger.info(f"Converting results with {conversion_func.__name__}")
        try:
            await asyncio.to_thread(
                conversion_func,
                input_file=input_file,
                output_dir=output_dir,
                output_file=output_file
            )
        except Exception as e:
            logger.error(f"Error converting results with {conversion_func.__name__}: {e}", exc_info=True)
            raise

    @handle_exceptions
    async def run_pipeline(self, configs: list[ModuleConfig]):
        logger.info("Starting the entire Thematic Analysis Pipeline")
        total_start_time = time.time()

        try:
            optimizer_config = OptimizerConfig()

            for idx, config in enumerate(configs):
                logger.info(f"--- Running {config.module_class.__name__} stage ---")
                await self.run_pipeline_with_config(config, optimizer_config)

                if idx < len(configs) - 1 and config.conversion_func:
                    next_config = configs[idx + 1]
                    await self.convert_results(
                        conversion_func=config.conversion_func,
                        input_file=config.output_filename_primary,
                        output_dir='data/',
                        output_file=os.path.basename(next_config.queries_file_standard)
                    )

            generate_theme_input(
                info_path='data/input/info.json',
                grouping_path='data/output/query_results_grouping.json',
                output_path='data/input/queries_theme.json'
            )

            final_config = configs[-1]
            await self.run_pipeline_with_config(final_config, optimizer_config)

            total_time = time.time() - total_start_time
            logger.info(f"Entire pipeline completed in {total_time:.2f}s")

        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}", exc_info=True)
            raise
        finally:
            logger.info("Thematic Analysis Pipeline execution finished")
            gc.collect()


################################################################################
# Module: processing
################################################################################


# File: processing/__init__.py
#------------------------------------------------------------------------------
# processing/__init__.py
"""
Processing package initialization.
"""


# File: processing/answer_generator.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
import dspy
import asyncio

from src.analysis.metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from src.utils.utils import check_answer_length

logger = logging.getLogger(__name__)

class QuestionAnswerSignature(dspy.Signature):
    input: str = dspy.InputField(
        desc=(
            "The combined input containing both the question and context. "
            "The format should be 'question: <question_text> context: <context_text>'."
        )
    )
    answer: str = dspy.OutputField(
        desc=(
            "The generated answer to the question. The answer should be concise, directly address "
            "the question, and be grounded in the provided context to ensure factual accuracy."
        )
    )

    def forward(self, input: str, max_tokens: int = 8192) -> Dict[str, str]:
        try:
            # Parse the input to extract question and context
            parts = input.split(' context: ', 1)
            question = parts[0].replace('question: ', '').strip()
            context = parts[1].strip() if len(parts) > 1 else ""

            logger.debug(f"Generating answer for question: '{question}' with context length: {len(context)} characters.")
            answer = self.language_model.generate(
                prompt=(
                    f"You are an expert in qualitative research and thematic analysis.\n\n"
                    f"**Guidelines**:\n"
                    f"- **Relevance:** Extract quotations that are closely related to the key themes.\n"
                    f"- **Diversity:** Ensure a range of perspectives and viewpoints.\n"
                    f"- **Clarity:** Choose clear and understandable quotations.\n"
                    f"- **Impact:** Select impactful quotations that highlight significant aspects of the data.\n"
                    f"- **Authenticity:** Maintain original expressions from participants.\n\n"
                    f"**Transcript Chunk**:\n{question}\n\n"
                    f"**Context:**\n{context}\n\n"
                    f"**Task:** Extract **3-5** relevant quotations from the transcript chunk based on the context provided. "
                    f"Provide each quotation in the following JSON format within a list:\n\n"
                    f"```json\n"
                    f"[\n"
                    f"    {{\"QUOTE\": \"This is the first quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the second quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the third quotation.\"}}\n"
                    f"]\n"
                    f"```"
                    f"Ensure that the response is a valid JSON array containing all relevant quotations. "
                    f"If no quotations are available, respond with an empty array `[]`."
                ),
                max_tokens=max_tokens,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=None
            ).strip()
            logger.info(f"Generated answer for question: '{question}'")
            logger.debug(f"Answer length: {len(answer)} characters.")
            return {"answer": answer}
        except Exception as e:
            logger.error(f"Error in QuestionAnswerSignature.forward: {e}", exc_info=True)
            return {"answer": "I'm sorry, I couldn't generate an answer at this time."}

try:
    qa_module = dspy.Program.load("optimized_program.json")
    logger.info("Optimized DSPy program loaded successfully.")
except Exception as e:
    try:
        qa_module = dspy.ChainOfThought(QuestionAnswerSignature)
        logger.info("Unoptimized DSPy module initialized successfully.")
    except Exception as inner_e:
        logger.error(f"Error initializing unoptimized DSPy module: {inner_e}", exc_info=True)
        raise

async def generate_answer(input: str, max_tokens: int = 8192) -> str:
    try:
        logger.debug(f"Generating answer for input with length: {len(input)} characters.")
        answer = await asyncio.to_thread(qa_module, input=input, max_tokens=max_tokens)
        return answer.get("answer", "I'm sorry, I couldn't generate an answer at this time.")
    except Exception as e:
        logger.error(f"Error in generate_answer: {e}", exc_info=True)
        return "I'm sorry, I couldn't generate an answer at this time."

async def evaluate_answer(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    try:
        logger.debug(f"Evaluating answer for input: '{example.get('input', '')}'")
        return await asyncio.to_thread(is_answer_fully_correct, example, pred)
    except Exception as e:
        logger.error(f"Error in evaluate_answer: {e}", exc_info=True)
        return False

async def generate_answer_dspy(query: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    logger.debug(f"Entering generate_answer_dspy with query='{query}' and {len(retrieved_chunks)} retrieved_chunks.")
    try:
        context = ""
        for i, chunk in enumerate(retrieved_chunks, 1):
            chunk_content = chunk['chunk'].get('original_content', '')
            chunk_context = chunk['chunk'].get('contextualized_content', '')
            context += f"Chunk {i}:\n"
            context += f"Content: {chunk_content}\n"
            context += f"Context: {chunk_context}\n\n"

        if not context.strip():
            logger.warning(f"No valid context found for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't find relevant information to answer your question.",
                "used_chunks": [],
                "num_chunks_used": 0
            }

        logger.debug(f"Formatted context with {len(retrieved_chunks)} sequential chunks:\n{context[:200]}...")
        used_chunks_info = [
            {
                "chunk_id": chunk['chunk'].get('chunk_id', ''),
                "doc_id": chunk['chunk'].get('doc_id', ''),
                "content_snippet": chunk['chunk'].get('original_content', '')[:100] + "..."
            }
            for chunk in retrieved_chunks
        ]
        logger.info(f"Total number of chunks used for context in query '{query}': {len(used_chunks_info)}")
        logger.info(f"Chunks used for context: {used_chunks_info}")
        input_data = f"question: {query} context: {context}"
        answer = await generate_answer(input_data)

        if not answer:
            logger.warning(f"No answer generated for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't generate an answer at this time.",
                "used_chunks": used_chunks_info,
                "num_chunks_used": len(used_chunks_info)
            }

        logger.debug(f"Generated answer for query '{query}': {answer}")
        logger.info(f"Number of chunks used for query '{query}': {len(used_chunks_info)}")
        example = {
            "context": context,
            "question": query
        }
        pred = {
            "answer": answer
        }
        suggestion = await evaluate_answer(example, pred)
        return {
            "answer": answer,
            "used_chunks": used_chunks_info,
            "num_chunks_used": len(used_chunks_info)
        }
    except Exception as e:
        logger.error(f"Error generating answer via DSPy for query '{query}': {e}", exc_info=True)
        return {
            "answer": "I'm sorry, I couldn't generate an answer at this time.",
            "used_chunks": [],
            "num_chunks_used": 0
        }

async def generate_answers_dspy(queries: List[str], retrieved_chunks_list: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    tasks = [
        generate_answer_dspy(query, retrieved_chunks)
        for query, retrieved_chunks in zip(queries, retrieved_chunks_list)
    ]
    return await asyncio.gather(*tasks)

def is_answer_factually_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    score = factuality_metric(example, pred)
    logger.debug(f"Factuality score: {score}")
    return score == 1


# File: processing/base.py
#------------------------------------------------------------------------------
# processing/base.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any

import dspy


class BaseValidator(ABC):
    """
    Abstract base class for validators.
    Each validator must implement a validate method
    that returns a filtered list of transcripts.
    """

    @abstractmethod
    def validate(self, transcripts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        pass


class BaseHandler(ABC):
    """
    Abstract base class for handlers (processing logic).
    Each handler must implement a `process_single_transcript`
    method that takes a transcript_item, a list of retrieved docs, and a dspy.Module.
    """

    @abstractmethod
    async def process_single_transcript(
        self,
        transcript_item: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]],
        module: dspy.Module
    ) -> Dict[str, Any]:
        pass


# File: processing/config.py
#------------------------------------------------------------------------------
# processing/config.py
import os

# --------------------------------------------------------------------
# Configuration Management
# --------------------------------------------------------------------
# Place all hard-coded paths and values (or environment variables) here
# to centralize their management.

INFO_PATH = os.getenv("INFO_PATH", "data/input/info.json")
COHERE_API_KEY = os.getenv("COHERE_API_KEY", "")
ST_WEIGHT = float(os.getenv("ST_WEIGHT", "0.5"))

DEFAULT_RESEARCH_OBJECTIVES = "No specific research objectives provided."
DEFAULT_THEORETICAL_FRAMEWORK = {}

# You can extend this file as needed for additional configuration parameters


# File: processing/factories.py
#------------------------------------------------------------------------------
# src/processing/factories.py

import dspy
from src.analysis.select_quotation_module import SelectQuotationModule, EnhancedQuotationModule
from src.analysis.extract_keyword_module import KeywordExtractionModule
from src.analysis.coding_module import CodingAnalysisModule
from src.analysis.theme_development_module import ThemedevelopmentAnalysisModule
from src.analysis.grouping_module import GroupingAnalysisModule

from .handlers import (
    QuotationHandler,
    KeywordHandler,
    CodingHandler,
    GroupingHandler,
    ThemeHandler
)
from .base import BaseHandler
from .logger import get_logger

logger = get_logger(__name__)

def get_handler_for_module(module: dspy.Module) -> BaseHandler:
    """
    Returns the appropriate handler instance for the given module.
    Enhanced with detailed logging for debugging purposes.
    """
    module_type = type(module).__name__
    module_class_module = type(module).__module__
    logger.debug(f"Attempting to get handler for module type: {module_type} (defined in {module_class_module})")

    # Add type debug logging
    logger.debug(f"Module type hierarchy: {type(module).__mro__}")
    
    # More explicit type checking
    if module_type in ['SelectQuotationModule', 'EnhancedQuotationModule'] or \
       isinstance(module, (SelectQuotationModule, EnhancedQuotationModule)):
        logger.debug(f"Handler found: QuotationHandler for module type: {module_type}")
        return QuotationHandler()
    elif module_type == 'KeywordExtractionModule' or isinstance(module, KeywordExtractionModule):
        logger.debug(f"Handler found: KeywordHandler for module type: {module_type}")
        return KeywordHandler()
    elif module_type == 'CodingAnalysisModule' or isinstance(module, CodingAnalysisModule):
        logger.debug(f"Handler found: CodingHandler for module type: {module_type}")
        return CodingHandler()
    elif module_type == 'GroupingAnalysisModule' or isinstance(module, GroupingAnalysisModule):
        logger.debug(f"Handler found: GroupingHandler for module type: {module_type}")
        return GroupingHandler()
    elif module_type == 'ThemedevelopmentAnalysisModule' or isinstance(module, ThemedevelopmentAnalysisModule):
        logger.debug(f"Handler found: ThemeHandler for module type: {module_type}")
        return ThemeHandler()
    else:
        logger.error(
            f"Unsupported module type: {module_type} (defined in {module_class_module})\n"
            f"Module MRO: {type(module).__mro__}"
        )
        raise ValueError(f"Unsupported module type: {module_type}")

# File: processing/handlers.py
#------------------------------------------------------------------------------
# src/processing/handlers.py

from typing import List, Dict, Any
import os
import json

import dspy

from .base import BaseHandler
from .logger import get_logger
from .config import INFO_PATH, DEFAULT_RESEARCH_OBJECTIVES, DEFAULT_THEORETICAL_FRAMEWORK
from src.decorators import handle_exceptions  # Reuse your existing decorator if needed

logger = get_logger(__name__)


class QuotationHandler(BaseHandler):
    @handle_exceptions
    async def process_single_transcript(
        self,
        transcript_item: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]],
        module: dspy.Module
    ) -> Dict[str, Any]:
        transcript_chunk = transcript_item.get('transcript_chunk', '').strip()
        if not transcript_chunk:
            logger.warning("Transcript chunk is empty. Skipping.")
            return {}

        logger.debug(f"Processing transcript chunk for quotation: {transcript_chunk[:100]}...")
        filtered_chunks = [chunk for chunk in retrieved_docs if chunk['score'] >= 0.7]
        contextualized_contents = [chunk['chunk']['contextualized_content'] for chunk in filtered_chunks]

        research_objectives = transcript_item.get('research_objectives', 'Extract relevant quotations.')
        theoretical_framework = transcript_item.get('theoretical_framework', {})

        response = module.forward(
            research_objectives=research_objectives,
            transcript_chunk=transcript_chunk,
            contextualized_contents=contextualized_contents,
            theoretical_framework=theoretical_framework
        )
        
        result = {
            "transcript_info": response.get("transcript_info", {
                "transcript_chunk": transcript_chunk,
                "research_objectives": research_objectives,
                "theoretical_framework": theoretical_framework
            }),
            "retrieved_chunks": retrieved_docs,
            "retrieved_chunks_count": len(retrieved_docs),
            "filtered_chunks_count": len(filtered_chunks),
            "contextualized_contents": contextualized_contents,
            "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in filtered_chunks],
            "quotations": response.get("quotations", []),
            "analysis": response.get("analysis", {}),
            "answer": response.get("answer", {})
        }

        if not result["quotations"]:
            logger.warning("No quotations selected.")
            result["answer"] = {"answer": "No relevant quotations were found."}

        logger.debug(f"Selected {len(result['quotations'])} quotations.")
        return result


class KeywordHandler(BaseHandler):
    @handle_exceptions
    async def process_single_transcript(
        self,
        transcript_item: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]],
        module: dspy.Module
    ) -> Dict[str, Any]:
        quotation = transcript_item.get('quotation', '').strip()
        if not quotation:
            logger.warning("Quotation is empty. Skipping.")
            return {}

        logger.debug(f"Processing quotation for keywords: {quotation[:100]}...")
        filtered_chunks = [chunk for chunk in retrieved_docs if chunk['score'] >= 0.7]
        contextualized_contents = [chunk['chunk']['contextualized_content'] for chunk in filtered_chunks]

        research_objectives = transcript_item.get('research_objectives', 'Extract relevant keywords.')
        theoretical_framework = transcript_item.get('theoretical_framework', {})

        response = module.forward(
            research_objectives=research_objectives,
            quotation=quotation,
            contextualized_contents=contextualized_contents,
            theoretical_framework=theoretical_framework
        )
        
        result = {
            "quotation_info": response.get("quotation_info", {
                "quotation": quotation,
                "research_objectives": research_objectives,
                "theoretical_framework": theoretical_framework
            }),
            "retrieved_chunks": retrieved_docs,
            "retrieved_chunks_count": len(retrieved_docs),
            "filtered_chunks_count": len(filtered_chunks),
            "contextualized_contents": contextualized_contents,
            "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in filtered_chunks],
            "keywords": response.get("keywords", []),
            "analysis": response.get("analysis", {})
        }

        if not result["keywords"]:
            logger.warning("No keywords extracted.")
            result["analysis"]["error"] = "No relevant keywords found."

        logger.debug(f"Extracted {len(result['keywords'])} keywords.")
        return result


class CodingHandler(BaseHandler):
    @handle_exceptions
    async def process_single_transcript(
        self,
        transcript_item: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]],
        module: dspy.Module
    ) -> Dict[str, Any]:
        quotation = transcript_item.get('quotation', '').strip()
        keywords = transcript_item.get('keywords', [])
        if not quotation:
            logger.warning("Quotation missing. Skipping.")
            return {}
        if not keywords:
            logger.warning("Keywords missing. Skipping.")
            return {}

        logger.debug(f"Processing coding analysis for quotation: '{quotation[:100]}', Keywords: {keywords}")
        filtered_chunks = [chunk for chunk in retrieved_docs if chunk['score'] >= 0.7]
        contextualized_contents = [chunk['chunk']['contextualized_content'] for chunk in filtered_chunks]

        research_objectives = transcript_item.get('research_objectives', 'Perform coding analysis.')
        theoretical_framework = transcript_item.get('theoretical_framework', {})

        response = module.forward(
            research_objectives=research_objectives,
            quotation=quotation,
            keywords=keywords,
            contextualized_contents=contextualized_contents,
            theoretical_framework=theoretical_framework
        )

        result = {
            "coding_info": response.get("coding_info", {
                "quotation": quotation,
                "keywords": keywords,
                "research_objectives": research_objectives,
                "theoretical_framework": theoretical_framework
            }),
            "retrieved_chunks": retrieved_docs,
            "retrieved_chunks_count": len(retrieved_docs),
            "filtered_chunks_count": len(filtered_chunks),
            "contextualized_contents": contextualized_contents,
            "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in filtered_chunks],
            "codes": response.get("codes", []),
            "analysis": response.get("analysis", {})
        }

        if not result["codes"]:
            logger.warning("No codes developed.")
            result["analysis"]["error"] = "No codes were developed."

        logger.debug(f"Developed {len(result['codes'])} codes.")
        return result


class GroupingHandler(BaseHandler):
    @handle_exceptions
    async def process_single_transcript(
        self,
        transcript_item: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]],
        module: dspy.Module
    ) -> Dict[str, Any]:
        codes = transcript_item.get('codes', [])
        if not codes:
            logger.warning("No codes provided for grouping. Skipping.")
            return {}

        logger.debug(f"Processing grouping analysis for {len(codes)} codes.")
        filtered_chunks = [chunk for chunk in retrieved_docs if chunk['score'] >= 0.7]

        if os.path.exists(INFO_PATH):
            with open(INFO_PATH, 'r', encoding='utf-8') as f:
                info = json.load(f)
        else:
            info = {
                "research_objectives": "Group codes into themes.",
                "theoretical_framework": {}
            }

        research_objectives = info.get('research_objectives', 'Group codes into themes.')
        theoretical_framework = info.get('theoretical_framework', {})

        batch_size = 20
        batched_groupings = []
        for i in range(0, len(codes), batch_size):
            code_batch = codes[i:i+batch_size]
            response = module.forward(
                research_objectives=research_objectives,
                theoretical_framework=theoretical_framework,
                codes=code_batch
            )
            batch_groupings = response.get("groupings", [])
            batched_groupings.extend(batch_groupings)

        result = {
            "grouping_info": {
                "codes": codes,
                "research_objectives": research_objectives,
                "theoretical_framework": theoretical_framework
            },
            "retrieved_chunks": retrieved_docs,
            "retrieved_chunks_count": len(retrieved_docs),
            "filtered_chunks_count": len(filtered_chunks),
            "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in filtered_chunks],
            "groupings": batched_groupings
        }

        if not batched_groupings:
            logger.warning("No groupings formed.")
            result["error"] = "No groupings could be formed."

        logger.debug(f"Developed {len(batched_groupings)} groupings.")
        return result


class ThemeHandler(BaseHandler):
    @handle_exceptions
    async def process_single_transcript(
        self,
        transcript_item: Dict[str, Any],
        retrieved_docs: List[Dict[str, Any]],
        module: dspy.Module
    ) -> Dict[str, Any]:
        groupings = transcript_item.get('groupings', [])
        if not groupings:
            logger.warning("No groupings provided for theme development. Skipping.")
            return {}

        logger.debug(f"Processing theme development for {len(groupings)} groupings.")
        filtered_chunks = [chunk for chunk in retrieved_docs if chunk['score'] >= 0.7]
        contextualized_contents = [chunk['chunk']['contextualized_content'] for chunk in filtered_chunks]

        research_objectives = transcript_item.get('research_objectives', 'Develop themes from groupings.')
        theoretical_framework = transcript_item.get('theoretical_framework', {})

        quotation = transcript_item.get("quotation", "")
        keywords = transcript_item.get("keywords", [])
        codes = transcript_item.get("codes", [])
        transcript_chunk = transcript_item.get("transcript_chunk", "")

        response = module.forward(
            research_objectives=research_objectives,
            quotation=quotation,
            keywords=keywords,
            codes=codes,
            theoretical_framework=theoretical_framework,
            transcript_chunk=transcript_chunk
        )

        result = {
            "theme_info": response.get("theme_info", {
                "groupings": groupings,
                "research_objectives": research_objectives,
                "theoretical_framework": theoretical_framework
            }),
            "retrieved_chunks": retrieved_docs,
            "retrieved_chunks_count": len(retrieved_docs),
            "filtered_chunks_count": len(filtered_chunks),
            "contextualized_contents": contextualized_contents,
            "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in filtered_chunks],
            "themes": response.get("themes", []),
            "analysis": response.get("analysis", {})
        }

        if not result["themes"]:
            logger.warning("No themes developed.")
            if "analysis" in result and isinstance(result["analysis"], dict):
                result["analysis"]["error"] = "No themes were developed."
            else:
                result["analysis"] = {"error": "No themes were developed."}

        logger.debug(f"Developed {len(result['themes'])} themes.")
        return result


# File: processing/logger.py
#------------------------------------------------------------------------------
# processing/logger.py
import logging

def get_logger(name: str) -> logging.Logger:
    """
    Returns a logger configured with the given name.
    You can configure different handlers/formatters here as needed.
    """
    logger = logging.getLogger(name)

    # If desired, configure or extend the logger format/level here
    if not logger.handlers:
        logger.setLevel(logging.DEBUG)
        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG)
        formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    return logger


# File: processing/query_processor.py
#------------------------------------------------------------------------------
# src/processing/query_processor.py

import os
import json
from typing import List, Dict, Any
import asyncio

from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.core.retrieval.reranking import retrieve_with_reranking, RerankerConfig, RerankerType

# Import local modules
from .logger import get_logger
from .config import COHERE_API_KEY, ST_WEIGHT
from .validators import get_validator_for_module
from .factories import get_handler_for_module
from .handlers import BaseHandler
from .base import BaseValidator

import dspy  # DSPy library with built-in parallel support
from src.utils.logger import setup_logging
from src.decorators import handle_exceptions  # or you can define your own

setup_logging()
logger = get_logger(__name__)


def save_results(results: List[Dict[str, Any]], output_file: str):
    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as outfile:
            json.dump(results, outfile, indent=4)
        logger.info(f"Saved results to '{output_file}'")
    except Exception as e:
        logger.error(f"Error saving results to '{output_file}': {e}", exc_info=True)


@handle_exceptions
async def process_queries(
    transcripts: List[Dict[str, Any]],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    output_file: str,
    optimized_program: dspy.Program,  # Not fully used in this code, but left for consistency
    module: dspy.Module
):
    """
    Main function that processes transcripts according to the specified module.
    This version leverages DSPy's Parallel class to perform multi-threaded, parallel API calls.
    """

    logger.info(f"Processing transcripts for '{output_file}'.")

    # 1. Get a validator for the module and validate transcripts
    validator: BaseValidator = get_validator_for_module(module)
    logger.debug(f"Validator obtained: {type(validator).__name__} for module: {type(module).__name__}")
    valid_transcripts = validator.validate(transcripts)
    if not valid_transcripts:
        logger.warning("No valid transcripts found after validation. Exiting.")
        return

    # 2. Get a handler for the module
    handler: BaseHandler = get_handler_for_module(module)
    logger.debug(f"Handler obtained: {type(handler).__name__} for module: {type(module).__name__}")

    # 3. Prepare reranker configuration
    reranker_config = RerankerConfig(
        reranker_type=RerankerType.COHERE,
        cohere_api_key=COHERE_API_KEY,
        st_weight=ST_WEIGHT
    )

    # 4. Define a synchronous helper function to process one transcript.
    #    This function will be run in a separate thread.
    def process_transcript_sync(transcript_item: Dict[str, Any]):
        try:
            # For modules that require retrieval, we look up the query.
            # For grouping/theme modules, we skip retrieval.
            if type(handler).__name__ not in ["GroupingHandler", "ThemeHandler"]:
                query = transcript_item.get('query') or transcript_item.get('transcript_chunk') or transcript_item.get('quotation', '')
                if not query or not query.strip():
                    logger.warning("Transcript has no valid query. Skipping.")
                    return None

                retrieved_docs = retrieve_with_reranking(
                    query=query,
                    db=db,
                    es_bm25=es_bm25,
                    k=k,
                    reranker_config=reranker_config
                )
            else:
                retrieved_docs = []

            # Process the single transcript by calling the async function synchronously.
            # We use asyncio.run to create a new event loop in this thread.
            result = asyncio.run(handler.process_single_transcript(
                transcript_item=transcript_item,
                retrieved_docs=retrieved_docs,
                module=module
            ))
            return result
        except Exception as e:
            logger.error(f"Error processing a transcript: {e}", exc_info=True)
            return None

    # 5. Build execution pairs for DSPy's Parallel executor.
    #    Each pair is a tuple: (function, (transcript_item,))
    exec_pairs = [(process_transcript_sync, (transcript_item,)) for transcript_item in valid_transcripts]

    # 6. Create a Parallel executor instance with the desired number of threads.
    parallel_executor = dspy.Parallel(num_threads=8, max_errors=10, disable_progress_bar=False)

    # 7. Execute the processing in parallel.
    #    We wrap the blocking call using asyncio.to_thread so as not to block the event loop.
    results = await asyncio.to_thread(parallel_executor.forward, exec_pairs)

    # 8. Filter out None results (i.e. transcripts that failed or were skipped).
    all_results = [res for res in results if res is not None]

    # 9. Save results to the specified output file.
    save_results(all_results, output_file)


# File: processing/validators.py
#------------------------------------------------------------------------------
# src/processing/validators.py

from typing import List, Dict, Any
from .base import BaseValidator
from .logger import get_logger

import dspy

logger = get_logger(__name__)

# Import your actual modules so we can check their types
from src.analysis.select_quotation_module import SelectQuotationModule, EnhancedQuotationModule
from src.analysis.extract_keyword_module import KeywordExtractionModule
from src.analysis.coding_module import CodingAnalysisModule
from src.analysis.theme_development_module import ThemedevelopmentAnalysisModule
from src.analysis.grouping_module import GroupingAnalysisModule


class QuotationValidator(BaseValidator):
    """
    Validator for SelectQuotationModule and EnhancedQuotationModule.
    Checks for a valid 'transcript_chunk'.
    """

    def validate(self, transcripts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        valid_transcripts = []
        for idx, transcript in enumerate(transcripts):
            if 'transcript_chunk' not in transcript or not isinstance(transcript['transcript_chunk'], str) or not transcript['transcript_chunk'].strip():
                logger.warning(f"Transcript at index {idx} missing or invalid 'transcript_chunk'. Skipping.")
                continue
            valid_transcripts.append(transcript)
        logger.info(f"QuotationValidator: Validated {len(valid_transcripts)}/{len(transcripts)} transcripts.")
        return valid_transcripts


class KeywordValidator(BaseValidator):
    """
    Validator for KeywordExtractionModule.
    Checks for a valid 'quotation'.
    """

    def validate(self, transcripts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        valid_transcripts = []
        for idx, transcript in enumerate(transcripts):
            if 'quotation' not in transcript or not isinstance(transcript['quotation'], str) or not transcript['quotation'].strip():
                logger.warning(f"Transcript at index {idx} missing or invalid 'quotation'. Skipping.")
                continue
            valid_transcripts.append(transcript)
        logger.info(f"KeywordValidator: Validated {len(valid_transcripts)}/{len(transcripts)} transcripts.")
        return valid_transcripts


class CodingValidator(BaseValidator):
    """
    Validator for CodingAnalysisModule.
    Checks for a valid 'quotation' and valid 'keywords'.
    """

    def validate(self, transcripts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        valid_transcripts = []
        for idx, transcript in enumerate(transcripts):
            required_string_fields = ['quotation']
            required_list_fields = ['keywords']
            missing_fields = []

            for field in required_string_fields:
                if field not in transcript or not isinstance(transcript[field], str) or not transcript[field].strip():
                    missing_fields.append(field)

            for field in required_list_fields:
                if field not in transcript or not isinstance(transcript[field], list) \
                   or not all(isinstance(kw, str) and kw.strip() for kw in transcript[field]):
                    missing_fields.append(field)

            if missing_fields:
                logger.warning(f"Transcript at index {idx} missing required fields {missing_fields}. Skipping.")
                continue

            valid_transcripts.append(transcript)

        logger.info(f"CodingValidator: Validated {len(valid_transcripts)}/{len(transcripts)} transcripts.")
        return valid_transcripts


class ThemeValidator(BaseValidator):
    """
    Validator for ThemedevelopmentAnalysisModule.
    Checks for a valid 'quotation', 'keywords', and 'codes'.
    """

    def validate(self, transcripts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        valid_transcripts = []
        for idx, transcript in enumerate(transcripts):
            if 'quotation' not in transcript or not isinstance(transcript['quotation'], str) or not transcript['quotation'].strip():
                logger.warning(f"Transcript at index {idx} missing or invalid 'quotation'. Skipping.")
                continue
            if 'keywords' not in transcript or not isinstance(transcript['keywords'], list) or not transcript['keywords']:
                logger.warning(f"Transcript at index {idx} missing or invalid 'keywords'. Skipping.")
                continue
            if 'codes' not in transcript or not isinstance(transcript['codes'], list) or not transcript['codes']:
                logger.warning(f"Transcript at index {idx} missing or invalid 'codes'. Skipping.")
                continue
            valid_transcripts.append(transcript)

        logger.info(f"ThemeValidator: Validated {len(valid_transcripts)}/{len(transcripts)} transcripts.")
        return valid_transcripts


class GroupingValidator(BaseValidator):
    """
    Validator for GroupingAnalysisModule.
    Checks for valid 'codes'.
    """

    def validate(self, transcripts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        valid_transcripts = []
        for idx, transcript in enumerate(transcripts):
            if 'codes' not in transcript or not isinstance(transcript['codes'], list) or not transcript['codes']:
                logger.warning(f"Transcript at index {idx} missing or invalid 'codes' for grouping. Skipping.")
                continue
            valid_transcripts.append(transcript)

        logger.info(f"GroupingValidator: Validated {len(valid_transcripts)}/{len(transcripts)} transcripts.")
        return valid_transcripts


def get_validator_for_module(module: dspy.Module) -> BaseValidator:
    """
    Returns the appropriate validator instance for the given module.
    """
    if isinstance(module, (SelectQuotationModule, EnhancedQuotationModule)):
        return QuotationValidator()
    elif isinstance(module, KeywordExtractionModule):
        return KeywordValidator()
    elif isinstance(module, CodingAnalysisModule):
        return CodingValidator()
    elif isinstance(module, ThemedevelopmentAnalysisModule):
        return ThemeValidator()
    elif isinstance(module, GroupingAnalysisModule):
        return GroupingValidator()
    else:
        logger.warning("No specific validator found for given module. Returning a pass-through validator.")
        return PassThroughValidator()


class PassThroughValidator(BaseValidator):
    """
    A pass-through validator that doesn't filter out anything.
    Used as a fallback for unknown module types.
    """
    def validate(self, transcripts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        return transcripts



################################################################################
# Module: utils
################################################################################


# File: utils/__init__.py
#------------------------------------------------------------------------------
"""
Utility functions and helpers.
"""

from decorators import handle_exceptions
from .logger import setup_logging
from .utils import check_answer_length, compute_similarity
from .validation_functions import validate_relevance, validate_quality, validate_context_clarity

__all__ = [
    'handle_exceptions',
    'setup_logging',
    'check_answer_length',
    'compute_similarity',
    'validate_relevance',
    'validate_quality',
    'validate_context_clarity'
]

# File: utils/history_utils.py
#------------------------------------------------------------------------------
# Add this to src/utils/history_utils.py

import os
import json
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class HistoryManager:
    def __init__(self, base_dir='interaction_histories'):
        self.base_dir = base_dir
        self._ensure_directory()

    def _ensure_directory(self):
        """Ensures the history directory exists."""
        try:
            os.makedirs(self.base_dir, exist_ok=True)
            logger.info(f"Ensured history directory exists at: {self.base_dir}")
        except Exception as e:
            logger.error(f"Error creating history directory: {e}")

    def save_history(self, history, prefix='quotation'):
        """Saves a history entry with timestamp."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}_interaction_{timestamp}.json"
            filepath = os.path.join(self.base_dir, filename)

            history_data = {
                'timestamp': timestamp,
                'history': history
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2)
            
            logger.info(f"Saved interaction history to: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"Error saving history: {e}")
            return None

    def get_latest_history(self, prefix='quotation'):
        """Gets the most recent history file."""
        try:
            files = [f for f in os.listdir(self.base_dir) if f.startswith(prefix)]
            if not files:
                return None
            latest_file = max(files, key=lambda x: os.path.getctime(os.path.join(self.base_dir, x)))
            with open(os.path.join(self.base_dir, latest_file), 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error getting latest history: {e}")
            return None

# File: utils/logger.py
#------------------------------------------------------------------------------

# src/utils/logger.py:
# ------------------------------------------------------------------------------
import logging
import logging.config
import os
import yaml
from typing import Optional
from functools import wraps
import time

def setup_logging(
    default_path: str = 'config/logging_config.yaml',
    default_level: int = logging.INFO,
    env_key: str = 'LOG_CFG'
) -> None:
    """
    Setup logging configuration with enhanced error handling and directory creation.
    
    Args:
        default_path: Path to the logging configuration file
        default_level: Default logging level if config file is not found
        env_key: Environment variable that can be used to override the config path
    """
    try:
        path = os.getenv(env_key, default_path)
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            # Create log directories for all handlers
            handlers = config.get('handlers', {})
            for handler in handlers.values():
                if 'filename' in handler:
                    log_dir = os.path.dirname(handler['filename'])
                    if log_dir:
                        os.makedirs(log_dir, exist_ok=True)
            
            logging.config.dictConfig(config)
            logging.info(f"Logging configuration loaded from {path}")
        else:
            logging.basicConfig(level=default_level)
            logging.warning(f"Logging config file not found at {path}. Using basic config.")
    except Exception as e:
        logging.basicConfig(level=default_level)
        logging.error(f"Error in logging configuration: {str(e)}")

def get_logger(name: str) -> logging.Logger:
    """
    Get a logger with the specified name and add a null handler if no handlers exist.
    
    Args:
        name: Name for the logger
        
    Returns:
        logging.Logger: Configured logger instance
    """
    logger = logging.getLogger(name)
    
    # Add a null handler if no handlers exist
    if not logger.handlers:
        logger.addHandler(logging.NullHandler())
    
    return logger

def log_execution_time(logger: Optional[logging.Logger] = None):
    """
    Decorator to log function execution time.
    
    Args:
        logger: Logger instance to use. If None, a module-level logger is used.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            nonlocal logger
            if logger is None:
                logger = get_logger(func.__module__)
            
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                logger.debug(
                    f"Function '{func.__name__}' executed in {execution_time:.2f} seconds"
                )
                return result
            except Exception as e:
                execution_time = time.time() - start_time
                logger.error(
                    f"Function '{func.__name__}' failed after {execution_time:.2f} seconds. "
                    f"Error: {str(e)}",
                    exc_info=True
                )
                raise
        return wrapper
    return decorator


# File: utils/utils.py
#------------------------------------------------------------------------------
# src/utils/utils.py

import logging
from functools import wraps
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

def handle_assertion(func, *args, **kwargs):
    """
    Handles exceptions in assertion functions.
    Logs start, pass, or fail.
    """
    func_name = func.__name__
    start_time = time.time()
    logger.debug(f"Starting assertion check: '{func_name}'")
    try:
        func(*args, **kwargs)
        elapsed_time = time.time() - start_time
        logger.debug(f"Assertion in '{func_name}' passed successfully in {elapsed_time:.4f}s.")
    except AssertionError as ae:
        elapsed_time = time.time() - start_time
        logger.error(f"Assertion in '{func_name}' failed in {elapsed_time:.4f}s. Error: {ae}")
        raise  # Re-raise the exception
    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"Error during assertion in '{func_name}' in {elapsed_time:.4f}s: {e}", exc_info=True)
        raise

def check_answer_length(answer: str, max_length: int = 500) -> bool:
    """
    Checks if the answer length is within the specified limit.

    Args:
        answer (str): The generated answer to evaluate.
        max_length (int, optional): The maximum allowed length for the answer. Defaults to 500.

    Returns:
        bool: True if the answer length is within the limit, False otherwise.
    """
    return len(answer) <= max_length

def compute_similarity(query1: str, query2: str) -> float:
    """
    Computes cosine similarity between two queries using TF-IDF vectorization.

    Args:
        query1 (str): The first query string.
        query2 (str): The second query string.

    Returns:
        float: Cosine similarity score between query1 and query2.
    """
    try:
        vectorizer = TfidfVectorizer().fit_transform([query1, query2])
        vectors = vectorizer.toarray()
        similarity = cosine_similarity([vectors[0]], [vectors[1]])[0][0]
        return similarity
    except Exception as e:
        logger.error(f"Error computing similarity: {e}", exc_info=True)
        return 0.0


# File: utils/validation_functions.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/validation_functions.py

import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def validate_relevance(quotations: List[str], research_objectives: str) -> bool:
    """
    Validates that each quotation is relevant to the research objectives.
    """
    try:
        for quote in quotations:
            # Simple keyword matching; can be enhanced with NLP techniques
            if not any(obj.lower() in quote.lower() for obj in research_objectives.split()):
                logger.debug(f"Quotation '{quote}' is not relevant to the research objectives.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_relevance: {e}", exc_info=True)
        return False

def validate_quality(quotations: List[Dict[str, Any]]) -> bool:
    """
    Validates the quality and representation of each quotation.
    """
    try:
        for quote in quotations:
            if len(quote["quotation"].strip()) < 10:  # Example quality check
                logger.debug(f"Quotation '{quote['QUOTE']}' is too short to be considered high quality.")
                return False
            # Additional quality checks can be added here
        return True
    except Exception as e:
        logger.error(f"Error in validate_quality: {e}", exc_info=True)
        return False

def validate_context_clarity(quotations: List[str], context: str) -> bool:
    """
    Validates that each quotation is clear and has sufficient context.
    """
    try:
        for quote in quotations:
            if quote.lower() not in context.lower():
                logger.debug(f"Quotation '{quote}' lacks sufficient context.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_context_clarity: {e}", exc_info=True)
        return False

