# Consolidated Source Code
# ==============================================================================



################################################################################
# Module: root
################################################################################


# File: __init__.py
#------------------------------------------------------------------------------
"""
Core functionality for the thematic analysis package.
Contains database and client implementations.
"""

from .contextual_vector_db import ContextualVectorDB
from .elasticsearch_bm25 import ElasticsearchBM25
from .openai_client import OpenAIClient

__all__ = ['ContextualVectorDB', 'ElasticsearchBM25', 'OpenAIClient']

# File: contextual_vector_db.py
#------------------------------------------------------------------------------
"""
Module: contextual_vector_db

This module contains the ContextualVectorDB class which manages the creation,
storage, and search functionalities for a vector database enhanced with contextual
information. It leverages external APIs (OpenAI) and libraries (FAISS, DSPy) while
implementing robust error handling and detailed logging.
"""

import os
import pickle
import numpy as np
import threading
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from dotenv import load_dotenv
import logging
import faiss
import dspy
import time

from src.core.openai_client import OpenAIClient
from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)


def log_exception(logger_obj: logging.Logger, message: str, e: Exception, context: Dict[str, Any] = None) -> None:
    """
    Helper function for logging exceptions with additional context.
    
    Args:
        logger_obj (logging.Logger): Logger instance.
        message (str): Custom error message.
        e (Exception): The caught exception.
        context (Dict[str, Any], optional): Additional contextual information.
    """
    context_str = ", ".join(f"{k}={v}" for k, v in (context or {}).items())
    logger_obj.error(f"{message}. Context: {context_str}. Exception: {e}", exc_info=True)


class SituateContextSignature(dspy.Signature):
    doc = dspy.InputField(desc="Full document content")
    chunk = dspy.InputField(desc="Specific chunk content")
    reasoning = dspy.OutputField(desc="Chain of thought reasoning")
    contextualized_content = dspy.OutputField(desc="Contextualized content for the chunk")


class SituateContext(dspy.Module):
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(SituateContextSignature)
        logger.debug("SituateContext module initialized.")

    def forward(self, doc: str, chunk: str):
        prompt = f"""
                <document>
                {doc}
                </document>
            
                CHUNK_CONTEXT_PROMPT = 
                Here is the chunk we want to situate within the whole document
                <chunk>
                {chunk}
                </chunk>

                Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
                Answer only with the succinct context and nothing else.
        """
        logger.debug("Generating contextualized content for a chunk.")
        return self.chain(doc=doc, chunk=chunk, prompt=prompt)


class ContextualVectorDB:
    def __init__(self, name: str, openai_api_key: str = None):
        if openai_api_key is None:
            load_dotenv()
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                logger.critical("OPENAI_API_KEY is not set in environment variables.")
                raise ValueError("OPENAI_API_KEY is required but not set.")

        self.name = name
        self.embeddings = []
        self.metadata = []
        self.db_path = f"./data/{name}/contextual_vector_db.pkl"
        self.faiss_index_path = f"./data/{name}/faiss_index.bin"

        try:
            self.client = OpenAIClient(api_key=openai_api_key)
            logger.debug(f"Initialized OpenAIClient for ContextualVectorDB '{self.name}'.")
        except Exception as e:
            log_exception(logger, "Failed to initialize OpenAIClient", e, {"name": self.name})
            raise

        # Initialize FAISS index attribute
        self.index = None

    def situate_context(self, doc: str, chunk: str) -> Tuple[str, Any]:
        logger.debug(f"Entering situate_context with doc length={len(doc)} and chunk length={len(chunk)}.")
        try:
            if not hasattr(self, 'situate_context_module'):
                self.situate_context_module = SituateContext()
                logger.debug("Initialized SituateContext module.")

            start_time = time.time()
            response = self.situate_context_module(doc=doc, chunk=chunk)
            elapsed_time = time.time() - start_time
            logger.debug(f"Generated contextualized_content using DSPy in {elapsed_time:.2f} seconds.")

            contextualized_content = response.contextualized_content
            usage_metrics = {}  # Actual usage metrics can be populated here if available.
            return contextualized_content, usage_metrics
        except Exception as e:
            log_exception(logger, "Error during DSPy situate_context", e, {"doc_length": len(doc), "chunk_length": len(chunk)})
            return "", None

    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 8):
        logger.debug("Entering load_data method.")
        if self.embeddings and self.metadata and os.path.exists(self.faiss_index_path):
            logger.info("Vector database is already loaded. Skipping data loading.")
            return
        if os.path.exists(self.db_path) and os.path.exists(self.faiss_index_path):
            logger.info("Loading vector database and FAISS index from disk.")
            self.load_db()
            self.load_faiss_index()
            return

        texts_to_embed, metadata = self._process_dataset(dataset, parallel_threads)

        if not texts_to_embed:
            logger.warning("No texts to embed after processing the dataset.")
            return

        self._embed_and_store(texts_to_embed, metadata, max_workers=parallel_threads)
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}.")

    def _process_dataset(self, dataset: List[Dict[str, Any]], parallel_threads: int) -> Tuple[List[str], List[Dict[str, Any]]]:
        texts_to_embed = []
        metadata = []
        total_chunks = sum(len(doc.get('chunks', [])) for doc in dataset)
        logger.info(f"Total chunks to process: {total_chunks}.")

        logger.info(f"Processing {total_chunks} chunks with {parallel_threads} threads.")
        try:
            with ThreadPoolExecutor(max_workers=parallel_threads) as executor:
                futures = []
                for doc in dataset:
                    for chunk in doc.get('chunks', []):
                        futures.append(executor.submit(self._generate_contextualized_content, doc, chunk))

                for future in tqdm(as_completed(futures), total=total_chunks, desc="Processing chunks"):
                    result = future.result()
                    if result:
                        texts_to_embed.append(result['text_to_embed'])
                        metadata.append(result['metadata'])
            logger.debug("Completed processing all chunks.")
        except Exception as e:
            log_exception(logger, "Error during processing chunks", e)
            return [], []

        return texts_to_embed, metadata

    def _generate_contextualized_content(self, doc: Dict[str, Any], chunk: Any) -> Dict[str, Any]:
        try:
            if not isinstance(doc, dict):
                logger.error(f"Document is not a dictionary: {doc}")
                return None

            if isinstance(chunk, dict):
                chunk_id = chunk.get('chunk_id')
                if not chunk_id:
                    # Assign a unique chunk_id combining doc_id and chunk's index
                    chunk_index = chunk.get('index', 0)
                    chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_{chunk_index}"
                    chunk['chunk_id'] = chunk_id
                content = chunk.get('content', '')
                original_index = chunk.get('original_index', chunk.get('index', 0))
            elif isinstance(chunk, str):
                # Handle case where chunk is a string
                content = chunk
                chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_0"
                original_index = 0
                logger.warning("Chunk is a string. Expected a dict. Assigning default values.")
            else:
                logger.error(f"Unsupported chunk type: {type(chunk)}. Skipping chunk.")
                return None

            logger.debug(f"Processing chunk_id='{chunk_id}' in doc_id='{doc.get('doc_id', 'unknown_doc_id')}'.")

            contextualized_text, usage = self.situate_context(doc.get('content', ''), content)
            if not contextualized_text:
                logger.warning(f"Contextualized content is empty for chunk_id='{chunk_id}'.")
                return None

            return {
                'text_to_embed': f"{content}\n\n{contextualized_text}",
                'metadata': {
                    'doc_id': doc.get('doc_id', ''),
                    'original_uuid': doc.get('original_uuid', ''),
                    'chunk_id': chunk_id,
                    'original_index': original_index,
                    'original_content': content,
                    'contextualized_content': contextualized_text
                }
            }
        except Exception as e:
            log_exception(logger, "Error generating contextualized content", e, {"doc_id": doc.get('doc_id', 'unknown'), "chunk": chunk})
            return None

    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]], max_workers: int = 4):
        logger.debug("Entering _embed_and_store method.")
        batch_size = 128
        embeddings = []
        logger.info("Starting embedding generation.")

        def embed_batch(batch: List[str]) -> List[List[float]]:
            try:
                logger.debug(f"Generating embeddings for batch of size {len(batch)}.")
                response = self.client.create_embeddings(
                    model="text-embedding-3-small",
                    input=batch
                )
                return [item['embedding'] for item in response['data']]
            except Exception as e:
                log_exception(logger, "Error during OpenAI embeddings for batch", e, {"batch_size": len(batch)})
                return []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i: i + batch_size]
                futures.append(executor.submit(embed_batch, batch))

            for future in as_completed(futures):
                try:
                    embeddings_batch = future.result()
                    if embeddings_batch:
                        embeddings.extend(embeddings_batch)
                        logger.debug(f"Processed a batch with {len(embeddings_batch)} embeddings.")
                except Exception as e:
                    log_exception(logger, "Error retrieving embeddings from future", e)

        if not embeddings:
            logger.warning("No embeddings were generated.")
            return

        self.embeddings = embeddings
        self.metadata = data
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Embedding generation completed. Total embeddings: {len(self.embeddings)}.")

    def _build_faiss_index(self):
        logger.debug("Entering _build_faiss_index method.")
        start_time = time.time()
        try:
            self.create_faiss_index()
            self.save_faiss_index()
        except Exception as e:
            log_exception(logger, "Error building FAISS index", e)
            raise
        elapsed_time = time.time() - start_time
        logger.info(f"FAISS index built and saved in {elapsed_time:.2f} seconds.")

    def create_faiss_index(self):
        logger.debug("Entering create_faiss_index method.")
        try:
            if not self.embeddings:
                logger.error("No embeddings available to create FAISS index.")
                raise ValueError("Embeddings list is empty.")

            embedding_dim = len(self.embeddings[0])
            logger.info(f"Embedding dimension: {embedding_dim}.")
            embeddings_np = np.array(self.embeddings).astype('float32')
            faiss.normalize_L2(embeddings_np)
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.index.add(embeddings_np)
            logger.info(f"FAISS index created with {self.index.ntotal} vectors.")
        except Exception as e:
            log_exception(logger, "Error creating FAISS index", e)
            raise

    def save_faiss_index(self):
        logger.debug("Entering save_faiss_index method.")
        os.makedirs(os.path.dirname(self.faiss_index_path), exist_ok=True)
        try:
            faiss.write_index(self.index, self.faiss_index_path)
            logger.info(f"FAISS index saved to '{self.faiss_index_path}'.")
        except Exception as e:
            log_exception(logger, "Error saving FAISS index", e, {"faiss_index_path": self.faiss_index_path})

    def load_faiss_index(self):
        logger.debug("Entering load_faiss_index method.")
        if not os.path.exists(self.faiss_index_path):
            logger.error(f"FAISS index file not found at '{self.faiss_index_path}'.")
            raise ValueError("FAISS index file not found.")
        try:
            self.index = faiss.read_index(self.faiss_index_path)
            logger.info(f"FAISS index loaded from '{self.faiss_index_path}' with {self.index.ntotal} vectors.")
        except Exception as e:
            log_exception(logger, "Error loading FAISS index", e, {"faiss_index_path": self.faiss_index_path})
            raise

    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        logger.debug(f"Entering search method with query='{query}' and k={k}.")
        if not self.embeddings or not self.metadata:
            logger.error("Embeddings or metadata are not loaded. Cannot perform search.")
            return []
        if not hasattr(self, 'index') or self.index is None:
            logger.error("FAISS index is not loaded.")
            return []

        try:
            start_time = time.time()
            logger.debug("Generating embedding for the query.")
            response = self.client.create_embeddings(
                model="text-embedding-3-small",
                input=[query]
            )
            query_embedding = response['data'][0]['embedding']
            elapsed_time = time.time() - start_time
            logger.debug(f"Generated embedding for query in {elapsed_time:.2f} seconds.")
        except Exception as e:
            log_exception(logger, "Error generating embedding for query", e, {"query": query})
            return []

        query_embedding_np = np.array([query_embedding]).astype('float32')
        faiss.normalize_L2(query_embedding_np)

        logger.debug("Performing FAISS search.")
        try:
            start_time = time.time()
            distances, indices = self.index.search(query_embedding_np, k)
            elapsed_time = time.time() - start_time
            logger.debug(f"FAISS search completed in {elapsed_time:.2f} seconds.")
            indices = indices.flatten()
            distances = distances.flatten()
        except Exception as e:
            log_exception(logger, "Error during FAISS search", e, {"query": query, "k": k})
            return []

        top_results = []
        for idx, score in zip(indices, distances):
            if idx < len(self.metadata):
                meta = self.metadata[idx]
                result = {
                    "doc_id": meta['doc_id'],
                    "chunk_id": meta['chunk_id'],
                    "original_index": meta.get('original_index', 0),
                    "content": meta['original_content'],
                    "contextualized_content": meta.get('contextualized_content'),
                    "score": float(score),
                    "metadata": meta
                }
                top_results.append(result)
                logger.debug(f"Retrieved chunk_id='{meta['chunk_id']}' with score={score:.4f}.")
            else:
                logger.warning(f"Index {idx} out of bounds for metadata.")

        logger.info(f"FAISS search returned {len(top_results)} results for query: '{query}'.")
        logger.debug(f"Chunks retrieved: {[res['chunk_id'] for res in top_results]}.")
        return top_results

    def save_db(self):
        logger.debug("Entering save_db method.")
        data = {
            "metadata": self.metadata,
        }
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        try:
            with open(self.db_path, "wb") as file:
                pickle.dump(data, file)
            logger.info(f"Vector database metadata saved to '{self.db_path}'.")
        except Exception as e:
            log_exception(logger, "Error saving vector database metadata", e, {"db_path": self.db_path})

    def load_db(self):
        logger.debug("Entering load_db method.")
        if not os.path.exists(self.db_path):
            logger.error(f"Vector database file not found at '{self.db_path}'. Use load_data to create a new database.")
            raise ValueError("Vector database file not found.")
        try:
            with open(self.db_path, "rb") as file:
                data = pickle.load(file)
            self.metadata = data.get("metadata", [])
            logger.info(f"Vector database metadata loaded from '{self.db_path}' with {len(self.metadata)} entries.")
            logger.debug(f"Chunks loaded: {[meta['chunk_id'] for meta in self.metadata]}.")
        except Exception as e:
            log_exception(logger, "Error loading vector database metadata", e, {"db_path": self.db_path})
            raise


# File: elasticsearch_bm25.py
#------------------------------------------------------------------------------
# src/core/elasticsearch_bm25.py

import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk, scan
from elasticsearch.exceptions import NotFoundError, RequestError
import json
import os

from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)

class ElasticsearchBM25:
    
    def __init__(
        self, 
        index_name: str,
        es_host: str = "http://localhost:9200",
        logger: Optional[logging.Logger] = None
    ):
        self.logger = logger or logging.getLogger(__name__)
        self.index_name = index_name
        self.es_client = Elasticsearch(es_host)
        
        if not self.es_client.ping():
            raise ConnectionError(f"Failed to connect to Elasticsearch at {es_host}")
            
        self._create_index()
        
    def _create_index(self) -> None:
        index_settings = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "english"
                        },
                        "custom_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "porter_stem"]
                        }
                    }
                },
                "similarity": {
                    "custom_bm25": {
                        "type": "BM25",
                        "k1": 1.2,
                        "b": 0.75,
                    }
                },
                "index": {
                    "refresh_interval": "1s",
                    "number_of_shards": 1,
                    "number_of_replicas": 0
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "contextualized_content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "doc_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "original_index": {"type": "integer"},
                    "metadata": {"type": "object", "enabled": True}
                }
            }
        }
        
        index_data_dir = f"./data/{self.index_name}"
        os.makedirs(index_data_dir, exist_ok=True)
        self.logger.debug(f"Ensured existence of index data directory: {index_data_dir}")
        
        try:
            if self.es_client.indices.exists(index=self.index_name):
                self.logger.info(f"Index '{self.index_name}' already exists. Skipping creation.")
                # Avoid modifying immutable settings
            else:
                self.es_client.indices.create(
                    index=self.index_name,
                    body=index_settings
                )
                self.logger.info(f"Successfully created index: {self.index_name}")
        except Exception as e:
            self.logger.error(f"Failed to create/update index '{self.index_name}': {str(e)}")
            raise

    def index_documents(
        self,
        documents: List[Dict[str, Any]],
        batch_size: int = 500
    ) -> Tuple[int, List[Dict[str, Any]]]:
        if not documents:
            self.logger.warning("No documents provided for indexing")
            return 0, []

        failed_docs = []
        success_count = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            actions = [{
                "_index": self.index_name,
                "_source": {
                    "content": doc.get("original_content", ""),
                    "contextualized_content": doc.get("contextualized_content", ""),
                    "doc_id": doc.get("doc_id", ""),
                    "chunk_id": doc.get("chunk_id", ""),
                    "original_index": doc.get("original_index", 0),
                    "metadata": doc.get("metadata", {})
                }
            } for doc in batch]
            
            try:
                success, failed = bulk(
                    self.es_client,
                    actions,
                    raise_on_error=False,
                    raise_on_exception=False
                )
                success_count += success
                if failed:
                    for fail in failed:
                        failed_doc = batch[fail.get('index', 0)]
                        failed_docs.append(failed_doc)
                    self.logger.warning(f"Failed to index {len(failed)} documents in batch")
                    
            except Exception as e:
                self.logger.error(f"Batch indexing error: {str(e)}")
                failed_docs.extend(batch)
                
        self.es_client.indices.refresh(index=self.index_name)
        self.logger.info(f"Indexed {success_count}/{len(documents)} documents successfully")
        return success_count, failed_docs

    def search(
        self,
        query: str,
        k: int = 20,
        min_score: float = 0.1,
        fields: List[str] = None,
        operator: str = "or",
        minimum_should_match: str = "30%"
    ) -> List[Dict[str, Any]]:
        if not fields:
            fields = ["content^1", "contextualized_content^1.5"]
            
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "operator": operator,
                    "minimum_should_match": minimum_should_match,
                    "type": "best_fields",
                    "tie_breaker": 0.3
                }
            },
            "min_score": min_score,
            "size": k,
            "_source": True
        }

        try:
            search_body_json = json.dumps(search_body, indent=2)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")
        except Exception as e:
            self.logger.error(f"Error converting search body to JSON: {e}")
            search_body_json = str(search_body)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")

        try:
            response = self.es_client.search(
                index=self.index_name,
                body=search_body
            )
            
            hits = [{
                "doc_id": hit["_source"]["doc_id"],
                "chunk_id": hit["_source"]["chunk_id"],
                "content": hit["_source"]["content"],
                "contextualized_content": hit["_source"].get("contextualized_content"),
                "score": hit["_score"],
                "metadata": hit["_source"].get("metadata", {})
            } for hit in response["hits"]["hits"]]
            
            self.logger.debug(
                f"Search for '{query}' returned {len(hits)} results "
                f"(max_score: {response['hits'].get('max_score', 0)})"
            )
            return hits
            
        except Exception as e:
            self.logger.error(f"Search error: {str(e)}")
            raise

    def search_content(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["content^1"],
            operator="or",
            minimum_should_match="30%"
        )

    def search_contextualized(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["contextualized_content^1.5"],
            operator="or",
            minimum_should_match="30%"
        )


# File: openai_client.py
#------------------------------------------------------------------------------
import logging
import os
from typing import List, Dict, Any
from openai import OpenAI
# Initialize logger
logger = logging.getLogger(__name__)

class OpenAIClient:
    """
    Wrapper for OpenAI API interactions using the updated SDK.
    """
    def __init__(self, api_key: str):
        """
        Initializes the OpenAI client with the provided API key.
        
        Args:
            api_key (str): OpenAI API key.
        """
        if not api_key:
            logger.error("OpenAI API key is not provided.")
            raise ValueError("OpenAI API key must be provided.")
        self.client = OpenAI(api_key=api_key)
        logger.debug("OpenAI client initialized successfully.")

    def create_chat_completion(self, model: str, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> Dict[str, Any]:
        """
        Creates a chat completion using OpenAI's API.
        
        Args:
            model (str): Model name to use.
            messages (List[Dict[str, str]]): List of message dictionaries.
            max_tokens (int): Maximum number of tokens in the response.
            temperature (float): Sampling temperature.
        
        Returns:
            Dict[str, Any]: API response as a dictionary.
        """
        if not model or not messages:
            logger.error("Model and messages must be provided for chat completion.")
            raise ValueError("Model and messages must be provided for chat completion.")
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            logger.debug(f"Chat completion created successfully for model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating chat completion: {e}")
            raise

    def create_embeddings(self, model: str, input: List[str]) -> Dict[str, Any]:
        """
        Creates embeddings for the given input texts using OpenAI's API.
        
        Args:
            model (str): Embedding model to use.
            input (List[str]): List of input texts.
        
        Returns:
            Dict[str, Any]: API response containing embeddings.
        """
        if not model or not input:
            logger.error("Model and input must be provided for creating embeddings.")
            raise ValueError("Model and input must be provided for creating embeddings.")
        
        try:
            response = self.client.embeddings.create(
                model=model,
                input=input
            )
            logger.debug(f"Embeddings created successfully using model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            raise



################################################################################
# Module: retrieval
################################################################################


# File: retrieval/__init__.py
#------------------------------------------------------------------------------
"""
Retrieval functionality for searching and ranking content.
"""

from .query_generator import QueryGeneratorSignature
from .retrieval import hybrid_retrieval, multi_stage_retrieval
from .reranking import retrieve_with_reranking

__all__ = [
    'QueryGeneratorSignature',
    'hybrid_retrieval',
    'multi_stage_retrieval',
    'retrieve_with_reranking',
    'SentenceTransformerReRanker'
]

# File: retrieval/query_generator.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Example/src/query_generator.py
# query_generator.py
import dspy
import logging
from typing import Dict

from src.utils.logger import setup_logging

logger = logging.getLogger(__name__)

class QueryGeneratorSignature(dspy.Signature):
    question: str = dspy.InputField(desc="The original user question.")
    context: str = dspy.InputField(desc="The accumulated context from previous retrievals.")
    new_query: str = dspy.OutputField(desc="The generated query for the next retrieval hop.")

    def forward(self, question: str, context: str) -> Dict[str, str]:
        try:
            if not question or not context:
                raise ValueError("Both 'question' and 'context' must be provided and non-empty.")
            
            prompt = (
                f"Given the question: '{question}'\n"
                f"and the context retrieved so far:\n{context}\n"
                "Generate a search query that will help find additional information needed to answer the question."
            )
            new_query = self.language_model.generate(
                prompt=prompt,
                max_tokens=50,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=["\n"]
            ).strip()
            logger.info(f"Generated new query: '{new_query}'")
            return {"new_query": new_query}
        except ValueError as ve:
            logger.error(f"ValueError in QueryGeneratorSignature.forward: {ve}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question
        except Exception as e:
            logger.error(f"Error in QueryGeneratorSignature.forward: {e}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question


# File: retrieval/reranking.py
#------------------------------------------------------------------------------
# File: retrieval/reranking.py
import logging
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer, util
import torch
import time
import cohere
import os
from enum import Enum

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.core.retrieval.retrieval import hybrid_retrieval

# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class RerankerType(str, Enum):
    SENTENCE_TRANSFORMER = "sentence_transformer"
    COHERE = "cohere"
    COMBINED = "combined"

class RerankerConfig:
    def __init__(self,
                 reranker_type: RerankerType = RerankerType.SENTENCE_TRANSFORMER,
                 st_model_name: str = 'all-MiniLM-L6-v2',
                 device: Optional[str] = None,
                 cohere_api_key: Optional[str] = None,
                 st_weight: float = 0.5):
        self.reranker_type = reranker_type
        self.st_model_name = st_model_name
        self.device = device
        self.cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")
        self.st_weight = st_weight

class SentenceTransformerReRanker:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        self.model = SentenceTransformer(model_name)
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')

        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        if not query or not documents:
            logger.warning("Query and documents must be provided for re-ranking.")
            return []

        try:
            start_time = time.time()
            logger.info(f"Starting ST reranking for query: '{query[:100]}...' with {len(documents)} documents")
            
            logger.debug("Encoding query and documents with SentenceTransformer")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            elapsed_time = time.time() - start_time
            logger.info(f"ST reranking completed in {elapsed_time:.2f}s. Top score: {re_ranked_docs[0]['score']:.4f}, Bottom score: {re_ranked_docs[-1]['score']:.4f}")
            return re_ranked_docs
            
        except Exception as e:
            logger.error(f"Error during ST reranking: {e}", exc_info=True)
            return []

class CohereReRanker:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("COHERE_API_KEY")
        if not self.api_key:
            raise ValueError("Cohere API key must be provided or set in COHERE_API_KEY environment variable")
        self.client = cohere.Client(self.api_key)
        logger.info("CohereReRanker initialized successfully")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        if not query or not documents:
            logger.warning("Query and documents must be provided for Cohere re-ranking.")
            return []

        try:
            start_time = time.time()
            logger.info(f"Starting Cohere reranking for query: '{query[:100]}...' with {len(documents)} documents")
            
            logger.debug("Calling Cohere rerank API")
            response = self.client.rerank(
                model="rerank-english-v3.0",
                query=query,
                documents=documents,
                top_n=min(top_k, len(documents))
            )
            
            re_ranked_docs = []
            for result in response.results:
                re_ranked_docs.append({
                    "document": documents[result.index],
                    "score": result.relevance_score
                })
            
            elapsed_time = time.time() - start_time
            logger.info(f"Cohere reranking completed in {elapsed_time:.2f}s. Top score: {re_ranked_docs[0]['score']:.4f}, Bottom score: {re_ranked_docs[-1]['score']:.4f}")
            return re_ranked_docs
            
        except Exception as e:
            logger.error(f"Error during Cohere reranking: {e}", exc_info=True)
            return []

class CombinedReRanker:
    def __init__(self, 
                 st_model_name: str = 'all-MiniLM-L6-v2',
                 device: Optional[str] = None,
                 cohere_api_key: Optional[str] = None,
                 st_weight: float = 0.5):
        self.st_reranker = SentenceTransformerReRanker(st_model_name, device)
        self.cohere_reranker = CohereReRanker(cohere_api_key)
        self.st_weight = st_weight
        logger.info(f"CombinedReRanker initialized with ST weight: {st_weight}")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        try:
            start_time = time.time()
            logger.info(f"Starting combined reranking for query: '{query[:100]}...' with {len(documents)} documents")
            
            logger.debug("Getting ST rankings")
            st_results = self.st_reranker.rerank(query, documents, top_k=len(documents))
            logger.debug("Getting Cohere rankings")
            cohere_results = self.cohere_reranker.rerank(query, documents, top_k=len(documents))
            
            logger.debug("Creating score maps and combining results")
            st_scores = {doc['document']: doc['score'] for doc in st_results}
            cohere_scores = {doc['document']: doc['score'] for doc in cohere_results}
            
            combined_scores = {}
            for doc in documents:
                st_score = st_scores.get(doc, 0.0)
                cohere_score = cohere_scores.get(doc, 0.0)
                combined_score = (st_score * self.st_weight) + (cohere_score * (1 - self.st_weight))
                combined_scores[doc] = combined_score
                logger.debug(f"Document scores - ST: {st_score:.4f}, Cohere: {cohere_score:.4f}, Combined: {combined_score:.4f}")
            
            sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
            final_results = [{"document": doc, "score": score} for doc, score in sorted_docs]
            
            elapsed_time = time.time() - start_time
            logger.info(f"Combined reranking completed in {elapsed_time:.2f}s. Top score: {final_results[0]['score']:.4f}, Bottom score: {final_results[-1]['score']:.4f}")
            return final_results
            
        except Exception as e:
            logger.error(f"Error in combined reranking: {e}", exc_info=True)
            logger.debug("Falling back to ST reranker")
            return self.st_reranker.rerank(query, documents, top_k=top_k)

class RerankerFactory:
    @staticmethod
    def create_reranker(config: RerankerConfig):
        logger.debug(f"Creating reranker of type: {config.reranker_type}")
        if config.reranker_type == RerankerType.SENTENCE_TRANSFORMER:
            return SentenceTransformerReRanker(model_name=config.st_model_name, device=config.device)
        elif config.reranker_type == RerankerType.COHERE:
            return CohereReRanker(api_key=config.cohere_api_key)
        elif config.reranker_type == RerankerType.COMBINED:
            return CombinedReRanker(
                st_model_name=config.st_model_name,
                device=config.device,
                cohere_api_key=config.cohere_api_key,
                st_weight=config.st_weight
            )
        else:
            error_msg = f"Unknown reranker type: {config.reranker_type}"
            logger.error(error_msg)
            raise ValueError(error_msg)

def retrieve_with_reranking(query: str,
                          db: ContextualVectorDB,
                          es_bm25: ElasticsearchBM25,
                          k: int,
                          reranker_config: Optional[RerankerConfig] = None) -> List[Dict[str, Any]]:
    start_time = time.time()
    logger.info(f"Starting retrieval and reranking for query: '{query[:100]}...'")

    if reranker_config is None:
        logger.debug("No reranker config provided, defaulting to SentenceTransformer")
        reranker_config = RerankerConfig(reranker_type=RerankerType.SENTENCE_TRANSFORMER)
    
    logger.debug(f"Using reranker type: {reranker_config.reranker_type}")

    try:
        logger.debug(f"Performing initial hybrid retrieval with k={k*10}")
        initial_results = hybrid_retrieval(query, db, es_bm25, k=k*10)
        logger.info(f"Initial retrieval returned {len(initial_results)} results in {time.time() - start_time:.2f}s")

        if not initial_results:
            logger.warning(f"No initial results retrieved for query '{query[:100]}...'. Skipping reranking.")
            return []

        documents = [doc['chunk']['original_content'] for doc in initial_results]
        
        logger.debug(f"Creating {reranker_config.reranker_type} reranker")
        reranker = RerankerFactory.create_reranker(reranker_config)
        
        logger.debug(f"Performing reranking with k={k}")
        re_ranked = reranker.rerank(query, documents, top_k=k)

        final_results = []
        for r in re_ranked:
            doc_index = documents.index(r['document'])
            final_results.append({
                "chunk": initial_results[doc_index]['chunk'],
                "score": r['score']
            })

        elapsed_time = time.time() - start_time
        logger.info(f"Retrieval and reranking completed in {elapsed_time:.2f}s. Returned {len(final_results)} results")
        if final_results:
            logger.debug(f"Top score: {final_results[0]['score']:.4f}, Bottom score: {final_results[-1]['score']:.4f}")
        return final_results

    except Exception as e:
        logger.error(f"Error during retrieval/reranking for query '{query[:100]}...': {e}", exc_info=True)
        return []


# File: retrieval/retrieval.py
#------------------------------------------------------------------------------
# File: retrieval.py
import logging
import time
from typing import List, Dict, Any
import dspy

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.core.retrieval.query_generator import QueryGeneratorSignature
from src.utils.utils import compute_similarity
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)


def hybrid_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    semantic_weight: float = 0.2,
    bm25_content_weight: float = 0.2,
    bm25_contextual_weight: float = 0.6,
    min_chunks: int = 1
) -> List[Dict[str, Any]]:
    """
    Performs hybrid retrieval by combining FAISS semantic search and dual BM25 contextual search using Reciprocal Rank Fusion.
    """
    logger.debug(
        f"Entering hybrid_retrieval with query='{query}', k={k}, "
        f"semantic_weight={semantic_weight}, bm25_content_weight={bm25_content_weight}, "
        f"bm25_contextual_weight={bm25_contextual_weight}, min_chunks={min_chunks}."
    )
    start_time = time.time()
    num_chunks_to_recall = k * 10  # Retrieve more to improve chances

    # Initialize reciprocal rank fusion score dictionary
    chunk_id_to_score = {}

    while True:
        # Semantic search
        logger.debug(f"Performing semantic search using FAISS for query: '{query}'")
        semantic_results = db.search(query, k=num_chunks_to_recall)
        ranked_semantic = [result['chunk_id'] for result in semantic_results]
        semantic_scores = {result['chunk_id']: result['score'] for result in semantic_results}
        logger.debug(f"Semantic search retrieved {len(ranked_semantic)} chunk IDs.")

        # BM25 search on 'content'
        logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        bm25_content_results = es_bm25.search_content(query, k=num_chunks_to_recall)
        ranked_bm25_content = [result['chunk_id'] for result in bm25_content_results]
        bm25_content_scores = {result['chunk_id']: result['score'] for result in bm25_content_results}
        logger.debug(f"BM25 'content' search retrieved {len(ranked_bm25_content)} chunk IDs.")

        # BM25 search on 'contextualized_content'
        logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        bm25_contextual_results = es_bm25.search_contextualized(query, k=num_chunks_to_recall)
        ranked_bm25_contextual = [result['chunk_id'] for result in bm25_contextual_results]
        bm25_contextual_scores = {result['chunk_id']: result['score'] for result in bm25_contextual_results}
        logger.debug(f"BM25 'contextualized_content' search retrieved {len(ranked_bm25_contextual)} chunk IDs.")

        # Combine all unique chunk IDs
        chunk_ids = list(set(ranked_semantic + ranked_bm25_content + ranked_bm25_contextual))
        logger.debug(f"Total unique chunk IDs after combining: {len(chunk_ids)}")

        # Calculate Reciprocal Rank Fusion scores
        for chunk_id in chunk_ids:
            score = 0
            if chunk_id in ranked_semantic:
                index = ranked_semantic.index(chunk_id)
                score += semantic_weight * (1 / (index + 1))
                logger.debug(
                    f"Added semantic RRF score for chunk_id {chunk_id}: "
                    f"{semantic_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_content:
                index = ranked_bm25_content.index(chunk_id)
                score += bm25_content_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_content_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_contextual:
                index = ranked_bm25_contextual.index(chunk_id)
                score += bm25_contextual_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'contextualized_content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_contextual_weight * (1 / (index + 1))}"
                )
            chunk_id_to_score[chunk_id] = score

        # Sort chunk IDs by their RRF scores in descending order
        sorted_chunk_ids = sorted(
            chunk_id_to_score.keys(),
            key=lambda x: chunk_id_to_score[x],
            reverse=True
        )
        logger.debug(f"Sorted chunk IDs based on RRF scores.")

        # Select top k chunks, ensuring at least min_chunks are returned
        final_results = []
        filtered_count = 0
        for chunk_id in sorted_chunk_ids[:k]:
            chunk_metadata = next(
                (chunk for chunk in db.metadata if chunk['chunk_id'] == chunk_id),
                None
            )
            if not chunk_metadata:
                filtered_count += 1
                logger.warning(f"Chunk metadata not found for chunk_id {chunk_id}")
                continue
            final_results.append({
                'chunk': chunk_metadata,
                'score': chunk_id_to_score[chunk_id]
            })

        logger.info(f"Filtered {filtered_count} chunks due to missing metadata.")
        logger.info(
            f"Total chunks retrieved after filtering: {len(final_results)} "
            f"(required min_chunks={min_chunks})"
        )

        if len(final_results) >= min_chunks or k >= num_chunks_to_recall:
            break
        else:
            k += 5  # Increment k to retrieve more chunks
            logger.info(
                f"Number of retrieved chunks ({len(final_results)}) is less than min_chunks ({min_chunks}). "
                f"Increasing k to {k} and retrying retrieval."
            )

    logger.debug(f"Hybrid retrieval returning {len(final_results)} chunks.")
    logger.info(
        f"Chunks used for hybrid retrieval for query '{query}': "
        f"[{', '.join([res['chunk']['chunk_id'] for res in final_results])}]"
    )
    end_time = time.time()
    logger.debug(f"Exiting hybrid_retrieval method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results


def multi_stage_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    max_hops: int = 3,
    max_results: int = 5,
    similarity_threshold: float = 0.9
) -> List[Dict[str, Any]]:
    accumulated_context = ""
    all_retrieved_chunks = {}
    current_query = query
    previous_query = ""
    query_generator = dspy.TypedChainOfThought(QueryGeneratorSignature)

    for hop in range(max_hops):
        logger.info(f"Starting hop {hop+1} with query: '{current_query}'")

        retrieved_chunks = hybrid_retrieval(current_query, db, es_bm25, k)

        # Update accumulated retrieved chunks
        for chunk in retrieved_chunks:
            chunk_id = chunk['chunk']['chunk_id']
            if chunk_id not in all_retrieved_chunks:
                all_retrieved_chunks[chunk_id] = chunk

        # Check if we have reached the desired number of results
        if len(all_retrieved_chunks) >= max_results:
            logger.info(f"Retrieved sufficient chunks ({len(all_retrieved_chunks)}). Terminating.")
            break

        # Accumulate new context from retrieved chunks
        new_context = "\n\n".join([
            chunk['chunk'].get('contextualized_content', '') or chunk['chunk'].get('original_content', '')
            for chunk in retrieved_chunks
        ])
        accumulated_context += "\n\n" + new_context

        # Generate a new query based on the accumulated context
        response = query_generator(question=query, context=accumulated_context)
        new_query = response.get('new_query', '').strip()
        if not new_query:
            logger.info("No new query generated. Terminating multi-stage retrieval.")
            break

        # Compute similarity between the new query and the previous query
        similarity = compute_similarity(current_query, new_query)
        logger.debug(f"Similarity between queries: {similarity:.4f}")
        if similarity >= similarity_threshold:
            logger.info("New query is too similar to the current query. Terminating multi-stage retrieval.")
            break

        # Update queries for the next iteration
        previous_query = current_query
        current_query = new_query

    # Sort and return the top results
    final_results = sorted(
        all_retrieved_chunks.values(),
        key=lambda x: x['score'],
        reverse=True
    )[:max_results]

    logger.info(f"Multi-stage retrieval completed with {len(final_results)} chunks.")
    return final_results

