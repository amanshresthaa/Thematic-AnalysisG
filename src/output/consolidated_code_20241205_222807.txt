# Consolidated Source Code
# ==============================================================================



################################################################################
# Module: root
################################################################################


# File: __init__.py
#------------------------------------------------------------------------------
"""
Thematic Analysis Package
A package for analyzing textual data using thematic analysis techniques.
"""

__version__ = '0.1.0'


################################################################################
# Module: analysis
################################################################################


# File: analysis/__init__.py
#------------------------------------------------------------------------------
#init
"""
Analysis modules for thematic analysis.
Contains metrics and quotation selection functionality.
"""

from .metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from .select_quotation import EnhancedQuotationSignature
from .select_quotation_module import SelectQuotationModule

__all__ = [
    'comprehensive_metric',
    'is_answer_fully_correct',
    'factuality_metric',
    'EnhancedQuotationSignature',
    'SelectQuotationModule'
]

# File: analysis/coding.py
#------------------------------------------------------------------------------
import logging
from typing import Dict, Any, List
import dspy
from dataclasses import dataclass
import json
from src.assertions_coding import (
    assert_robustness,
    assert_reflectiveness,
    assert_resplendence,
    assert_relevance,
    assert_radicality,
    assert_righteousness,
    assert_code_representation,
    assert_code_specificity,
    assert_code_relevance,
    assert_code_distinctiveness,
    run_all_coding_assertions
)

logger = logging.getLogger(__name__)

@dataclass
class SixRsEvaluation:
    """Evaluation metrics for each dimension of the 6Rs framework."""
    robust: str
    reflective: str
    resplendent: str
    relevant: str
    radical: str
    righteous: str

class CodingAnalysisSignature(dspy.Signature):
    """
    Signature for conducting a comprehensive thematic coding analysis utilizing the 6Rs framework.
    
    This signature facilitates the systematic analysis of qualitative data by guiding the user
    through the process of thematic coding, ensuring methodological rigor and theoretical alignment.
    """

    # Input Fields
    research_objectives: str = dspy.InputField(
        desc=(
            "A detailed statement of the study's overarching goals and specific research questions "
            "that direct the coding analysis. This should clearly articulate what the research aims to "
            "accomplish and the key questions it seeks to address, providing a foundation for the "
            "subsequent coding process."
        )
    )

    quotation: str = dspy.InputField(
        desc=(
            "The specific excerpt, passage, or segment selected from the data set for coding analysis. "
            "This quotation serves as the primary source text from which themes and codes will be derived, "
            "capturing the essential content to be analyzed."
        )
    )

    keywords: List[str] = dspy.InputField(
        desc=(
            "A curated list of keywords previously identified to inform and guide the coding process. "
            "Each keyword should be a distinct string that represents a significant theme or concept "
            "relevant to the research objectives, aiding in the systematic identification of patterns "
            "within the quotation."
        )
    )

    contextualized_contents: List[str] = dspy.InputField(
        desc=(
            "Additional contextual information that provides background and deeper insight into the "
            "primary quotation. This may include related texts, situational context, historical background, "
            "or any other supplementary content that enhances the understanding and interpretation of the "
            "quotation being analyzed."
        )
    )

    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc=(
            "The foundational theoretical framework that underpins the analysis, detailing the guiding "
            "theoretical approach. This dictionary should include:\n"
            " - **theory**: The primary theoretical perspective or model being applied to the analysis.\n"
            " - **philosophical_approach**: The underlying philosophical stance that informs the theoretical "
            "framework, such as positivism, interpretivism, critical theory, etc.\n"
            " - **rationale**: A comprehensive justification for selecting this particular theoretical approach, "
            "explaining how it aligns with and supports the research objectives and questions."
        )
    )

    # Output Fields
    coding_info: Dict[str, Any] = dspy.OutputField(
        desc=(
            "Comprehensive context and metadata related to the coding analysis, including:\n"
            " - **quotation**: The original passage selected for analysis.\n"
            " - **research_objectives**: The specific goals and research questions that guide the analysis.\n"
            " - **theoretical_framework**: Detailed information about the theoretical foundation supporting the analysis.\n"
            " - **keywords**: The list of keywords extracted or utilized from the quotation to inform coding."
        )
    )

    codes: List[Dict[str, Any]] = dspy.OutputField(
        desc=(
            "A structured collection of developed codes, each accompanied by an in-depth analysis. Each code entry includes:\n"
            " - **code**: The name or label of the developed code.\n"
            " - **definition**: A precise and clear explanation of the code's meaning and scope.\n"
            " - **6Rs_framework**: The specific dimensions of the 6Rs framework (robust, reflective, resplendent, relevant, radical, righteous) that the code satisfies. Only the dimensions that apply are listed (e.g., ['robust', 'relevant']).\n"
            " - **6Rs_evaluation**: Detailed justifications and evaluation metrics for the Rs listed in **6Rs_framework**, explaining how the code satisfies those dimensions. Evaluations focus only on these specific Rs, including:\n"
            "     * **robust**: Assessment of how effectively the code captures the fundamental essence of the data.\n"
            "     * **reflective**: Evaluation of the code's alignment and relationship with the theoretical framework.\n"
            "     * **resplendent**: Analysis of the code's comprehensiveness and the depth of understanding it provides.\n"
            "     * **relevant**: Determination of the code's appropriateness and contextual fit within the data.\n"
            "     * **radical**: Identification of the code's uniqueness and the novelty of the insights it offers.\n"
            "     * **righteous**: Verification of the code's logical consistency and alignment with the overarching theoretical framework."
        )
    )


    analysis: Dict[str, Any] = dspy.OutputField(
        desc=(
            "An extensive analysis of the coding process, encompassing:\n"
            " - **theoretical_integration**: An explanation of how the developed codes integrate with and apply the theoretical framework.\n"
            " - **methodological_reflection**: Critical reflections on the coding methodology, including:\n"
            "     * **code_robustness**: An evaluation of the strength, reliability, and consistency of the codes.\n"
            "     * **theoretical_alignment**: Analysis of the extent to which the codes align with the theoretical framework.\n"
            "     * **researcher_reflexivity**: Consideration of the researcher's own influence, biases, and assumptions in the coding process.\n"
            " - **practical_implications**: Insights derived from the coding analysis and their potential applications or implications for practice, policy, or further research."
        )
    )

    def create_prompt(self, research_objectives: str, quotation: str,
                     keywords: List[str], contextualized_contents: List[str],
                     theoretical_framework: Dict[str, str]) -> str:
        """Generates a detailed prompt for conducting enhanced coding analysis."""

        # Format keywords for clarity
        keywords_formatted = "\n".join([
            f"- **{kw}**"
            for kw in keywords
        ])

        # Format contextualized contents with clear labeling
        contents_formatted = "\n\n".join([
            f"**Content {i+1}:**\n{content}"
            for i, content in enumerate([quotation] + contextualized_contents)
        ])

        # Extract theoretical framework components with default empty strings
        theory = theoretical_framework.get("theory", "N/A")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "N/A")
        rationale = theoretical_framework.get("rationale", "N/A")

        # Construct the prompt with clear sections and instructions
        prompt = (
            f"You are an experienced qualitative researcher specializing in thematic coding analysis "
            f"utilizing the 6Rs framework and grounded in {theory}. Your objective is to develop and critically analyze codes based on "
            f"the provided keywords and quotation, ensuring methodological rigor and theoretical alignment.\n\n"

            f"**Quotation for Analysis:**\n{quotation}\n\n"

            f"**Identified Keywords:**\n{keywords_formatted}\n\n"

            f"**Additional Contextualized Contents:**\n{contents_formatted}\n\n"

            f"**Research Objectives:**\n{research_objectives}\n\n"

            f"**Theoretical Framework:**\n"
            f"- **Theory:** {theory}\n"
            f"- **Philosophical Approach:** {philosophical_approach}\n"
            f"- **Rationale:** {rationale}\n\n"

            f"**Guidelines for Analysis:**\n"
            f"Your analysis should adhere to the 6Rs framework, addressing each dimension as follows:\n"
            f"1. **Robust:** Ensure that the code captures the true essence of the data in a theoretically sound manner.\n"
            f"2. **Reflective:** Demonstrate clear relationships between the data and the theoretical framework.\n"
            f"3. **Resplendent:** Provide a comprehensive understanding that encompasses all relevant aspects.\n"
            f"4. **Relevant:** Accurately represent the data, ensuring appropriateness and contextual fit.\n"
            f"5. **Radical:** Introduce unique and innovative insights that advance understanding.\n"
            f"6. **Righteous:** Maintain logical alignment with the overarching theoretical framework.\n\n"

            f"**Example Code Development:**\n"
            f"- **Code:** Economic Vulnerability\n"
            f"  - **Definition:** Victims originate from economically disadvantaged backgrounds, lacking financial stability.\n"
            f"  - **Keywords:** Poverty, Lack of Education\n"
            f"  - **6Rs Evaluation:** Robust, Relevant\n"
            f"  - **Theoretical Alignment:** Connects economic factors with victim vulnerability as per {theory}.\n"
            f"  - **Supporting Quotes:** [\"Victims of sex trafficking often come from vulnerable backgrounds, such as poverty...\"]\n"
            f"  - **Analytical Memos:** Economic hardship is a primary factor that increases susceptibility to trafficking offers.\n\n"

            f"**Instructions:**\n"
            f"- Each code should include its definition, associated keywords, 6Rs evaluation, theoretical alignment, supporting quotes, and analytical memos.\n"
            f"- Ensure that the codes are directly related to the theoretical framework and research objectives.\n"
            f"- Use the identified keywords as foundational themes for each code.\n"
            f"- Present the response in JSON format encapsulated within ```json``` blocks."
        )
        return prompt

    def parse_response(self, response: str) -> Dict[str, Any]:
        """Extracts and parses the JSON content from the language model's response."""
        try:
            import re
            # Use regex to find JSON content within code blocks
            json_match = re.search(r"```json\s*(\{.*?\})\s*```", response, re.DOTALL)
            if not json_match:
                logger.error("No valid JSON found in the response.")
                logger.debug(f"Full response received: {response}")
                return {}
            json_string = json_match.group(1)

            # Parse the JSON string into a Python dictionary
            response_json = json.loads(json_string)
            return response_json
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}. Response: {response}")
            return {}
        except Exception as e:
            logger.error(f"Unexpected error during response parsing: {e}")
            return {}

    def validate_codes(self, codes: List[Dict[str, Any]], research_objectives: str,
                      theoretical_framework: Dict[str, str]) -> None:
        """
        Validates the developed codes against the 6Rs framework and other assertions.
        
        This method ensures that each code meets the defined quality standards and aligns
        with the research objectives and theoretical framework.

        Args:
            codes (List[Dict[str, Any]]): The list of developed codes with their metadata.
            research_objectives (str): The research goals and questions.
            theoretical_framework (Dict[str, str]): Details of the theoretical foundation.

        Raises:
            AssertionError: If any code fails to meet the validation criteria.
        """
        try:
            run_all_coding_assertions(
                codes=codes,
                research_objectives=research_objectives,
                theoretical_framework=theoretical_framework
            )
            logger.debug("All coding assertions passed successfully.")
        except AssertionError as ae:
            logger.error(f"Code validation failed: {ae}")
            # Additional logging to identify problematic codes
            for code in codes:
                try:
                    assert_code_relevance(code, research_objectives, theoretical_framework)
                    # Add other individual assertions as needed
                except AssertionError as individual_ae:
                    logger.error(f"Validation failed for code '{code.get('code', 'Unknown')}': {individual_ae}")
            raise

    def forward(self, research_objectives: str, quotation: str,
                keywords: List[str], contextualized_contents: List[str],
                theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """Executes the coding analysis with retry mechanism."""
        for attempt in range(3):
            try:
                logger.debug(f"Attempt {attempt + 1} - Initiating coding analysis.")

                # Generate the prompt for the language model
                prompt = self.create_prompt(
                    research_objectives,
                    quotation,
                    keywords,
                    contextualized_contents,
                    theoretical_framework
                )

                # Interact with the language model to generate a response
                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=3000,
                    temperature=0.5  # Adjusted for greater consistency
                ).strip()

                logger.debug(f"Attempt {attempt + 1} - Response received from language model.")

                # Parse the JSON response
                parsed_response = self.parse_response(response)

                if not parsed_response:
                    raise ValueError("Parsed response is empty or invalid JSON.")

                # Extract codes and analysis from the parsed response
                codes = parsed_response.get("codes", [])
                analysis = parsed_response.get("analysis", {})

                if not codes:
                    raise ValueError("No codes were generated. Please check the prompt and input data.")

                # Validate the developed codes
                self.validate_codes(
                    codes=codes,
                    research_objectives=research_objectives,
                    theoretical_framework=theoretical_framework
                )

                logger.info(f"Attempt {attempt + 1} - Successfully developed and validated {len(codes)} codes.")
                return parsed_response

            except AssertionError as ae:
                logger.warning(f"Attempt {attempt + 1} - Assertion failed during coding analysis: {ae}")
                logger.debug(f"Attempt {attempt + 1} - Response causing assertion failure: {response}")
            except ValueError as ve:
                logger.warning(f"Attempt {attempt + 1} - ValueError: {ve}")
                logger.debug(f"Attempt {attempt + 1} - Response causing ValueError: {response}")
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} - Error in CodingAnalysisSignature.forward: {e}", exc_info=True)

        logger.error(f"Failed to develop valid codes after 3 attempts. Last response: {response}")
        # Optionally, provide a summary of issues or next steps
        return {
            "error": "Failed to develop valid codes after 3 attempts. Please review the input data and prompt for possible improvements."
        }


# File: analysis/coding_module.py
#------------------------------------------------------------------------------
#analysis/coding_module.py
import logging
from typing import Dict, Any, List
import dspy
from analysis.coding import CodingAnalysisSignature

logger = logging.getLogger('my_logger')

class CodingAnalysisModule(dspy.Module):
    """
    DSPy module for developing and analyzing codes using the 6Rs framework,
    building upon extracted keywords.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(CodingAnalysisSignature)

    def forward(self, research_objectives: str, quotation: str,
                keywords: List[str], contextualized_contents: List[str],
                theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """
        Execute coding analysis with the 6Rs framework, building upon extracted keywords.

        Args:
            research_objectives (str): Research goals and questions
            quotation (str): Selected quotation for analysis
            keywords (List[Dict[str, Any]]): Previously extracted keywords
            contextualized_contents (List[str]): Additional context
            theoretical_framework (Dict[str, str]): Theoretical foundation

        Returns:
            Dict[str, Any]: Complete coding analysis results including codes and analysis
        """
        logger.info("Starting coding analysis.")
        logger.debug(f"Input parameters: Research Objectives='{research_objectives[:100]}', "
                     f"Quotation='{quotation[:100]}', Keywords={keywords}, "
                     f"Contextualized Contents={contextualized_contents[:2]}, "
                     f"Theoretical Framework={theoretical_framework}")

        try:
            response = self.chain(
                research_objectives=research_objectives,
                quotation=quotation,
                keywords=keywords,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            logger.info("Successfully completed coding analysis.")
            logger.debug(f"Response: {response}")

            if not response.get("codes"):
                logger.warning("No codes were generated. Possible issue with inputs or prompt formulation.")
            
            return response
        except Exception as e:
            logger.error(f"Error during coding analysis: {e}", exc_info=True)
            return {}


# File: analysis/extract_keyword.py
#------------------------------------------------------------------------------
# src/analysis/extract_keyword.py

import logging
from typing import Dict, Any, List
import dspy
from dataclasses import dataclass
import json
from src.assertions_keyword import (
    assert_keywords_not_exclusive_to_quotation,
    assert_keyword_specificity,
    assert_keyword_distinctiveness,
    assert_keyword_relevance
)

logger = logging.getLogger(__name__)

@dataclass
class KeywordAnalysisValue:
    """Analysis values for each of the 6Rs framework dimensions."""
    realness: str
    richness: str
    repetition: str
    rationale: str
    repartee: str
    regal: str

class KeywordExtractionSignature(dspy.Signature):
    """
    A comprehensive signature for conducting thematic keyword extraction from quotations,
    following advanced qualitative methodologies. This signature supports systematic keyword analysis
    with robust pattern recognition and theoretical integration, aligned with the 6Rs framework.
    """

    # Input Fields
    research_objectives: str = dspy.InputField(
        desc="Research goals and questions guiding the keyword analysis"
    )

    quotation: str = dspy.InputField(
        desc="Selected quotation for keyword extraction"
    )

    contextualized_contents: List[str] = dspy.InputField(
        desc="Additional contextual content to support interpretation"
    )

    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc="""Theoretical foundation including:
        - theory: Primary theoretical approach
        - philosophical_approach: Underlying philosophical foundation
        - rationale: Justification for chosen approach"""
    )

    # Output Fields
    quotation_info: Dict[str, Any] = dspy.OutputField(
        desc="""Comprehensive context information including:
        - quotation: Selected quotation content
        - research_objectives: Analysis goals
        - theoretical_framework: Complete framework details"""
    )

    retrieved_chunks: List[str] = dspy.OutputField(
        desc="Collection of relevant transcript segments retrieved for analysis"
    )

    retrieved_chunks_count: int = dspy.OutputField(
        desc="Number of transcript chunks retrieved and analyzed"
    )

    used_chunk_ids: List[str] = dspy.OutputField(
        desc="Identifiers of transcript chunks utilized in the analysis"
    )

    keywords: List[Dict[str, Any]] = dspy.OutputField(
        desc="""Extracted keywords with detailed analysis:
        - keyword: The keyword text
        - category: Classification of the keyword
        - 6Rs framework: List of R's keyword falls under, Example: [Realness, Richness]
        - analysis_value: (Only for R's keyword falls under)
            * realness: Reflection of genuine experiences
            * richness: Depth of meaning
            * repetition: Recurrence in quotation
            * rationale: Connection to theoretical foundations
            * repartee: Contribution to discussions
            * regal: Centrality to the topic"""
    )

    analysis: Dict[str, Any] = dspy.OutputField(
        desc="""Comprehensive keyword analysis including:
        - patterns_identified: Key patterns discovered
        - theoretical_interpretation: Framework application
        - methodological_reflection:
            * pattern_robustness
            * theoretical_alignment
            * researcher_reflexivity
        - practical_implications: Applied insights"""
    )

    def create_prompt(self, research_objectives: str, quotation: str,
                     contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> str:
        """Creates the prompt for enhanced keyword extraction from quotation."""

        chunks_formatted = "\n\n".join([
            f"Content {i+1}:\n{content}"
            for i, content in enumerate([quotation] + contextualized_contents)
        ])

        theory = theoretical_framework.get("theory", "")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "")
        rationale = theoretical_framework.get("rationale", "")

        prompt = (
            f"You are an experienced qualitative researcher conducting keyword analysis using "
            f"the 6Rs methodology. Your task is to identify and analyze meaningful keywords "
            f"from the following quotation while maintaining methodological rigor.\n\n"

            f"Quotation for Analysis:\n"
            f"{quotation}\n\n"

            f"Additional Context:\n"
            f"{chunks_formatted}\n\n"

            f"Research Objectives:\n"
            f"{research_objectives}\n\n"

            f"Theoretical Framework:\n"
            f"Theory: {theory}\n"
            f"Philosophical Approach: {philosophical_approach}\n"
            f"Rationale: {rationale}\n\n"

            f"Your analysis should follow the 6Rs framework:\n"
            f"1. Realness: Select words reflecting genuine experiences\n"
            f"2. Richness: Identify words with deep meaning\n"
            f"3. Repetition: Note recurring patterns\n"
            f"4. Rationale: Connect to theoretical foundations\n"
            f"5. Repartee: Consider discussion value\n"
            f"6. Regal: Focus on centrality to topic\n\n"

            f"Consider how each keyword contributes to understanding the quotation "
            f"within its broader context and theoretical framework.\n\n"

            f"Your final output should follow this JSON structure:\n\n"

            "{\n"
            "  \"quotation_info\": {\n"
            "    \"quotation\": \"\",                      // Selected quotation content\n"
            "    \"research_objectives\": \"\",             // Analysis goals\n"
            "    \"theoretical_framework\": {\n"
            "      \"theory\": \"\",                        // Primary theoretical approach\n"
            "      \"philosophical_approach\": \"\",        // Philosophical foundation\n"
            "      \"rationale\": \"\"                      // Justification for approach\n"
            "    }\n"
            "  },\n"
            "  \"retrieved_chunks\": [],                    // List of retrieved chunks (if needed)\n"
            "  \"retrieved_chunks_count\": 0,               // Count of retrieved chunks\n"
            "  \"used_chunk_ids\": [],                      // List of used chunk IDs\n"
            "  \"keywords\": [\n"
            "    {\n"
            "      \"keyword\": \"\",                       // The keyword text\n"
            "      \"category\": \"\",                      // Classification of the keyword\n"
            "      \"6Rs_framework\": [],                   // List of R's keyword falls under\n"
            "      },\n"
            "      \"analysis_value\": {\n"
            "        \"realness\": \"\",                     // Reflection of genuine experiences\n"
            "        \"richness\": \"\",                     // Depth of meaning\n"
            "        \"repetition\": \"\",                   // Recurrence in quotation\n"
            "        \"rationale\": \"\",                    // Connection to theoretical foundations\n"
            "        \"repartee\": \"\",                     // Contribution to discussions\n"
            "        \"regal\": \"\"                         // Centrality to the topic\n"
            "      }\n"
            "    }\n"
            "  ],\n"
            "  \"analysis\": {\n"
            "    \"patterns_identified\": [\"\"],             // Key patterns found\n"
            "    \"theoretical_interpretation\": \"\",        // Framework application\n"
            "    \"methodological_reflection\": {\n"
            "      \"pattern_robustness\": \"\",              // Pattern evidence\n"
            "      \"theoretical_alignment\": \"\",           // Framework fit\n"
            "      \"researcher_reflexivity\": \"\"           // Interpretation awareness\n"
            "    },\n"
            "    \"practical_implications\": \"\"             // Applied insights\n"
            "  }\n"
            "}\n\n"

            f"**Important Instructions:**\n"
            f"- **Your final output must strictly follow the JSON structure provided above, including all fields exactly as specified, even if some fields are empty. Do not omit any fields.**\n"
            f"- **Use double quotes for all strings.**\n"
            f"- **Do not include any additional commentary or text outside of the JSON structure.**\n\n"

            f"Remember to wrap your analysis process in <analysis_process> tags throughout your analysis to show your chain of thought before providing the final JSON output.\n\n"
        )
        return prompt

    def parse_response(self, response: str) -> Dict[str, Any]:
        """Parses the complete response from the language model."""
        try:
            # Use regex to extract JSON content within a code block tagged as json
            import re
            json_match = re.search(r"```json\s*(\{.*\})\s*```", response, re.DOTALL)
            if not json_match:
                logger.error("No valid JSON found in response.")
                logger.debug(f"Full response received: {response}")
                return {}
            json_string = json_match.group(1)

            response_json = json.loads(json_string)
            return response_json
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}, Response: {response}")
            return {}
        except Exception as e:
            logger.error(f"Error parsing response: {e}")
            return {}

    def validate_keywords(self, keywords: List[Dict[str, Any]], quotation: str,
                         contextualized_contents: List[str], research_objectives: str,
                         theoretical_framework: Dict[str, str]) -> None:
        """Validates extracted keywords against the assertions."""
        try:
            assert_keywords_not_exclusive_to_quotation(keywords, quotation, contextualized_contents)
            assert_keyword_specificity(keywords, theoretical_framework)
            assert_keyword_relevance(keywords, research_objectives, contextualized_contents)
            assert_keyword_distinctiveness(keywords)
            logger.debug("All keyword assertions passed successfully.")
        except AssertionError as ae:
            logger.error(f"Keyword validation failed: {ae}")
            raise

    def forward(self, research_objectives: str, quotation: str,
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """Executes keyword extraction with retry mechanism."""
        for attempt in range(3):
            try:
                logger.debug(f"Attempt {attempt + 1} - Starting keyword extraction from quotation")

                # Generate the prompt
                prompt = self.create_prompt(
                    research_objectives,
                    quotation,
                    contextualized_contents,
                    theoretical_framework
                )

                # Generate response from the language model
                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=3000,
                    temperature=0.7
                ).strip()

                logger.debug(f"Attempt {attempt + 1} - Response received from language model.")

                # Parse the complete response
                parsed_response = self.parse_response(response)

                if not parsed_response:
                    raise ValueError("Parsed response is empty. Possibly invalid JSON.")

                # Extract components
                keywords = parsed_response.get("keywords", [])
                analysis = parsed_response.get("analysis", {})

                # Validate extracted keywords
                self.validate_keywords(
                    keywords=keywords,
                    quotation=quotation,
                    contextualized_contents=contextualized_contents,
                    research_objectives=research_objectives,
                    theoretical_framework=theoretical_framework
                )

                logger.info(f"Attempt {attempt + 1} - Successfully extracted and validated {len(keywords)} keywords.")
                return parsed_response

            except AssertionError as ae:
                logger.warning(f"Attempt {attempt + 1} - Assertion failed during keyword extraction: {ae}")
                logger.debug(f"Attempt {attempt + 1} - Response causing assertion failure: {response}")
                # Optionally, refine the prompt or handle the failure as needed
                # For simplicity, we'll retry up to 3 times
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} - Error in KeywordExtractionSignature.forward: {e}", exc_info=True)
                # Continue to next attempt

        logger.error(f"Failed to extract valid keywords after {3} attempts. Last response: {response}")
        return {}


# File: analysis/extract_keyword_module.py
#------------------------------------------------------------------------------
# src/analysis/extract_keyword_module.py

import logging
from typing import Dict, Any, List
import dspy

from src.analysis.extract_keyword import KeywordExtractionSignature

logger = logging.getLogger(__name__)

class KeywordExtractionModule(dspy.Module):
    """
    DSPy module for extracting and analyzing keywords from quotations using the 6Rs framework.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(KeywordExtractionSignature)

    def forward(self, research_objectives: str, quotation: str,
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """
        Execute keyword extraction and analysis with the 6Rs framework.
        """
        try:
            logger.debug("Running KeywordExtractionModule with integrated keyword assertions.")
            response = self.chain(
                research_objectives=research_objectives,
                quotation=quotation,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            return response
        except Exception as e:
            logger.error(f"Error in KeywordExtractionModule.forward: {e}", exc_info=True)
            return {}


# File: analysis/metrics.py
#------------------------------------------------------------------------------
#metrics.py
import logging
from typing import List, Dict, Any, Callable
import dspy

from src.utils.utils import check_answer_length
from src.utils.logger import setup_logging
# Initialize logger
logger = logging.getLogger(__name__)

class BaseAssessment(dspy.Signature):
    """
    Base class for all assessment signatures.
    """
    context: str = dspy.InputField(
        desc=(
            "The contextual information provided to generate the answer. This includes all relevant "
            "documents, data chunks, or information sources that the answer is based upon."
        )
    )
    question: str = dspy.InputField(
        desc=(
            "The original question that was posed. This is used to understand the intent and scope "
            "of the answer in relation to the provided context."
        )
    )
    answer: str = dspy.InputField(
        desc=(
            "The answer generated by the system that needs to be evaluated for factual correctness "
            "based on the provided context."
        )
    )

    def generate_prompt(self, context: str, question: str, answer: str, task: str) -> str:
        return (
            f"Context: {context}\n"
            f"Question: {question}\n"
            f"Answer: {answer}\n\n"
            f"{task}"
        )

class Assess(BaseAssessment):
    """
    Assess the factual correctness of an answer based on the provided context.

    This signature evaluates whether the generated answer accurately reflects the information
    present in the given context. It leverages a language model to perform a nuanced analysis
    beyond simple keyword matching, ensuring a thorough assessment of factual accuracy.
    """
    factually_correct: str = dspy.OutputField(
        desc=(
            "Indicator of whether the answer is factually correct based on the context. "
            "Should be 'Yes' if the answer accurately reflects the information in the context, "
            "and 'No' otherwise."
        )
    )

    def forward(self, context: str, question: str, answer: str) -> Dict[str, str]:
        try:
            logger.debug(f"Assessing factual correctness for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Based on the context provided, evaluate whether the answer is factually correct.\n"
                "Respond with 'Yes' if the answer accurately reflects the information in the context.\n"
                "Respond with 'No' if the answer contains factual inaccuracies or is not supported by the context.\n"
                "Please respond with 'Yes' or 'No' only."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=3,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Factuality assessment response: '{response}'")
            if response.lower() in ['yes', 'no']:
                result = response.capitalize()
            else:
                logger.warning(f"Unexpected response from factuality assessment: '{response}'. Defaulting to 'No'.")
                result = 'No'
            logger.info(f"Factuality assessment result: {result} for question: '{question}'")
            return {"factually_correct": result}
        except Exception as e:
            logger.error(f"Error in Assess.forward: {e}", exc_info=True)
            return {"factually_correct": "No"}


class AssessRelevance(BaseAssessment):
    """
    Assess the relevance of an answer to the given question and context.
    
    This signature evaluates whether the generated answer directly and comprehensively 
    addresses the user's query, considering the provided context.
    """
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug(f"Assessing relevance for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the relevance of the answer to the question based on the context.\n"
                "Provide a relevance score between 1 (not relevant) and 5 (highly relevant)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Relevance assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected relevance score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid relevance score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Relevance assessment result: {result} for question: '{question}'")
            return {"relevance_score": result}
        except Exception as e:
            logger.error(f"Error in AssessRelevance.forward: {e}", exc_info=True)
            return {"relevance_score": 1}


class AssessCoherence(BaseAssessment):
    """
    Assess the coherence of an answer.
    
    This signature evaluates whether the generated answer is well-structured and logically consistent.
    """
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing coherence of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the coherence of the above answer.\n"
                "Provide a coherence score between 1 (not coherent) and 5 (highly coherent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Coherence assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected coherence score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid coherence score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Coherence assessment result: {result}")
            return {"coherence_score": result}
        except Exception as e:
            logger.error(f"Error in AssessCoherence.forward: {e}", exc_info=True)
            return {"coherence_score": 1}


class AssessConciseness(BaseAssessment):
    """
    Assess the conciseness of an answer.
    
    This signature evaluates whether the generated answer is succinct and free from unnecessary verbosity.
    """
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing conciseness of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the conciseness of the above answer.\n"
                "Provide a conciseness score between 1 (not concise) and 5 (highly concise)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Conciseness assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected conciseness score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid conciseness score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Conciseness assessment result: {result}")
            return {"conciseness_score": result}
        except Exception as e:
            logger.error(f"Error in AssessConciseness.forward: {e}", exc_info=True)
            return {"conciseness_score": 1}


class AssessFluency(BaseAssessment):
    """
    Assess the fluency of an answer.
    
    This signature evaluates whether the generated answer exhibits natural language flow and is free from grammatical errors.
    """
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing fluency of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the fluency of the above answer.\n"
                "Provide a fluency score between 1 (not fluent) and 5 (highly fluent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Fluency assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected fluency score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid fluency score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Fluency assessment result: {result}")
            return {"fluency_score": result}
        except Exception as e:
            logger.error(f"Error in AssessFluency.forward: {e}", exc_info=True)
            return {"fluency_score": 1}


class ComprehensiveAssessment(BaseAssessment):
    """
    Comprehensive assessment of generated answers across multiple dimensions.
    """
    factually_correct: str = dspy.OutputField(desc="Whether the answer is factually correct ('Yes'/'No').")
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, Any]:
        try:
            logger.debug(f"Performing comprehensive assessment for question: '{question}'")
            
            # Assess factual correctness
            factuality = Assess()(context=context, question=question, answer=answer)['factually_correct']
            
            # Assess relevance
            relevance = AssessRelevance()(context=context, question=question, answer=answer)['relevance_score']
            
            # Assess coherence
            coherence = AssessCoherence()(context=context, question=question, answer=answer)['coherence_score']
            
            # Assess conciseness
            conciseness = AssessConciseness()(context=context, question=question, answer=answer)['conciseness_score']
            
            # Assess fluency
            fluency = AssessFluency()(context=context, question=question, answer=answer)['fluency_score']
            
            # Aggregate scores with adjusted weights
            composite_score = {
                "factually_correct": factuality,
                "relevance_score": relevance,
                "coherence_score": coherence,
                "conciseness_score": conciseness,
                "fluency_score": fluency
            }
            logger.info(f"Comprehensive assessment result: {composite_score}")
            return composite_score
        except Exception as e:
            logger.error(f"Error in ComprehensiveAssessment.forward: {e}", exc_info=True)
            return {
                "factually_correct": "No",
                "relevance_score": 1,
                "coherence_score": 1,
                "conciseness_score": 1,
                "fluency_score": 1
            }


def comprehensive_metric(example: Dict[str, Any], pred: Dict[str, Any], trace: Any = None) -> float:
    """
    Comprehensive metric to evaluate the answer across multiple dimensions.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
        trace (Any, optional): Trace information for optimization (unused here).
    
    Returns:
        float: A combined score representing the quality of the answer.
    """
    try:
        logger.debug(f"Evaluating comprehensive metrics for question: '{example.get('question', '')}'")
        assessment = comprehensive_assessment_module(
            context=example.get('context', ''),
            question=example.get('question', ''),
            answer=pred.get('answer', '')
        )
        logger.debug(f"Comprehensive assessment: {assessment}")
        
        # Convert 'factually_correct' to binary
        factually_correct = 1 if assessment.get("factually_correct") == "Yes" else 0
        
        # Normalize scores between 0 and 1
        relevance = assessment.get("relevance_score", 1) / 5
        coherence = assessment.get("coherence_score", 1) / 5
        conciseness = assessment.get("conciseness_score", 1) / 5
        fluency = assessment.get("fluency_score", 1) / 5
        
        # Define adjusted weights for each metric
        weights = {
            "factually_correct": 0.5,
            "relevance": 0.2,
            "coherence": 0.1,
            "conciseness": 0.1,
            "fluency": 0.1
        }
        
        # Calculate the composite score
        composite_score = (
            weights["factually_correct"] * factually_correct +
            weights["relevance"] * relevance +
            weights["coherence"] * coherence +
            weights["conciseness"] * conciseness +
            weights["fluency"] * fluency
        )
        
        logger.info(f"Comprehensive metric score: {composite_score}")
        return composite_score
    except Exception as e:
        logger.error(f"Error in comprehensive_metric: {e}", exc_info=True)
        return 0.0


def is_answer_fully_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    """
    Determines if the answer meets all quality metrics.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
    
    Returns:
        bool: True if all metrics meet the desired thresholds, False otherwise.
    """
    scores = comprehensive_metric(example, pred)
    # Define threshold (e.g., composite score should be at least 0.8)
    is_factual = scores >= 0.8
    logger.debug(f"Is answer fully correct (scores >= 0.8): {is_factual}")
    return is_factual


def factuality_metric(example: Dict[str, Any], pred: Dict[str, Any]) -> int:
    """
    Metric to evaluate factual correctness of the answer.

    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.

    Returns:
        int: 1 if factually correct, 0 otherwise.
    """
    try:
        assess = Assess()
        result = assess(context=example.get('context', ''), question=example.get('question', ''), answer=pred.get('answer', ''))
        factually_correct = result.get('factually_correct', 'No')
        logger.debug(f"Factuality metric result: {factually_correct}")
        return 1 if factually_correct == 'Yes' else 0
    except Exception as e:
        logger.error(f"Error in factuality_metric: {e}", exc_info=True)
        return 0 


# Initialize the assessment modules
try:
    # Use the unoptimized module directly without caching
    comprehensive_assessment_module = dspy.TypedChainOfThought(ComprehensiveAssessment)
    logger.info("Comprehensive Assessment DSPy module initialized successfully.")
except Exception as e:
    logger.error(f"Error initializing Comprehensive Assessment DSPy module: {e}", exc_info=True)
    raise


# File: analysis/select_quotation.py
#------------------------------------------------------------------------------
#analysis/select_quotation.py
import logging
import re
import json
from typing import List, Dict, Any

import dspy

from src.assertions import (
    assert_pattern_representation,
    assert_research_objective_alignment,
    assert_selective_transcription,
    assert_creswell_categorization,
    assert_reader_engagement
)

logger = logging.getLogger(__name__)

class EnhancedQuotationSignature(dspy.Signature):
    """
    A comprehensive signature for conducting thematic analysis of interview transcripts
    following Braun and Clarke's (2006) methodology. This signature supports systematic
    qualitative analysis with robust pattern recognition and theoretical integration.
    """
    
    # Input Fields
    research_objectives: str = dspy.InputField(
        desc="Research goals and questions guiding the thematic analysis"
    )
    
    transcript_chunk: str = dspy.InputField(
        desc="Primary transcript segment for analysis"
    )
    
    contextualized_contents: List[str] = dspy.InputField(
        desc="Additional contextual content to support interpretation"
    )
    
    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc="""Theoretical foundation including:
        - theory: Primary theoretical approach
        - philosophical_approach: Underlying philosophical foundation
        - rationale: Justification for chosen approach"""
    )
    
    # Output Fields
    transcript_info: Dict[str, Any] = dspy.OutputField(
        desc="""Comprehensive context information including:
        - transcript_chunk: Selected content
        - research_objectives: Analysis goals
        - theoretical_framework: Complete framework details"""
    )
    
    retrieved_chunks: List[str] = dspy.OutputField(
        desc="Collection of relevant transcript segments retrieved for analysis"
    )
    
    retrieved_chunks_count: int = dspy.OutputField(
        desc="Number of transcript chunks retrieved and analyzed"
    )
    
    used_chunk_ids: List[str] = dspy.OutputField(
        desc="Identifiers of transcript chunks utilized in the analysis"
    )
    
    quotations: List[Dict[str, Any]] = dspy.OutputField(
        desc="""Selected quotations with detailed analysis:
        - quotation: Exact quote text
        - creswell_category: Classification (longer/discrete/embedded)
        - classification: Content type
        - context: 
            * preceding_question
            * situation
            * pattern_representation
        - analysis_value:
            * relevance to objectives
            * pattern support
            * theoretical alignment"""
    )
    
    analysis: Dict[str, Any] = dspy.OutputField(
        desc="""Comprehensive thematic analysis including:
        - philosophical_underpinning: Analysis approach
        - patterns_identified: Key patterns discovered
        - theoretical_interpretation: Framework application
        - methodological_reflection:
            * pattern_robustness
            * theoretical_alignment
            * researcher_reflexivity
        - practical_implications: Applied insights"""
    )
    
    answer: Dict[str, Any] = dspy.OutputField(
        desc="""Analysis synthesis and contributions:
        - summary: Key findings
        - theoretical_contribution: Theory advancement
        - methodological_contribution:
            * approach
            * pattern_validity
            * theoretical_integration"""
    )

    def create_prompt(self, research_objectives: str, transcript_chunk: str, 
                     contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> str:
        """Creates the prompt for the language model."""
        
        # Format contextualized contents
        chunks_formatted = "\n\n".join([
            f"Content {i+1}:\n{content}" 
            for i, content in enumerate([transcript_chunk] + contextualized_contents)
        ])

        theory = theoretical_framework.get("theory", "")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "")
        rationale = theoretical_framework.get("rationale", "")

        prompt = (
            f"You are an experienced qualitative researcher conducting a thematic analysis of "
            f"interview transcripts using Braun and Clarke's (2006) approach. Your task is to "
            f"analyze the provided transcript chunks while adhering to key principles from their "
            f"thematic analysis methodology.\n\n"

            f"First, review the transcript chunks and contextualized_content:\n\n"
            f"{chunks_formatted}\n\n"
            
            f"Research Objectives:\n"
            f"<research_objectives>\n"
            f"{research_objectives}\n"
            f"</research_objectives>\n\n"

            f"Theoretical Framework:\n"
            f"<theoretical_framework>\n"
            f"Theory: {theory}\n"
            f"Philosophical Approach: {philosophical_approach}\n"
            f"Rationale: {rationale}\n"
            f"</theoretical_framework>\n\n"
                    
            f"Your analysis should follow these steps:\n\n"

            f"1. **Quotation Selection**:\n"
            f"   - Select quotes that demonstrate robust patterns in the data.\n"
            f"   - Classify quotes using Creswell's categories:\n"
            f"     a) Longer quotations: For complex understandings\n"
            f"     b) Discrete quotations: For diverse perspectives\n"
            f"     c) Embedded quotations: Brief phrases showing text shifts\n"
            f"   - Ensure quotes enhance reader engagement and highlight unique findings.\n"
            f"   - Provide adequate context for accurate comprehension.\n\n"

            f"2. **Pattern Recognition**:\n"
            f"   - Identify patterns emerging from data rather than predetermined categories.\n"
            f"   - Support patterns with multiple quotations.\n"
            f"   - Maintain theoretical alignment while remaining open to emerging themes.\n"
            f"   - Document methodological decisions transparently.\n\n"

            f"3. **Theoretical Integration**:\n"
            f"   - Demonstrate clear philosophical underpinning.\n"
            f"   - Show how findings connect to the theoretical framework.\n"
            f"   - Practice researcher reflexivity throughout analysis.\n"
            f"   - Balance selectivity with comprehensiveness.\n\n"

            f"For each step of your analysis, wrap your analysis process in <analysis_process> tags to explain your thought process and reasoning before providing the final output. It's OK for this section to be quite long.\n\n"

            f"Your final output should follow this JSON structure:\n\n"

            "{\n"
            "  \"transcript_info\": {\n"
            "    \"transcript_chunk\": \"\",                    // Selected transcript content\n"
            "    \"research_objectives\": \"\",                 // Research goals guiding analysis\n"
            "    \"theoretical_framework\": {\n"
            "      \"theory\": \"\",                            // Primary theoretical approach\n"
            "      \"philosophical_approach\": \"\",            // Philosophical foundation\n"
            "      \"rationale\": \"\"                          // Justification for approach\n"
            "    }\n"
            "  },\n"
            "  \"retrieved_chunks\": [],                        // List of retrieved chunks (if needed)\n"
            "  \"retrieved_chunks_count\": 0,                   // Count of retrieved chunks\n"
            "  \"contextualized_contents\": [],                 // List of contextualized contents\n"
            "  \"used_chunk_ids\": [],                          // List of used chunk IDs\n"
            "  \"quotations\": [\n"
            "    {\n"
            "      \"quotation\": \"\",                         // Exact quote text\n"
            "      \"creswell_category\": \"\",                 // longer/discrete/embedded\n"
            "      \"classification\": \"\",                    // Content type\n"
            "      \"context\": {\n"
            "        \"preceding_question\": \"\",              // Prior question\n"
            "        \"situation\": \"\",                       // Context description\n"
            "        \"pattern_representation\": \"\"           // Pattern linkage\n"
            "      },\n"
            "      \"analysis_value\": {\n"
            "        \"relevance\": \"\",                       // Research objective alignment\n"
            "        \"pattern_support\": \"\",                 // Pattern evidence\n"
            "        \"theoretical_alignment\": \"\"            // Framework connection\n"
            "      }\n"
            "    }\n"
            "  ],\n"
            "  \"analysis\": {\n"
            "    \"philosophical_underpinning\": \"\",          // Analysis approach\n"
            "    \"patterns_identified\": [\"\"],               // Key patterns found\n"
            "    \"theoretical_interpretation\": \"\",          // Framework application\n"
            "    \"methodological_reflection\": {\n"
            "      \"pattern_robustness\": \"\",                // Pattern evidence\n"
            "      \"theoretical_alignment\": \"\",             // Framework fit\n"
            "      \"researcher_reflexivity\": \"\"             // Interpretation awareness\n"
            "    },\n"
            "    \"practical_implications\": \"\"               // Applied insights\n"
            "  },\n"
            "  \"answer\": {\n"
            "    \"summary\": \"\",                            // Key findings\n"
            "    \"theoretical_contribution\": \"\",            // Theory advancement\n"
            "    \"methodological_contribution\": {\n"
            "      \"approach\": \"\",                         // Method used\n"
            "      \"pattern_validity\": \"\",                 // Evidence quality\n"
            "      \"theoretical_integration\": \"\"           // Theory-data synthesis\n"
            "    }\n"
            "  }\n"
            "}\n\n"

            f"**Important Instructions:**\n"
            f"- **Your final output must strictly follow the JSON structure provided below, including all fields exactly as specified, even if some fields are empty. Do not omit any fields.**\n"
            f"- **Use double quotes for all strings.**\n"
            f"- **Do not include any additional commentary or text outside of the JSON structure.**\n\n"
            
            f"Remember to wrap your analysis process in <analysis_process> tags throughout your analysis to show your chain of thought before providing the final JSON output.\n\n"
        )
        return prompt

    def parse_response(self, response: str) -> Dict[str, Any]:
        """Parses the complete response from the language model."""
        try:
            # Use regex to extract JSON content within a code block tagged as json
            json_match = re.search(r"```json\s*(\{.*\})\s*```", response, re.DOTALL)
            if not json_match:
                logger.error("No valid JSON found in response.")
                logger.debug(f"Full response received: {response}")
                return {}
            json_string = json_match.group(1)

            response_json = json.loads(json_string)
            return response_json
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}, Response: {response}")
            return {}
        except Exception as e:
            logger.error(f"Error parsing response: {e}")
            return {}

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """Executes the quotation selection and analysis process with retry mechanism."""
        for attempt in range(3):  # Retry mechanism with up to 3 attempts
            try:
                logger.debug(f"Attempt {attempt + 1} - Starting enhanced quotation selection and analysis process.")
                
                # Generate the prompt
                prompt = self.create_prompt(
                    research_objectives,
                    transcript_chunk,
                    contextualized_contents,
                    theoretical_framework
                )
                
                # Generate response from the language model
                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=6000,
                    temperature=0.5
                ).strip()
                
                logger.debug(f"Attempt {attempt + 1} - Response received from language model.")
                
                # Parse the complete response
                parsed_response = self.parse_response(response)
                
                if not parsed_response:
                    raise ValueError("Parsed response is empty. Possibly invalid JSON.")
                
                # Extract components
                quotations = parsed_response.get("quotations", [])
                analysis = parsed_response.get("analysis", {})
                
                # Apply assertions
                assert_pattern_representation(quotations, analysis.get("patterns_identified", []))
                assert_research_objective_alignment(quotations, research_objectives)
                assert_selective_transcription(quotations, transcript_chunk)
                assert_creswell_categorization(quotations)
                assert_reader_engagement(quotations)
                
                
                logger.info(f"Attempt {attempt + 1} - Successfully completed analysis with {len(quotations)} quotations.")
                return parsed_response

            except AssertionError as af:
                logger.warning(f"Attempt {attempt + 1} - Assertion failed during analysis: {af}")
                logger.debug(f"Attempt {attempt + 1} - Response causing assertion failure: {response}")
                # Handle failed assertion by refining the prompt and retrying
                parsed_response = self.handle_failed_assertion(
                    af, research_objectives, transcript_chunk, contextualized_contents, theoretical_framework
                )
                if parsed_response:
                    return parsed_response
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} - Error in EnhancedQuotationSignature.forward: {e}", exc_info=True)
                # Continue to next attempt
                
        logger.error(f"Failed to generate valid output after multiple attempts. Last response: {response}")
        return {}

    def handle_failed_assertion(self, assertion_failure: AssertionError,
                                research_objectives: str, transcript_chunk: str,
                                contextualized_contents: List[str],
                                theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """
        Handles failed assertions by attempting to generate improved analysis.
        This method refines the prompt based on the specific assertion failure and retries the generation.
        """
        try:
            logger.debug("Handling failed assertion by refining the prompt.")

            # Generate the initial prompt
            focused_prompt = self.create_prompt(
                research_objectives,
                transcript_chunk,
                contextualized_contents,
                theoretical_framework
            )
            
            # Append instructions to address the specific assertion failure
            focused_prompt += (
                f"\n\nThe previous attempt failed because: {assertion_failure}\n"
                f"Please ensure that your analysis addresses this specific issue while maintaining "
                f"all other requirements for thorough theoretical analysis."
            )

            # Generate a new response with the refined prompt
            response = self.language_model.generate(
                prompt=focused_prompt,
                max_tokens=2000,
                temperature=0.5
            ).strip()

            logger.debug("Response received from language model after handling assertion failure.")

            # Parse the new response
            parsed_response = self.parse_response(response)
            
            if not parsed_response:
                raise ValueError("Parsed response is empty after handling assertion failure.")
            
            # Re-apply all assertions
            quotations = parsed_response.get("quotations", [])
            analysis = parsed_response.get("analysis", {})
            
            assert_pattern_representation(quotations, analysis.get("patterns_identified", []))
            assert_research_objective_alignment(quotations, research_objectives)
            assert_selective_transcription(quotations, transcript_chunk)
            assert_creswell_categorization(quotations)
            assert_reader_engagement(quotations)
            

            logger.info("Successfully handled failed assertion and obtained valid analysis.")
            return parsed_response

        except AssertionError as af_inner:
            logger.error(f"Refined analysis still failed assertions: {af_inner}")
            return {}
        except Exception as e:
            logger.error(f"Error in handle_failed_assertion: {e}", exc_info=True)
            return {}

class EnhancedQuotationModule(dspy.Module):
    """
    DSPy module implementing the enhanced quotation selection and theoretical analysis functionality.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(EnhancedQuotationSignature)

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Running EnhancedQuotationModule with integrated theoretical analysis.")
            response = self.chain(
                research_objectives=research_objectives,
                transcript_chunk=transcript_chunk,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            return response
        except Exception as e:
            logger.error(f"Error in EnhancedQuotationModule.forward: {e}", exc_info=True)
            return {}


# File: analysis/select_quotation_alt.py
#------------------------------------------------------------------------------
#analysis/select_quotation.py
import logging
import re
import json
from typing import List, Dict, Any

import dspy

from src.assertions import (
    assert_pattern_representation,
    assert_research_objective_alignment,
    assert_selective_transcription,
    assert_creswell_categorization,
    assert_reader_engagement
)

logger = logging.getLogger(__name__)

class EnhancedQuotationSignature(dspy.Signature):
    """
    A comprehensive signature for conducting thematic analysis of interview transcripts
    following Braun and Clarke's (2006) methodology. This signature supports systematic
    qualitative analysis with robust pattern recognition and theoretical integration. in French
    """
    
    # Input Fields
    research_objectives: str = dspy.InputField(
        desc="Research goals and questions guiding the thematic analysis"
    )
    
    transcript_chunk: str = dspy.InputField(
        desc="Primary transcript segment for analysis"
    )
    
    contextualized_contents: List[str] = dspy.InputField(
        desc="Additional contextual content to support interpretation"
    )
    
    theoretical_framework: Dict[str, str] = dspy.InputField(
        desc="""Theoretical foundation including:
        - theory: Primary theoretical approach
        - philosophical_approach: Underlying philosophical foundation
        - rationale: Justification for chosen approach"""
    )
    
    # Output Fields
    transcript_info: Dict[str, Any] = dspy.OutputField(
        desc="""Comprehensive context information including:
        - transcript_chunk: Selected content
        - research_objectives: Analysis goals
        - theoretical_framework: Complete framework details in French"""
    )
    
    retrieved_chunks: List[str] = dspy.OutputField(
        desc="Collection of relevant transcript segments retrieved for analysis"
    )
    
    retrieved_chunks_count: int = dspy.OutputField(
        desc="Number of transcript chunks retrieved and analyzed"
    )
    
    used_chunk_ids: List[str] = dspy.OutputField(
        desc="Identifiers of transcript chunks utilized in the analysis"
    )
    
    quotations: List[Dict[str, Any]] = dspy.OutputField(
        desc="""Selected quotations with detailed analysis:
        - quotation: Exact quote text
        - creswell_category: Classification (longer/discrete/embedded)
        - classification: Content type
        - context: 
            * preceding_question
            * situation
            * pattern_representation
        - analysis_value:
            * relevance to objectives
            * pattern support
            * theoretical alignment in French""" 
    )
    
    analysis: Dict[str, Any] = dspy.OutputField(
        desc="""Comprehensive thematic analysis including:
        - philosophical_underpinning: Analysis approach
        - patterns_identified: Key patterns discovered
        - theoretical_interpretation: Framework application
        - methodological_reflection:
            * pattern_robustness
            * theoretical_alignment
            * researcher_reflexivity
        - practical_implications: Applied insights in French"""
    )
    
    answer: Dict[str, Any] = dspy.OutputField(
        desc="""Analysis synthesis and contributions:
        - summary: Key findings
        - theoretical_contribution: Theory advancement
        - methodological_contribution:
            * approach
            * pattern_validity
            * theoretical_integration in French"""
    )

    def create_prompt(self, research_objectives: str, transcript_chunk: str, 
                     contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> str:
        """Creates the prompt for the language model."""
        
        # Format contextualized contents
        chunks_formatted = "\n\n".join([
            f"Content {i+1}:\n{content}" 
            for i, content in enumerate([transcript_chunk] + contextualized_contents)
        ])

        theory = theoretical_framework.get("theory", "")
        philosophical_approach = theoretical_framework.get("philosophical_approach", "")
        rationale = theoretical_framework.get("rationale", "")

        prompt = (
            f"You are an experienced qualitative researcher conducting a thematic analysis of "
            f"interview transcripts using Braun and Clarke's (2006) approach. Your task is to "
            f"analyze the provided transcript chunks while adhering to key principles from their "
            f"thematic analysis methodology.\n\n"

            f"First, review the transcript chunks and contextualized_content:\n\n"
            f"{chunks_formatted}\n\n"
            
            f"Research Objectives:\n"
            f"<research_objectives>\n"
            f"{research_objectives}\n"
            f"</research_objectives>\n\n"

            f"Theoretical Framework:\n"
            f"<theoretical_framework>\n"
            f"Theory: {theory}\n"
            f"Philosophical Approach: {philosophical_approach}\n"
            f"Rationale: {rationale}\n"
            f"</theoretical_framework>\n\n"
                    
            f"Your analysis should follow these steps:\n\n"

            f"1. **Quotation Selection**:\n"
            f"   - Select quotes that demonstrate robust patterns in the data.\n"
            f"   - Classify quotes using Creswell's categories:\n"
            f"     a) Longer quotations: For complex understandings\n"
            f"     b) Discrete quotations: For diverse perspectives\n"
            f"     c) Embedded quotations: Brief phrases showing text shifts\n"
            f"   - Ensure quotes enhance reader engagement and highlight unique findings.\n"
            f"   - Provide adequate context for accurate comprehension.\n\n"

            f"2. **Pattern Recognition**:\n"
            f"   - Identify patterns emerging from data rather than predetermined categories.\n"
            f"   - Support patterns with multiple quotations.\n"
            f"   - Maintain theoretical alignment while remaining open to emerging themes.\n"
            f"   - Document methodological decisions transparently.\n\n"

            f"3. **Theoretical Integration**:\n"
            f"   - Demonstrate clear philosophical underpinning.\n"
            f"   - Show how findings connect to the theoretical framework.\n"
            f"   - Practice researcher reflexivity throughout analysis.\n"
            f"   - Balance selectivity with comprehensiveness.\n\n"

            f"For each step of your analysis, wrap your analysis process in <analysis_process> tags to explain your thought process and reasoning before providing the final output. It's OK for this section to be quite long.\n\n"

            f"Your final output should follow this JSON structure:\n\n"

            "{\n"
            "  \"transcript_info\": {\n"
            "    \"transcript_chunk\": \"\",                    // Selected transcript content\n"
            "    \"research_objectives\": \"\",                 // Research goals guiding analysis\n"
            "    \"theoretical_framework\": {\n"
            "      \"theory\": \"\",                            // Primary theoretical approach\n"
            "      \"philosophical_approach\": \"\",            // Philosophical foundation\n"
            "      \"rationale\": \"\"                          // Justification for approach\n"
            "    }\n"
            "  },\n"
            "  \"retrieved_chunks\": [],                        // List of retrieved chunks (if needed)\n"
            "  \"retrieved_chunks_count\": 0,                   // Count of retrieved chunks\n"
            "  \"contextualized_contents\": [],                 // List of contextualized contents\n"
            "  \"used_chunk_ids\": [],                          // List of used chunk IDs\n"
            "  \"quotations\": [\n"
            "    {\n"
            "      \"quotation\": \"\",                         // Exact quote text\n"
            "      \"creswell_category\": \"\",                 // longer/discrete/embedded\n"
            "      \"classification\": \"\",                    // Content type\n"
            "      \"context\": {\n"
            "        \"preceding_question\": \"\",              // Prior question\n"
            "        \"situation\": \"\",                       // Context description\n"
            "        \"pattern_representation\": \"\"           // Pattern linkage\n"
            "      },\n"
            "      \"analysis_value\": {\n"
            "        \"relevance\": \"\",                       // Research objective alignment\n"
            "        \"pattern_support\": \"\",                 // Pattern evidence\n"
            "        \"theoretical_alignment\": \"\"            // Framework connection\n"
            "      }\n"
            "    }\n"
            "  ],\n"
            "  \"analysis\": {\n"
            "    \"philosophical_underpinning\": \"\",          // Analysis approach\n"
            "    \"patterns_identified\": [\"\"],               // Key patterns found\n"
            "    \"theoretical_interpretation\": \"\",          // Framework application\n"
            "    \"methodological_reflection\": {\n"
            "      \"pattern_robustness\": \"\",                // Pattern evidence\n"
            "      \"theoretical_alignment\": \"\",             // Framework fit\n"
            "      \"researcher_reflexivity\": \"\"             // Interpretation awareness\n"
            "    },\n"
            "    \"practical_implications\": \"\"               // Applied insights\n"
            "  },\n"
            "  \"answer\": {\n"
            "    \"summary\": \"\",                            // Key findings\n"
            "    \"theoretical_contribution\": \"\",            // Theory advancement\n"
            "    \"methodological_contribution\": {\n"
            "      \"approach\": \"\",                         // Method used\n"
            "      \"pattern_validity\": \"\",                 // Evidence quality\n"
            "      \"theoretical_integration\": \"\"           // Theory-data synthesis\n"
            "    }\n"
            "  }\n"
            "}\n\n"

            f"**Important Instructions:**\n"
            f"- **Your final output must strictly follow the JSON structure provided below, including all fields exactly as specified, even if some fields are empty. Do not omit any fields.**\n"
            f"- **Use double quotes for all strings.**\n"
            f"- **Do not include any additional commentary or text outside of the JSON structure.**\n\n"
            
            f"Remember to wrap your analysis process in <analysis_process> tags throughout your analysis to show your chain of thought before providing the final JSON output.\n\n"
        )
        return prompt

    def parse_response(self, response: str) -> Dict[str, Any]:
        """Parses the complete response from the language model."""
        try:
            # Use regex to extract JSON content within a code block tagged as json
            json_match = re.search(r"```json\s*(\{.*\})\s*```", response, re.DOTALL)
            if not json_match:
                logger.error("No valid JSON found in response.")
                logger.debug(f"Full response received: {response}")
                return {}
            json_string = json_match.group(1)

            response_json = json.loads(json_string)
            return response_json
        except json.JSONDecodeError as e:
            logger.error(f"JSON decoding failed: {e}, Response: {response}")
            return {}
        except Exception as e:
            logger.error(f"Error parsing response: {e}")
            return {}

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """Executes the quotation selection and analysis process with retry mechanism."""
        for attempt in range(3):  # Retry mechanism with up to 3 attempts
            try:
                logger.debug(f"Attempt {attempt + 1} - Starting enhanced quotation selection and analysis process.")
                
                # Generate the prompt
                prompt = self.create_prompt(
                    research_objectives,
                    transcript_chunk,
                    contextualized_contents,
                    theoretical_framework
                )
                
                # Generate response from the language model
                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=6000,
                    temperature=0.5
                ).strip()
                
                logger.debug(f"Attempt {attempt + 1} - Response received from language model.")
                
                # Parse the complete response
                parsed_response = self.parse_response(response)
                
                if not parsed_response:
                    raise ValueError("Parsed response is empty. Possibly invalid JSON.")
                
                # Extract components
                quotations = parsed_response.get("quotations", [])
                analysis = parsed_response.get("analysis", {})
                
                # Apply assertions
                assert_pattern_representation(quotations, analysis.get("patterns_identified", []))
                assert_research_objective_alignment(quotations, research_objectives)
                assert_selective_transcription(quotations, transcript_chunk)
                assert_creswell_categorization(quotations)
                assert_reader_engagement(quotations)
                
                
                logger.info(f"Attempt {attempt + 1} - Successfully completed analysis with {len(quotations)} quotations.")
                return parsed_response

            except AssertionError as af:
                logger.warning(f"Attempt {attempt + 1} - Assertion failed during analysis: {af}")
                logger.debug(f"Attempt {attempt + 1} - Response causing assertion failure: {response}")
                # Handle failed assertion by refining the prompt and retrying
                parsed_response = self.handle_failed_assertion(
                    af, research_objectives, transcript_chunk, contextualized_contents, theoretical_framework
                )
                if parsed_response:
                    return parsed_response
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} - Error in EnhancedQuotationSignature.forward: {e}", exc_info=True)
                # Continue to next attempt
                
        logger.error(f"Failed to generate valid output after multiple attempts. Last response: {response}")
        return {}

    def handle_failed_assertion(self, assertion_failure: AssertionError,
                                research_objectives: str, transcript_chunk: str,
                                contextualized_contents: List[str],
                                theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        """
        Handles failed assertions by attempting to generate improved analysis.
        This method refines the prompt based on the specific assertion failure and retries the generation.
        """
        try:
            logger.debug("Handling failed assertion by refining the prompt.")

            # Generate the initial prompt
            focused_prompt = self.create_prompt(
                research_objectives,
                transcript_chunk,
                contextualized_contents,
                theoretical_framework
            )
            
            # Append instructions to address the specific assertion failure
            focused_prompt += (
                f"\n\nThe previous attempt failed because: {assertion_failure}\n"
                f"Please ensure that your analysis addresses this specific issue while maintaining "
                f"all other requirements for thorough theoretical analysis."
            )

            # Generate a new response with the refined prompt
            response = self.language_model.generate(
                prompt=focused_prompt,
                max_tokens=2000,
                temperature=0.5
            ).strip()

            logger.debug("Response received from language model after handling assertion failure.")

            # Parse the new response
            parsed_response = self.parse_response(response)
            
            if not parsed_response:
                raise ValueError("Parsed response is empty after handling assertion failure.")
            
            # Re-apply all assertions
            quotations = parsed_response.get("quotations", [])
            analysis = parsed_response.get("analysis", {})
            
            assert_pattern_representation(quotations, analysis.get("patterns_identified", []))
            assert_research_objective_alignment(quotations, research_objectives)
            assert_selective_transcription(quotations, transcript_chunk)
            assert_creswell_categorization(quotations)
            assert_reader_engagement(quotations)
            

            logger.info("Successfully handled failed assertion and obtained valid analysis.")
            return parsed_response

        except AssertionError as af_inner:
            logger.error(f"Refined analysis still failed assertions: {af_inner}")
            return {}
        except Exception as e:
            logger.error(f"Error in handle_failed_assertion: {e}", exc_info=True)
            return {}

class EnhancedQuotationModule(dspy.Module):
    """
    DSPy module implementing the enhanced quotation selection and theoretical analysis functionality.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(EnhancedQuotationSignature)

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Running EnhancedQuotationModule with integrated theoretical analysis.")
            response = self.chain(
                research_objectives=research_objectives,
                transcript_chunk=transcript_chunk,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            return response
        except Exception as e:
            logger.error(f"Error in EnhancedQuotationModule.forward: {e}", exc_info=True)
            return {}


# File: analysis/select_quotation_module.py
#------------------------------------------------------------------------------
#analysis/select_quotation_module.py
import logging
from typing import Dict, Any, List
import dspy

from src.analysis.select_quotation import EnhancedQuotationModule
from src.assertions import (
    assert_pattern_representation,
    assert_research_objective_alignment,
    assert_selective_transcription,
    assert_creswell_categorization,
    assert_reader_engagement
)

logger = logging.getLogger(__name__)

class SelectQuotationModule(dspy.Module):
    """
    DSPy module to select and analyze quotations based on research objectives,
    transcript chunks, and theoretical framework.
    """
    def __init__(self):
        super().__init__()
        self.enhanced_module = EnhancedQuotationModule()

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Running SelectQuotationModule with integrated theoretical analysis.")
            response = self.enhanced_module.forward(
                research_objectives=research_objectives,
                transcript_chunk=transcript_chunk,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            # Apply new assertions
            quotations = response.get("quotations", [])
            analysis = response.get("analysis", {})
            patterns = analysis.get("patterns_identified", [])

            # Apply the new assertions
            assert_pattern_representation(quotations, patterns)
            assert_research_objective_alignment(quotations, research_objectives)
            assert_selective_transcription(quotations, transcript_chunk)
            assert_creswell_categorization(quotations)
            assert_reader_engagement(quotations)

            # The response already includes 'transcript_info', 'quotations', 'analysis', and 'answer'
            return response
        except Exception as e:
            logger.error(f"Error in SelectQuotationModule.forward: {e}", exc_info=True)
            return {}


# File: analysis/select_quotation_module_alt.py
#------------------------------------------------------------------------------
#analysis/select_quotation_module.py
import logging
from typing import Dict, Any, List
import dspy

from src.analysis.select_quotation_alt import EnhancedQuotationModule
from src.assertions import (
    assert_pattern_representation,
    assert_research_objective_alignment,
    assert_selective_transcription,
    assert_creswell_categorization,
    assert_reader_engagement
)

logger = logging.getLogger(__name__)

class SelectQuotationModule(dspy.Module):
    """
    DSPy module to select and analyze quotations based on research objectives,
    transcript chunks, and theoretical framework.
    """
    def __init__(self):
        super().__init__()
        self.enhanced_module = EnhancedQuotationModule()

    def forward(self, research_objectives: str, transcript_chunk: str, 
                contextualized_contents: List[str], theoretical_framework: Dict[str, str]) -> Dict[str, Any]:
        try:
            logger.debug("Running SelectQuotationModule with integrated theoretical analysis.")
            response = self.enhanced_module.forward(
                research_objectives=research_objectives,
                transcript_chunk=transcript_chunk,
                contextualized_contents=contextualized_contents,
                theoretical_framework=theoretical_framework
            )
            # Apply new assertions
            quotations = response.get("quotations", [])
            analysis = response.get("analysis", {})
            patterns = analysis.get("patterns_identified", [])

            # Apply the new assertions
            assert_pattern_representation(quotations, patterns)
            assert_research_objective_alignment(quotations, research_objectives)
            assert_selective_transcription(quotations, transcript_chunk)
            assert_creswell_categorization(quotations)
            assert_reader_engagement(quotations)

            # The response already includes 'transcript_info', 'quotations', 'analysis', and 'answer'
            return response
        except Exception as e:
            logger.error(f"Error in SelectQuotationModule.forward: {e}", exc_info=True)
            return {}



################################################################################
# Module: root
################################################################################


# File: assertions.py
#------------------------------------------------------------------------------

#src/assertions_quotation.py
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def assert_pattern_representation(quotations: List[Dict[str, Any]], patterns: List[str]) -> None:
    """
    Ensure quotations represent robust patterns in the data.
    According to the paper: "Quotations should symbolize robust patterns within the data"
    and "Select quotes that demonstrate robust patterns in the data."

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata
        patterns (List[str]): Identified patterns in the data

    Raises:
        AssertionError: If quotations don't demonstrate robust patterns
    """
    if not patterns:
        raise AssertionError("No patterns provided for analysis")

    pattern_support = {pattern: [] for pattern in patterns}

    for quote in quotations:
        pattern_representation = quote.get("context", {}).get("pattern_representation", "")
        for pattern in patterns:
            if pattern.lower() in pattern_representation.lower():
                pattern_support[pattern].append(quote["quotation"])

    # Each pattern should be supported by multiple quotations for robustness
    for pattern, supporting_quotes in pattern_support.items():
        if len(supporting_quotes) < 2:
            error_msg = f"Pattern '{pattern}' is not robustly supported by multiple quotations"
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_research_objective_alignment(quotations: List[Dict[str, Any]], research_objectives: str) -> None:
    """
    Ensure quotations align with research objectives.
    According to the paper: "The evaluation objectives provide a focus or domain of relevance
    for conducting the analysis"

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata
        research_objectives (str): Research objectives guiding the analysis

    Raises:
        AssertionError: If quotations don't align with objectives
    """
    for quote in quotations:
        relevance = quote.get('analysis_value', {}).get('relevance', '')
        if not relevance or not any(obj.lower() in relevance.lower() for obj in research_objectives.split('.')):
            error_msg = f"Quotation does not align with research objectives: '{quote.get('quotation', '')[:50]}...'"
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_selective_transcription(quotations: List[Dict[str, Any]], transcript: str) -> None:
    """
    Ensure quotations are selectively chosen for relevance.
    According to the paper: "A more useful transcript is a more selective one" and
    "selecting parts relevant to the evaluation objectives"

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata
        transcript (str): Original transcript text

    Raises:
        AssertionError: If quotation selection isn't properly selective
    """
    total_words = len(transcript.split())
    quoted_words = sum(len(quote.get('quotation', '').split()) for quote in quotations)

    # Check if quotations are too verbose (should be selective)
    if quoted_words > total_words * 0.3:  # Maximum 30% of original text
        error_msg = "Quotation selection is not selective enough"
        logger.error(error_msg)
        raise AssertionError(error_msg)

def assert_creswell_categorization(quotations: List[Dict[str, Any]]) -> None:
    """
    Verify proper use of Creswell's quotation categories.
    According to the paper: "Creswell (2012) classified quotations into three types:
    discrete, embedded, and longer quotations"

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata

    Raises:
        AssertionError: If quotations don't follow Creswell's guidelines
    """
    categories = {'longer': 0, 'discrete': 0, 'embedded': 0}

    for quote in quotations:
        category = quote.get("creswell_category", "").lower()
        if category not in categories:
            error_msg = f"Invalid Creswell category '{category}'"
            logger.error(error_msg)
            raise AssertionError(error_msg)

        quote_length = len(quote.get("quotation", "").split())

        # Length guidelines based on Creswell's classifications
        if category == "longer" and quote_length < 40:
            error_msg = f"'Longer' quotation is too short ({quote_length} words)"
            logger.error(error_msg)
            raise AssertionError(error_msg)
        elif category == "discrete" and quote_length > 30:
            error_msg = f"'Discrete' quotation is too long ({quote_length} words)"
            logger.error(error_msg)
            raise AssertionError(error_msg)
        elif category == "embedded" and quote_length > 10:
            error_msg = f"'Embedded' quotation is too long ({quote_length} words)"
            logger.error(error_msg)
            raise AssertionError(error_msg)

        categories[category] += 1

def assert_reader_engagement(quotations: List[Dict[str, Any]]) -> None:
    """
    Ensure quotations enhance reader engagement.
    According to the paper: "Quotations can enhance the readers' engagement with the text"
    and should not be chosen merely to "incite controversy"

    Args:
        quotations (List[Dict[str, Any]]): List of quotations with metadata

    Raises:
        AssertionError: If quotations don't promote proper engagement
    """
    for quote in quotations:
        # Check for essential engagement elements
        has_context = bool(quote.get("context", {}).get("situation"))
        has_interpretation = bool(quote.get("analysis_value", {}).get("relevance"))
        has_pattern = bool(quote.get("context", {}).get("pattern_representation"))

        if not all([has_context, has_interpretation, has_pattern]):
            error_msg = "Quotation lacks essential engagement elements (context, interpretation, or pattern connection)"
            logger.error(error_msg)
            raise AssertionError(error_msg)

        # Check against controversial selection without substance
        quote_text = quote.get("quotation", "")
        if "!" in quote_text and not has_interpretation:
            error_msg = "Quotation appears selected for controversy without substantive contribution"
            logger.error(error_msg)
            raise AssertionError(error_msg)


# File: assertions_alt.py
#------------------------------------------------------------------------------
# src/assertions.py
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def assert_relevant_quotations(quotations: List[Dict[str, Any]], themes: List[str]) -> None:
    """
    Ensure that each quotation is relevant to at least one identified theme.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        themes (List[str]): List of identified themes in the analysis.
    
    Raises:
        AssertionError: If a quotation does not align with any theme.
    """
    for quote in quotations:
        if not any(theme.lower() in [t.lower() for t in quote.get('themes', [])] for theme in themes):
            error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with any identified theme."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_confidentiality(quotations: List[Dict[str, Any]], sensitive_keywords: List[str]) -> None:
    """
    Ensure that no quotations contain sensitive or identifiable information.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        sensitive_keywords (List[str]): List of keywords that should not appear in quotations.
    
    Raises:
        AssertionError: If a quotation contains sensitive information.
    """
    for quote in quotations:
        quote_text = quote.get("quotation", "").lower()
        for keyword in sensitive_keywords:
            if keyword.lower() in quote_text:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' contains sensitive keyword '{keyword}'."
                logger.error(error_msg)
                raise AssertionError(error_msg)

def assert_diversity_of_quotations(quotations: List[Dict[str, Any]], min_participants: int = 3) -> None:
    """
    Ensure that quotations represent a diverse set of participants.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        min_participants (int): Minimum number of different participants required.
    
    Raises:
        AssertionError: If quotations do not represent the required diversity.
    """
    participant_ids = {q.get("participant_id") for q in quotations}
    if len(participant_ids) < min_participants:
        error_msg = f"Only {len(participant_ids)} unique participants are represented in quotations; minimum required is {min_participants}."
        logger.error(error_msg)
        raise AssertionError(error_msg)

def assert_contextual_adequacy(quotations: List[Dict[str, Any]], transcript_chunks: List[str]) -> None:
    """
    Ensure that each quotation has adequate context.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        transcript_chunks (List[str]): List of transcript chunks.
    
    Raises:
        AssertionError: If a quotation lacks necessary context or is not found in transcript.
    """
    for quote in quotations:
        quote_text = quote.get("quotation", "")
        context = quote.get("context", "").strip()
        # Check if the quote exists in any of the transcript chunks
        in_transcript = any(quote_text in chunk for chunk in transcript_chunks)
        if not in_transcript:
            error_msg = f"Quotation '{quote_text[:50]}...' not found in any transcript chunk."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        if not context:
            error_msg = f"Quotation '{quote_text[:50]}...' lacks contextual information."
            logger.error(error_msg)
            raise AssertionError(error_msg)

def assert_philosophical_alignment(quotations: List[Dict[str, Any]], theoretical_framework: str) -> None:
    """
    Ensure that quotations align with the researcher's philosophical stance.
    
    Args:
        quotations (List[Dict[str, Any]]): List of quotations with associated metadata.
        theoretical_framework (str): The theoretical and philosophical framework guiding the analysis.
    
    Raises:
        AssertionError: If any quotation does not align with the specified orientation.
    """
    framework_lower = theoretical_framework.lower()
    for quote in quotations:
        alignment_score = float(quote.get("alignment_score", 0.0))
        analysis_notes = quote.get("analysis_notes", "").lower()
        if "constructivist" in framework_lower:
            if alignment_score < 0.7 or "subjective" not in analysis_notes:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with constructivist orientation."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        elif "critical realism" in framework_lower:
            if alignment_score < 0.7 or "objective" not in analysis_notes:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' does not align with critical realism orientation."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        else:
            if alignment_score < 0.7:
                error_msg = f"Quotation '{quote.get('quote', '')[:50]}...' has insufficient alignment score."
                logger.error(error_msg)
                raise AssertionError(error_msg)


# File: assertions_coding.py
#------------------------------------------------------------------------------
# src/assertions_coding.py

import logging
from typing import List, Dict, Any, Set

logger = logging.getLogger(__name__)

def assert_robustness(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure coding accurately and comprehensively captures the essence of the data.

    Raises:
        AssertionError: If any code does not comprehensively represent the data.
    """
    for code in codes:
        data_extracts = code.get("data_extracts", [])
        if not data_extracts:
            error_msg = f"Code '{code.get('code_name', '')}' has no data extracts."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        # Example metric: keyword diversity (assuming each extract has 'keywords')
        keyword_set: Set[str] = set()
        for extract in data_extracts:
            keywords = extract.get("keywords", [])
            keyword_set.update([kw.lower() for kw in keywords])
        
        if len(keyword_set) < 2:
            error_msg = f"Code '{code.get('code_name', '')}' lacks keyword diversity."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code.get('code_name', '')}' passes robustness check.")

def assert_reflectiveness(codes: List[Dict[str, Any]], theoretical_framework: Dict[str, str]) -> None:
    """
    Ensure codes reflect the relationship between data and the theoretical framework.

    Raises:
        AssertionError: If any code does not align with the theoretical framework.
    """
    framework_concepts = set(k.lower() for k in theoretical_framework.keys())
    
    for code in codes:
        data_extracts = code.get("data_extracts", [])
        aligned = False
        for extract in data_extracts:
            keywords = extract.get("keywords", [])
            if any(kw.lower() in framework_concepts for kw in keywords):
                aligned = True
                break
        if not aligned:
            error_msg = f"Code '{code.get('code_name', '')}' does not align with the theoretical framework."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code.get('code_name', '')}' passes reflectiveness check.")

def assert_resplendence(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes offer rich and comprehensive explanations of the context.

    Raises:
        AssertionError: If any code lacks comprehensive contextual explanations.
    """
    for code in codes:
        analysis_notes = code.get("analysis_notes", "")
        if not analysis_notes or len(analysis_notes.split()) < 50:
            error_msg = f"Code '{code.get('code_name', '')}' lacks comprehensive analysis notes."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code.get('code_name', '')}' passes resplendence check.")

def assert_relevance(codes: List[Dict[str, Any]], research_objectives: str) -> None:
    """
    Ensure codes accurately represent the data and align with research objectives.

    Raises:
        AssertionError: If any code does not align with research objectives.
    """
    objectives = [obj.strip().lower() for obj in research_objectives.split('.') if obj.strip()]
    if not objectives:
        error_msg = "No research objectives provided for relevance check."
        logger.error(error_msg)
        raise AssertionError(error_msg)
    
    for code in codes:
        code_name = code.get("code_name", "")
        aligns = False
        data_extracts = code.get("data_extracts", [])
        for extract in data_extracts:
            relevance = extract.get("relevance", "")
            if any(obj in relevance.lower() for obj in objectives):
                aligns = True
                break
        if not aligns:
            error_msg = f"Code '{code_name}' does not align with any research objectives."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes relevance check.")

def assert_radicality(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes provide unique insights and may challenge dominant narratives.

    Raises:
        AssertionError: If any code lacks uniqueness or does not offer fresh perspectives.
    """
    seen_codes = set()
    for code in codes:
        code_name = code.get("code_name", "").lower()
        if code_name in seen_codes:
            error_msg = f"Code '{code.get('code_name', '')}' is not unique."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        seen_codes.add(code_name)
        
        data_extracts = code.get("data_extracts", [])
        unique_insights = set()
        for extract in data_extracts:
            insight = extract.get("insight", "").lower()
            if insight:
                unique_insights.add(insight)
        
        if len(unique_insights) < 1:
            error_msg = f"Code '{code.get('code_name', '')}' does not offer unique insights."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code.get('code_name', '')}' passes radicality check.")

def assert_righteousness(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes fit logically within the coding framework and avoid overlaps.

    Raises:
        AssertionError: If any code overlaps conceptually with others or lacks logical consistency.
    """
    code_definitions = {code.get("code_name", "").lower(): code for code in codes}
    
    for code_name, code in code_definitions.items():
        related_codes = code.get("related_codes", [])
        for related_code in related_codes:
            related_code_lower = related_code.lower()
            if related_code_lower not in code_definitions:
                error_msg = f"Related code '{related_code}' for code '{code_name}' does not exist."
                logger.error(error_msg)
                raise AssertionError(error_msg)
            
            # Example overlap check: shared keywords
            code_keywords = set(kw.lower() for extract in code.get("data_extracts", []) for kw in extract.get("keywords", []))
            related_keywords = set(kw.lower() for extract in code_definitions[related_code_lower].get("data_extracts", []) for kw in extract.get("keywords", []))
            overlap = code_keywords.intersection(related_keywords)
            if len(overlap) > 5:  # Threshold for overlap
                error_msg = f"Code '{code_name}' overlaps significantly with related code '{related_code}'."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes righteousness check.")

def assert_code_representation(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure each code accurately encapsulates underlying concepts and meanings.

    Raises:
        AssertionError: If any code does not consistently reflect its definition.
    """
    for code in codes:
        code_name = code.get("code_name", "")
        definition = code.get("definition", "").lower()
        data_extracts = code.get("data_extracts", [])
        
        for extract in data_extracts:
            quotation = extract.get("quotation", "").lower()
            if definition not in quotation:
                error_msg = f"Data extract in code '{code_name}' does not reflect the code's definition."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes code representation check.")

def assert_code_specificity(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes are specific enough to convey clear meanings within the theoretical framework.

    Raises:
        AssertionError: If any code is too vague or broad.
    """
    for code in codes:
        code_name = code.get("code_name", "")
        if len(code_name.split()) < 2:
            error_msg = f"Code name '{code_name}' is too broad or vague."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        data_extracts = code.get("data_extracts", [])
        for extract in data_extracts:
            keywords = extract.get("keywords", [])
            for kw in keywords:
                if len(kw.split()) < 2:
                    error_msg = f"Keyword '{kw}' in code '{code_name}' is too broad or vague."
                    logger.error(error_msg)
                    raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes specificity check.")

def assert_code_relevance(codes: List[Dict[str, Any]], research_objectives: str) -> None:
    """
    Ensure each code aligns with research objectives and contributes to answering research questions.

    Raises:
        AssertionError: If any code does not contribute to the research objectives.
    """
    objectives = [obj.strip().lower() for obj in research_objectives.split('.') if obj.strip()]
    if not objectives:
        error_msg = "No research objectives provided for code relevance check."
        logger.error(error_msg)
        raise AssertionError(error_msg)
    
    for code in codes:
        code_name = code.get("code_name", "")
        aligns = False
        data_extracts = code.get("data_extracts", [])
        for extract in data_extracts:
            relevance = extract.get("relevance", "")
            if any(obj in relevance.lower() for obj in objectives):
                aligns = True
                break
        if not aligns:
            error_msg = f"Code '{code_name}' does not contribute to any research objectives."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Code '{code_name}' passes code relevance check.")

def assert_code_distinctiveness(codes: List[Dict[str, Any]]) -> None:
    """
    Ensure codes are distinct and do not overlap or duplicate each other.

    Raises:
        AssertionError: If any codes are too similar or duplicated.
    """
    seen_definitions = {}
    for code in codes:
        code_name = code.get("code_name", "").lower()
        definition = code.get("definition", "").lower()
        if definition in seen_definitions:
            error_msg = f"Code '{code_name}' has a duplicate definition with code '{seen_definitions[definition]}'."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        seen_definitions[definition] = code_name
        logger.debug(f"Code '{code_name}' is distinct.")

def run_all_coding_assertions(
    codes: List[Dict[str, Any]],
    research_objectives: str,
    theoretical_framework: Dict[str, str]
) -> None:
    """
    Run all coding assertions to validate the coding framework.

    Args:
        codes (List[Dict[str, Any]]): List of codes with metadata.
        research_objectives (str): Research objectives guiding the analysis.
        theoretical_framework (Dict[str, str]): Theoretical framework for context.

    Raises:
        AssertionError: If any of the assertions fail.
    """
    assert_robustness(codes)
    assert_reflectiveness(codes, theoretical_framework)
    assert_resplendence(codes)
    assert_relevance(codes, research_objectives)
    assert_radicality(codes)
    assert_righteousness(codes)
    assert_code_representation(codes)
    assert_code_specificity(codes)
    assert_code_relevance(codes, research_objectives)
    assert_code_distinctiveness(codes)
    logger.info("All coding assertions passed successfully.")


# File: assertions_keyword.py
#------------------------------------------------------------------------------
# src/assertions_keyword.py

import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def assert_keywords_not_exclusive_to_quotation(
    keywords: List[Dict[str, Any]], 
    quotation: str, 
    contextualized_contents: List[str]
) -> None:
    """
    Assert that keywords capture concepts beyond verbatim quotes.
    
    According to the guideline: "Keywords should represent broader concepts, not just exact phrases from quotations."

    Args:
        keywords (List[Dict[str, Any]]): List of extracted keywords with their analysis.
        quotation (str): The original quotation.
        contextualized_contents (List[str]): Additional contextual content for analysis.
    
    Raises:
        AssertionError: If any keyword is too closely tied to verbatim quotes.
    """
    for keyword in keywords:
        keyword_text = keyword.get("keyword", "").strip().lower()
        if not keyword_text:
            error_msg = "Keyword is empty or missing."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        if keyword_text in quotation.lower():
            # Check if the keyword is supported by contextualized contents
            if not any(keyword_text in context.lower() for context in contextualized_contents):
                error_msg = f"Keyword '{keyword_text}' appears to be too closely tied to the quotation."
                logger.error(error_msg)
                raise AssertionError(error_msg)
            else:
                logger.debug(f"Keyword '{keyword_text}' is adequately supported by contextual content.")

def assert_keyword_specificity(
    keywords: List[Dict[str, Any]], 
    theoretical_framework: Dict[str, str]
) -> None:
    """
    Assert that keywords are sufficiently specific and meaningful.
    
    According to the guideline: "Keywords should be specific enough to convey meaningful concepts within the theoretical framework."

    Args:
        keywords (List[Dict[str, Any]]): List of extracted keywords with their analysis.
        theoretical_framework (Dict[str, str]): Theoretical framework for context.
    
    Raises:
        AssertionError: If any keyword is too broad or vague.
    """
    for keyword in keywords:
        keyword_text = keyword.get("keyword", "").strip()
        if not keyword_text:
            error_msg = "Keyword is empty or missing."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        # Example check: keyword length (can be customized based on requirements)
        if len(keyword_text.split()) < 2:
            error_msg = f"Keyword '{keyword_text}' is too broad or vague."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        logger.debug(f"Keyword '{keyword_text}' meets specificity requirements.")

def assert_keyword_relevance(
    keywords: List[Dict[str, Any]], 
    research_objectives: str,
    contextualized_contents: List[str]
) -> None:
    """
    Assert that keywords are relevant to the research objectives.
    
    According to the guideline: "Keywords should align closely with the research objectives to ensure focused analysis."

    Args:
        keywords (List[Dict[str, Any]]): List of extracted keywords with their analysis.
        research_objectives (str): The research objectives.
        contextualized_contents (List[str]): Additional contextual content for analysis.
    
    Raises:
        AssertionError: If any keyword is not sufficiently relevant to the research objectives.
    """
    objectives = [obj.strip().lower() for obj in research_objectives.split('.') if obj.strip()]
    if not objectives:
        error_msg = "No research objectives provided for relevance check."
        logger.error(error_msg)
        raise AssertionError(error_msg)
    
    for keyword in keywords:
        keyword_text = keyword.get("keyword", "").strip().lower()
        if not keyword_text:
            error_msg = "Keyword is empty or missing."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        # Check if keyword aligns with any research objective
        if not any(obj in keyword_text for obj in objectives):
            # Additionally, check if keyword appears in contextualized contents
            if not any(keyword_text in context.lower() for context in contextualized_contents):
                error_msg = f"Keyword '{keyword_text}' is not sufficiently relevant to the research objectives."
                logger.error(error_msg)
                raise AssertionError(error_msg)
        
        logger.debug(f"Keyword '{keyword_text}' is relevant to the research objectives.")

def assert_keyword_distinctiveness(
    keywords: List[Dict[str, Any]]
) -> None:
    """
    Assert that keywords are sufficiently distinct from one another.
    
    According to the guideline: "Keywords should be unique and not redundant to ensure comprehensive coverage of concepts."

    Args:
        keywords (List[Dict[str, Any]]): List of extracted keywords with their analysis.
    
    Raises:
        AssertionError: If any keywords are too similar or duplicated.
    """
    seen_keywords = set()
    for keyword in keywords:
        keyword_text = keyword.get("keyword", "").strip().lower()
        if not keyword_text:
            error_msg = "Keyword is empty or missing."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        if keyword_text in seen_keywords:
            error_msg = f"Keyword '{keyword_text}' is duplicated and not distinct."
            logger.error(error_msg)
            raise AssertionError(error_msg)
        
        seen_keywords.add(keyword_text)
        logger.debug(f"Keyword '{keyword_text}' is distinct.")



# File: clearcache.py
#------------------------------------------------------------------------------
import os
import shutil
import logging
from elasticsearch import Elasticsearch
from typing import List, Optional
import json

def setup_logging() -> logging.Logger:
    """Configure and return a logger instance."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def clear_elasticsearch_cache(logger: logging.Logger, es_host: str = "http://localhost:9200") -> None:
    """Clear Elasticsearch indices."""
    try:
        es = Elasticsearch(es_host)
        if es.ping():
            indices_to_delete = ["contextual_bm25_index"]
            for index in indices_to_delete:
                if es.indices.exists(index=index):
                    es.indices.delete(index=index)
                    logger.info(f"Deleted Elasticsearch index: {index}")
            logger.info("Elasticsearch cache cleared successfully")
    except Exception as e:
        logger.error(f"Error clearing Elasticsearch cache: {e}")

def remove_file_or_directory(path: str, logger: logging.Logger) -> None:
    """Remove a file or directory with proper error handling."""
    try:
        if os.path.isfile(path):
            os.remove(path)
            logger.info(f"Removed file: {path}")
        elif os.path.isdir(path):
            shutil.rmtree(path, ignore_errors=True)
            logger.info(f"Removed directory: {path}")
    except Exception as e:
        logger.error(f"Error removing {path}: {e}")

def clear_pickle_files(data_dir: str, logger: logging.Logger) -> None:
    """Remove all pickle files in the specified directory and its subdirectories."""
    try:
        for root, _, files in os.walk(data_dir):
            for file in files:
                if file.endswith((".pkl", ".pickle")):
                    file_path = os.path.join(root, file)
                    os.remove(file_path)
                    logger.info(f"Removed pickle file: {file_path}")
    except Exception as e:
        logger.error(f"Error clearing pickle files: {e}")

def clear_temporary_files(data_dir: str, logger: logging.Logger) -> None:
    """Remove temporary files like .pyc, .pyo, and __pycache__ directories."""
    try:
        # First, collect all paths to remove
        cache_dirs = []
        compiled_files = []
        
        for root, dirs, files in os.walk(data_dir, topdown=True):
            # Collect __pycache__ directories
            if "__pycache__" in dirs:
                cache_dir = os.path.join(root, "__pycache__")
                cache_dirs.append(cache_dir)
                dirs.remove("__pycache__")  # Prevent recursing into __pycache__
            
            # Collect .pyc and .pyo files
            for file in files:
                if file.endswith((".pyc", ".pyo")):
                    file_path = os.path.join(root, file)
                    compiled_files.append(file_path)
        
        # Remove collected paths
        for cache_dir in cache_dirs:
            if os.path.exists(cache_dir):  # Check again in case it was already removed
                shutil.rmtree(cache_dir, ignore_errors=True)
                logger.info(f"Removed __pycache__ directory: {cache_dir}")
        
        for file_path in compiled_files:
            if os.path.exists(file_path):  # Check again in case it was already removed
                os.remove(file_path)
                logger.info(f"Removed compiled Python file: {file_path}")
                
    except Exception as e:
        logger.error(f"Error clearing temporary files: {e}")

def create_directories(directories: List[str], logger: logging.Logger) -> None:
    """Create necessary directories."""
    for directory in directories:
        try:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"Created directory: {directory}")
        except Exception as e:
            logger.error(f"Error creating directory {directory}: {e}")

def clear_model_cache(cache_dir: str, logger: logging.Logger) -> None:
    """Clear any cached model files or artifacts."""
    try:
        model_paths = [
            os.path.join(cache_dir, "models"),
            os.path.join(cache_dir, "optimized_models"),
            os.path.join(cache_dir, "checkpoints")
        ]
        for path in model_paths:
            if os.path.exists(path):
                shutil.rmtree(path, ignore_errors=True)
                logger.info(f"Cleared model cache: {path}")
    except Exception as e:
        logger.error(f"Error clearing model cache: {e}")

def clear_output_files(output_dir: str, logger: logging.Logger) -> None:
    """Clear generated output files."""
    try:
        # Clear JSON output files
        for root, _, files in os.walk(output_dir):
            for file in files:
                if file.endswith((".json", ".jsonl")):
                    file_path = os.path.join(root, file)
                    os.remove(file_path)
                    logger.info(f"Removed output file: {file_path}")
    except Exception as e:
        logger.error(f"Error clearing output files: {e}")

def clear_all_cache(base_dir: Optional[str] = None) -> None:
    """Main function to clear all cache and temporary files."""
    logger = setup_logging()
    
    if base_dir is None:
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    logger.info(f"Starting cache clearing process in: {base_dir}")
    
    # Define paths relative to base directory
    data_dir = os.path.join(base_dir, "data")
    cache_dir = os.path.join(base_dir, "cache")
    logs_dir = os.path.join(base_dir, "logs")
    output_dir = os.path.join(base_dir, "output")
    
    # 1. Clear Elasticsearch cache
    clear_elasticsearch_cache(logger)
    
    # 2. Clear data directories
    data_paths = [
        os.path.join(data_dir, "contextual_db"),
        os.path.join(data_dir, "optimized_program.json"),
        logs_dir,
        cache_dir,
        os.path.join(data_dir, "contextual_db", "faiss_index.bin"),
        output_dir
    ]
    
    for path in data_paths:
        remove_file_or_directory(path, logger)
    
    # 3. Clear pickle files
    clear_pickle_files(data_dir, logger)
    
    # 4. Clear temporary Python files from the entire project directory
    clear_temporary_files(base_dir, logger)
    
    # 5. Clear model cache
    clear_model_cache(cache_dir, logger)
    
    # 6. Clear output files
    clear_output_files(output_dir, logger)
    
    # 7. Recreate necessary directories
    required_dirs = [
        data_dir,
        os.path.join(data_dir, "contextual_db"),
        logs_dir,
        cache_dir,
        output_dir
    ]
    create_directories(required_dirs, logger)
    
    logger.info("Cache clearing completed successfully")

if __name__ == "__main__":
    clear_all_cache()


################################################################################
# Module: convert
################################################################################


# File: convert/convertcodingfortheme.py
#------------------------------------------------------------------------------
# convertcodingfortheme.py

import json
import os
from typing import List, Dict, Any

def extract_coding_info(entry: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extracts and simplifies coding information from a single JSON entry.
    """
    try:
        coding_info = entry.get('coding_info', {})
        quotation = coding_info.get('quotation', '').strip()
        keywords = coding_info.get('keywords', [])
        research_objectives = coding_info.get('research_objectives', '').strip()
        theoretical_framework = coding_info.get('theoretical_framework', {})

        # Extract codes
        codes = entry.get('codes', [])
        coding = [code_entry.get('code', '').strip() for code_entry in codes if 'code' in code_entry]

        # Combine transcript chunks
        retrieved_chunks = entry.get('retrieved_chunks', [])
        transcript_contents = [chunk.get('original_content', '').strip() for chunk in retrieved_chunks]
        transcript_chunk = ' '.join([quotation] + transcript_contents)

        simplified_entry = {
            "quotation": quotation,
            "keywords": keywords,
            "coding": coding,
            "research_objectives": research_objectives,
            "theoretical_framework": theoretical_framework,
            "transcript_chunk": transcript_chunk
        }

        return simplified_entry

    except Exception as e:
        print(f"Error processing entry: {e}")
        return {}

def process_input_file(input_file: str, output_dir: str, output_file: str):
    """
    Processes the input JSON file to extract and simplify coding information.
    """
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    try:
        # Read the input JSON file
        with open(input_file, 'r', encoding='utf-8') as f:
            input_data = json.load(f)

        # Check if input_data is a list; if not, make it a list
        if not isinstance(input_data, list):
            input_data = [input_data]

        simplified_data: List[Dict[str, Any]] = []

        for idx, entry in enumerate(input_data):
            simplified_entry = extract_coding_info(entry)
            if simplified_entry:
                simplified_data.append(simplified_entry)
            else:
                print(f"Entry at index {idx} could not be processed and was skipped.")

        # Define the output path
        output_path = os.path.join(output_dir, output_file)

        # Write the simplified data to the output JSON file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(simplified_data, f, indent=4, ensure_ascii=False)

        print(f"Successfully processed and saved to {output_path}")

    except FileNotFoundError:
        print(f"Error: The file {input_file} was not found.")
    except json.JSONDecodeError as jde:
        print(f"Error decoding JSON: {jde}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    # Example usage; this will only run when the script is executed directly
    process_input_file(
        input_file='query_results_coding_analysis.json',  # Replace with your input file path
        output_dir='data/input',                  # Replace with your desired output directory
        output_file='queries_theme.json'  # Replace with your desired output file name
    )


# File: convert/convertkeywordforcoding.py
#------------------------------------------------------------------------------
# convertkeywordforcoding.py

import json
import os
import re
from typing import List, Dict, Any

def split_into_sentences(text: str) -> List[str]:
    """
    Splits a given text into sentences using regular expressions.
    """
    sentence_endings = re.compile(r'(?<=[.!?]) +')
    sentences = sentence_endings.split(text.strip())
    return [sentence.strip() for sentence in sentences if sentence.strip()]

def extract_keywords(keywords_list: List[Dict[str, Any]]) -> List[str]:
    """
    Extracts the keyword strings from the keywords list.
    """
    return [keyword_entry.get('keyword', '') for keyword_entry in keywords_list if 'keyword' in keyword_entry]

def convert_query_results(input_file: str, output_dir: str, output_file: str):
    """
    Convert query results to a simplified format and save them in the specified directory.
    """
    # Ensure the output directory exists
    os.makedirs(os.path.join(output_dir, 'input'), exist_ok=True)

    try:
        # Read the input JSON file
        with open(input_file, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
        
        simplified_data: List[Dict[str, Any]] = []

        for entry in input_data:
            quotation_info = entry.get('quotation_info', {})
            full_quotation = quotation_info.get('quotation', '')
            research_objectives = quotation_info.get('research_objectives', '')
            theoretical_framework = quotation_info.get('theoretical_framework', {})
            transcript_chunk = full_quotation

            # Split the full quotation into sentences
            sentences = split_into_sentences(full_quotation)

            # Extract keywords
            keywords = extract_keywords(entry.get('keywords', []))

            for sentence in sentences:
                simplified_entry = {
                    "quotation": sentence,
                    "keywords": keywords,
                    "research_objectives": research_objectives,
                    "theoretical_framework": theoretical_framework,
                    "transcript_chunk": transcript_chunk
                }
                simplified_data.append(simplified_entry)

        # Define the output path
        output_path = os.path.join(output_dir, 'input', output_file)

        # Write the simplified data to the output JSON file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(simplified_data, f, indent=4, ensure_ascii=False)

        print(f"Successfully converted and saved to {output_path}")

    except FileNotFoundError:
        print(f"Error: The file {input_file} was not found.")
    except json.JSONDecodeError as jde:
        print(f"Error decoding JSON: {jde}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    # Example usage; this will only run when the script is executed directly
    convert_query_results(
        input_file='query_results_keyword_extraction.json',
        output_dir='data',
        output_file='queries_coding.json'
    )


# File: convert/convertquotationforkeyword.py
#------------------------------------------------------------------------------
# convertquotationforkeyword.py
import json
import os
from typing import List, Dict, Any

def convert_query_results(input_file: str, output_dir: str, output_file: str):
    """
    Convert query results to a simplified format and save them in the specified directory.
    
    The simplified format includes:
    - quotation: The actual quotation text.
    - research_objectives: The research objectives from transcript_info.
    - theoretical_framework: The theoretical framework details from transcript_info (preserved as a nested dictionary).
    - transcript_chunk: (Optional) The full transcript chunk for context.
    
    If a result contains multiple quotations, each quotation will be a separate entry in the simplified data.
    """
    # Create output directory if it doesn't exist
    os.makedirs(os.path.join(output_dir, 'input'), exist_ok=True)

    try:
        # Read the query results file
        with open(input_file, 'r', encoding='utf-8') as f:
            results = json.load(f)

        # Convert to simplified format
        simplified_data: List[Dict[str, Any]] = []
        for result in results:
            transcript_info = result.get('transcript_info', {})
            research_objectives = transcript_info.get('research_objectives', '')
            theoretical_framework = transcript_info.get('theoretical_framework', {})
            transcript_chunk = transcript_info.get('transcript_chunk', '')

            quotations = result.get('quotations', [])
            for quotation_entry in quotations:
                quotation = quotation_entry.get('quotation', '')
                simplified_entry = {
                    "quotation": quotation,
                    "research_objectives": research_objectives,
                    "theoretical_framework": theoretical_framework,  # Preserved as a nested dictionary
                    "transcript_chunk": transcript_chunk  # Optional: Include for additional context
                }
                simplified_data.append(simplified_entry)

        # Save to output file
        output_path = os.path.join(output_dir, 'input', output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(simplified_data, f, indent=4, ensure_ascii=False)

        print(f"Successfully converted and saved to {output_path}")

    except Exception as e:
        print(f"Error converting file: {e}")

if __name__ == "__main__":
    # Example usage; this will only run when the script is executed directly
    convert_query_results(
        input_file='query_results_quotation.json',
        output_dir='data',
        output_file='queries_keyword.json'
    )



################################################################################
# Module: root
################################################################################


# File: copymd.py
#------------------------------------------------------------------------------
import os
import shutil
from pathlib import Path
import sys
from datetime import datetime

def setup_backup_directory() -> Path:
    """
    Create a new backup directory with timestamp in the same folder as the script.
    
    Returns:
        Path: Path to the created backup directory
    """
    script_dir = Path(__file__).parent
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = script_dir / f"md_backup_{timestamp}"
    backup_dir.mkdir(parents=True, exist_ok=True)
    print(f"Created directory: {backup_dir}")
    return backup_dir

def get_flattened_name(file_path: Path) -> str:
    """
    Generate a flattened filename based on the file's path.
    Example: docs/models/arch.md becomes models_arch.md
    
    Args:
        file_path (Path): Original file path
    
    Returns:
        str: New flattened filename
    """
    # Get the path relative to the docs directory
    rel_parts = file_path.parent.parts[file_path.parent.parts.index('docs')+1:]
    if rel_parts:
        # Join all directory names with underscore and append the filename
        return f"{'_'.join(rel_parts)}_{file_path.name}"
    return file_path.name

def copy_markdown_files(dest_dir: Path) -> tuple[int, list[str]]:
    """
    Copy all Markdown files from docs directory to destination directory with flattened names.
    
    Args:
        dest_dir (Path): Path to destination directory
    
    Returns:
        tuple[int, list[str]]: Number of files copied and list of any errors encountered
    """
    script_dir = Path(__file__).parent
    docs_path = script_dir / 'docs'
    
    if not docs_path.exists():
        return 0, [f"Source docs directory '{docs_path}' does not exist"]
    
    files_copied = 0
    errors = []
    
    try:
        # Track used names to handle potential duplicates
        used_names = set()
        
        for md_file in docs_path.rglob("*.md"):
            try:
                # Generate new flattened name
                new_name = get_flattened_name(md_file)
                
                # Handle potential name collisions
                base_name = new_name
                counter = 1
                while new_name in used_names:
                    name_parts = base_name.rsplit('.', 1)
                    new_name = f"{name_parts[0]}_{counter}.{name_parts[1]}"
                    counter += 1
                
                used_names.add(new_name)
                
                # Copy the file with new name
                dest_file = dest_dir / new_name
                shutil.copy2(md_file, dest_file)
                
                # Print original path -> new name
                rel_path = md_file.relative_to(docs_path)
                print(f"Copied: {rel_path} -> {new_name}")
                files_copied += 1
                
            except Exception as e:
                errors.append(f"Error copying {md_file.name}: {str(e)}")
    
    except Exception as e:
        errors.append(f"Error accessing directory: {str(e)}")
    
    return files_copied, errors

def main():
    # Create destination directory with timestamp
    dest_dir = setup_backup_directory()
    
    # Copy files and handle results
    files_copied, errors = copy_markdown_files(dest_dir)
    
    # Print summary
    print(f"\nFiles copied: {files_copied}")
    if errors:
        print("\nErrors encountered:")
        for error in errors:
            print(f"- {error}")
        
        if files_copied == 0:
            try:
                dest_dir.rmdir()
                print(f"\nRemoved empty directory: {dest_dir}")
            except:
                pass
        sys.exit(1)
    
    if files_copied == 0:
        print("\nNo markdown files found in docs directory.")
        try:
            dest_dir.rmdir()
            print(f"Removed empty directory: {dest_dir}")
        except:
            pass
        sys.exit(1)
    
    print(f"\nFiles successfully copied to: {dest_dir}")
    sys.exit(0)

if __name__ == "__main__":
    main()


################################################################################
# Module: core
################################################################################


# File: core/__init__.py
#------------------------------------------------------------------------------
"""
Core functionality for the thematic analysis package.
Contains database and client implementations.
"""

from .contextual_vector_db import ContextualVectorDB
from .elasticsearch_bm25 import ElasticsearchBM25
from .openai_client import OpenAIClient

__all__ = ['ContextualVectorDB', 'ElasticsearchBM25', 'OpenAIClient']

# File: core/contextual_vector_db.py
#------------------------------------------------------------------------------
import os
import pickle
import numpy as np
import threading
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from dotenv import load_dotenv
import logging
import faiss
import dspy
import time

from src.core.openai_client import OpenAIClient
from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)


class SituateContextSignature(dspy.Signature):
    doc = dspy.InputField(desc="Full document content")
    chunk = dspy.InputField(desc="Specific chunk content")
    reasoning = dspy.OutputField(desc="Chain of thought reasoning")
    contextualized_content = dspy.OutputField(desc="Contextualized content for the chunk")


class SituateContext(dspy.Module):
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(SituateContextSignature)
        logger.debug("SituateContext module initialized.")

    def forward(self, doc: str, chunk: str):
        prompt = f"""
                <document>
                {doc}
                </document>
            

                CHUNK_CONTEXT_PROMPT = 
                Here is the chunk we want to situate within the whole document
                <chunk>
                {chunk}
                </chunk>

                Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
                Answer only with the succinct context and nothing else.
    """

        logger.debug("Generating contextualized content for a chunk.")
        return self.chain(doc=doc, chunk=chunk, prompt=prompt)


class ContextualVectorDB:
    def __init__(self, name: str, openai_api_key: str = None):
        if openai_api_key is None:
            load_dotenv()
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                logger.critical("OPENAI_API_KEY is not set in environment variables.")
                raise ValueError("OPENAI_API_KEY is required but not set.")

        self.name = name
        self.embeddings = []
        self.metadata = []
        self.db_path = f"./data/{name}/contextual_vector_db.pkl"
        self.faiss_index_path = f"./data/{name}/faiss_index.bin"

        self.client = OpenAIClient(api_key=openai_api_key)
        logger.debug(f"Initialized OpenAIClient for ContextualVectorDB '{self.name}'.")

        # Initialize FAISS index attribute
        self.index = None

    def situate_context(self, doc: str, chunk: str) -> Tuple[str, Any]:
        logger.debug(f"Entering situate_context with doc length={len(doc)} and chunk length={len(chunk)}.")
        try:
            if not hasattr(self, 'situate_context_module'):
                self.situate_context_module = SituateContext()
                logger.debug("Initialized SituateContext module.")

            start_time = time.time()
            response = self.situate_context_module(doc=doc, chunk=chunk)
            elapsed_time = time.time() - start_time
            logger.debug(f"Generated contextualized_content using DSPy in {elapsed_time:.2f} seconds.")

            contextualized_content = response.contextualized_content
            usage_metrics = {}  # Placeholder for actual usage metrics if available
            return contextualized_content, usage_metrics
        except Exception as e:
            logger.error(f"Error during DSPy situate_context: {e}", exc_info=True)
            return "", None

    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 8):
        logger.debug("Entering load_data method.")
        if self.embeddings and self.metadata and os.path.exists(self.faiss_index_path):
            logger.info("Vector database is already loaded. Skipping data loading.")
            return
        if os.path.exists(self.db_path) and os.path.exists(self.faiss_index_path):
            logger.info("Loading vector database and FAISS index from disk.")
            self.load_db()
            self.load_faiss_index()
            return

        texts_to_embed, metadata = self._process_dataset(dataset, parallel_threads)

        if not texts_to_embed:
            logger.warning("No texts to embed after processing the dataset.")
            return

        self._embed_and_store(texts_to_embed, metadata, max_workers=parallel_threads)
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}.")

    def _process_dataset(self, dataset: List[Dict[str, Any]], parallel_threads: int) -> Tuple[List[str], List[Dict[str, Any]]]:
        texts_to_embed = []
        metadata = []
        total_chunks = sum(len(doc.get('chunks', [])) for doc in dataset)
        logger.info(f"Total chunks to process: {total_chunks}.")

        logger.info(f"Processing {total_chunks} chunks with {parallel_threads} threads.")
        try:
            with ThreadPoolExecutor(max_workers=parallel_threads) as executor:
                futures = []
                for doc in dataset:
                    for chunk in doc.get('chunks', []):
                        futures.append(executor.submit(self._generate_contextualized_content, doc, chunk))

                for future in tqdm(as_completed(futures), total=total_chunks, desc="Processing chunks"):
                    result = future.result()
                    if result:
                        texts_to_embed.append(result['text_to_embed'])
                        metadata.append(result['metadata'])
            logger.debug(f"Completed processing all chunks.")
        except Exception as e:
            logger.error(f"Error during processing chunks: {e}", exc_info=True)
            return [], []

        return texts_to_embed, metadata

    def _generate_contextualized_content(self, doc: Dict[str, Any], chunk: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(doc, dict):
            logger.error(f"Document is not a dictionary: {doc}")
            return None

        if isinstance(chunk, dict):
            chunk_id = chunk.get('chunk_id')
            if not chunk_id:
                # Assign a unique chunk_id combining doc_id and chunk's index
                chunk_index = chunk.get('index', 0)
                chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_{chunk_index}"
                chunk['chunk_id'] = chunk_id
            content = chunk.get('content', '')
            original_index = chunk.get('original_index', chunk.get('index', 0))
        elif isinstance(chunk, str):
            # Handle case where chunk is a string
            content = chunk
            chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_0"
            original_index = 0
            logger.warning(f"Chunk is a string. Expected a dict. Assigning default values.")
        else:
            logger.error(f"Unsupported chunk type: {type(chunk)}. Skipping chunk.")
            return None

        logger.debug(f"Processing chunk_id='{chunk_id}' in doc_id='{doc.get('doc_id', 'unknown_doc_id')}'.")

        contextualized_text, usage = self.situate_context(doc.get('content', ''), content)
        if not contextualized_text:
            logger.warning(f"Contextualized content is empty for chunk_id='{chunk_id}'.")
            return None

        return {
            'text_to_embed': f"{content}\n\n{contextualized_text}",
            'metadata': {
                'doc_id': doc.get('doc_id', ''),
                'original_uuid': doc.get('original_uuid', ''),
                'chunk_id': chunk_id,
                'original_index': original_index,
                'original_content': content,
                'contextualized_content': contextualized_text
            }
        }

    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]], max_workers: int = 4):
        logger.debug("Entering _embed_and_store method.")
        batch_size = 128
        embeddings = []
        logger.info("Starting embedding generation.")

        # Define a helper function for embedding a batch
        def embed_batch(batch):
            try:
                logger.debug(f"Generating embeddings for batch of size {len(batch)}.")
                response = self.client.create_embeddings(
                    model="text-embedding-ada-002",
                    input=batch
                )
                return [item['embedding'] for item in response['data']]
            except Exception as e:
                logger.error(f"Error during OpenAI embeddings for batch: {e}", exc_info=True)
                return []

        # Use ThreadPoolExecutor for parallel embedding
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i: i + batch_size]
                futures.append(executor.submit(embed_batch, batch))
            
            for future in as_completed(futures):
                embeddings_batch = future.result()
                if embeddings_batch:
                    embeddings.extend(embeddings_batch)
                    logger.debug(f"Processed a batch with {len(embeddings_batch)} embeddings.")

        if not embeddings:
            logger.warning("No embeddings were generated.")
            return

        self.embeddings = embeddings
        self.metadata = data
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Embedding generation completed. Total embeddings: {len(self.embeddings)}.")

    def _build_faiss_index(self):
        logger.debug("Entering _build_faiss_index method.")
        start_time = time.time()
        self.create_faiss_index()
        self.save_faiss_index()
        elapsed_time = time.time() - start_time
        logger.info(f"FAISS index built and saved in {elapsed_time:.2f} seconds.")

    def create_faiss_index(self):
        logger.debug("Entering create_faiss_index method.")
        try:
            if not self.embeddings:
                logger.error("No embeddings available to create FAISS index.")
                raise ValueError("Embeddings list is empty.")

            embedding_dim = len(self.embeddings[0])
            logger.info(f"Embedding dimension: {embedding_dim}.")
            embeddings_np = np.array(self.embeddings).astype('float32')
            faiss.normalize_L2(embeddings_np)
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.index.add(embeddings_np)
            logger.info(f"FAISS index created with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error creating FAISS index: {e}", exc_info=True)
            raise

    def save_faiss_index(self):
        logger.debug("Entering save_faiss_index method.")
        os.makedirs(os.path.dirname(self.faiss_index_path), exist_ok=True)
        try:
            faiss.write_index(self.index, self.faiss_index_path)
            logger.info(f"FAISS index saved to '{self.faiss_index_path}'.")
        except Exception as e:
            logger.error(f"Error saving FAISS index to '{self.faiss_index_path}': {e}", exc_info=True)

    def load_faiss_index(self):
        logger.debug("Entering load_faiss_index method.")
        if not os.path.exists(self.faiss_index_path):
            logger.error(f"FAISS index file not found at '{self.faiss_index_path}'.")
            raise ValueError("FAISS index file not found.")
        try:
            self.index = faiss.read_index(self.faiss_index_path)
            logger.info(f"FAISS index loaded from '{self.faiss_index_path}' with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error loading FAISS index from '{self.faiss_index_path}': {e}", exc_info=True)
            raise

    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        logger.debug(f"Entering search method with query='{query}' and k={k}.")
        if not self.embeddings or not self.metadata:
            logger.error("Embeddings or metadata are not loaded. Cannot perform search.")
            return []
        if not hasattr(self, 'index') or self.index is None:
            logger.error("FAISS index is not loaded.")
            return []

        try:
            start_time = time.time()
            logger.debug("Generating embedding for the query.")
            response = self.client.create_embeddings(
                model="text-embedding-ada-002",
                input=[query]
            )
            query_embedding = response['data'][0]['embedding']
            elapsed_time = time.time() - start_time
            logger.debug(f"Generated embedding for query in {elapsed_time:.2f} seconds.")
        except Exception as e:
            logger.error(f"Error generating embedding for query '{query}': {e}", exc_info=True)
            return []

        query_embedding_np = np.array([query_embedding]).astype('float32')
        faiss.normalize_L2(query_embedding_np)

        logger.debug("Performing FAISS search.")
        try:
            start_time = time.time()
            distances, indices = self.index.search(query_embedding_np, k)
            elapsed_time = time.time() - start_time
            logger.debug(f"FAISS search completed in {elapsed_time:.2f} seconds.")
            indices = indices.flatten()
            distances = distances.flatten()
        except Exception as e:
            logger.error(f"Error during FAISS search: {e}", exc_info=True)
            return []

        top_results = []
        for idx, score in zip(indices, distances):
            if idx < len(self.metadata):
                meta = self.metadata[idx]
                result = {
                    "doc_id": meta['doc_id'],
                    "chunk_id": meta['chunk_id'],
                    "original_index": meta.get('original_index', 0),
                    "content": meta['original_content'],
                    "contextualized_content": meta.get('contextualized_content'),
                    "score": float(score),
                    "metadata": meta
                }
                top_results.append(result)
                logger.debug(f"Retrieved chunk_id='{meta['chunk_id']}' with score={score:.4f}.")
            else:
                logger.warning(f"Index {idx} out of bounds for metadata.")

        logger.info(f"FAISS search returned {len(top_results)} results for query: '{query}'.")
        logger.debug(f"Chunks retrieved: {[res['chunk_id'] for res in top_results]}.")
        return top_results

    def save_db(self):
        logger.debug("Entering save_db method.")
        data = {
            "metadata": self.metadata,
        }
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        try:
            with open(self.db_path, "wb") as file:
                pickle.dump(data, file)
            logger.info(f"Vector database metadata saved to '{self.db_path}'.")
        except Exception as e:
            logger.error(f"Error saving vector database metadata to '{self.db_path}': {e}", exc_info=True)

    def load_db(self):
        logger.debug("Entering load_db method.")
        if not os.path.exists(self.db_path):
            logger.error(f"Vector database file not found at '{self.db_path}'. Use load_data to create a new database.")
            raise ValueError("Vector database file not found.")
        try:
            with open(self.db_path, "rb") as file:
                data = pickle.load(file)
            self.metadata = data.get("metadata", [])
            logger.info(f"Vector database metadata loaded from '{self.db_path}' with {len(self.metadata)} entries.")
            logger.debug(f"Chunks loaded: {[meta['chunk_id'] for meta in self.metadata]}.")
        except Exception as e:
            logger.error(f"Error loading vector database metadata from '{self.db_path}': {e}", exc_info=True)
            raise


# File: core/elasticsearch_bm25.py
#------------------------------------------------------------------------------
# src/core/elasticsearch_bm25.py

import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk, scan
from elasticsearch.exceptions import NotFoundError, RequestError
import json
import os

from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)

class ElasticsearchBM25:
    
    def __init__(
        self, 
        index_name: str,
        es_host: str = "http://localhost:9200",
        logger: Optional[logging.Logger] = None
    ):
        self.logger = logger or logging.getLogger(__name__)
        self.index_name = index_name
        self.es_client = Elasticsearch(es_host)
        
        if not self.es_client.ping():
            raise ConnectionError(f"Failed to connect to Elasticsearch at {es_host}")
            
        self._create_index()
        
    def _create_index(self) -> None:
        index_settings = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "english"
                        },
                        "custom_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "porter_stem"]
                        }
                    }
                },
                "similarity": {
                    "custom_bm25": {
                        "type": "BM25",
                        "k1": 1.2,
                        "b": 0.75,
                    }
                },
                "index": {
                    "refresh_interval": "1s",
                    "number_of_shards": 1,
                    "number_of_replicas": 0
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "contextualized_content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "doc_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "original_index": {"type": "integer"},
                    "metadata": {"type": "object", "enabled": True}
                }
            }
        }
        
        index_data_dir = f"./data/{self.index_name}"
        os.makedirs(index_data_dir, exist_ok=True)
        self.logger.debug(f"Ensured existence of index data directory: {index_data_dir}")
        
        try:
            if self.es_client.indices.exists(index=self.index_name):
                self.logger.info(f"Index '{self.index_name}' already exists. Skipping creation.")
                # Avoid modifying immutable settings
            else:
                self.es_client.indices.create(
                    index=self.index_name,
                    body=index_settings
                )
                self.logger.info(f"Successfully created index: {self.index_name}")
        except Exception as e:
            self.logger.error(f"Failed to create/update index '{self.index_name}': {str(e)}")
            raise

    def index_documents(
        self,
        documents: List[Dict[str, Any]],
        batch_size: int = 500
    ) -> Tuple[int, List[Dict[str, Any]]]:
        if not documents:
            self.logger.warning("No documents provided for indexing")
            return 0, []

        failed_docs = []
        success_count = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            actions = [{
                "_index": self.index_name,
                "_source": {
                    "content": doc.get("original_content", ""),
                    "contextualized_content": doc.get("contextualized_content", ""),
                    "doc_id": doc.get("doc_id", ""),
                    "chunk_id": doc.get("chunk_id", ""),
                    "original_index": doc.get("original_index", 0),
                    "metadata": doc.get("metadata", {})
                }
            } for doc in batch]
            
            try:
                success, failed = bulk(
                    self.es_client,
                    actions,
                    raise_on_error=False,
                    raise_on_exception=False
                )
                success_count += success
                if failed:
                    for fail in failed:
                        failed_doc = batch[fail.get('index', 0)]
                        failed_docs.append(failed_doc)
                    self.logger.warning(f"Failed to index {len(failed)} documents in batch")
                    
            except Exception as e:
                self.logger.error(f"Batch indexing error: {str(e)}")
                failed_docs.extend(batch)
                
        self.es_client.indices.refresh(index=self.index_name)
        self.logger.info(f"Indexed {success_count}/{len(documents)} documents successfully")
        return success_count, failed_docs

    def search(
        self,
        query: str,
        k: int = 20,
        min_score: float = 0.1,
        fields: List[str] = None,
        operator: str = "or",
        minimum_should_match: str = "30%"
    ) -> List[Dict[str, Any]]:
        if not fields:
            fields = ["content^1", "contextualized_content^1.5"]
            
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "operator": operator,
                    "minimum_should_match": minimum_should_match,
                    "type": "best_fields",
                    "tie_breaker": 0.3
                }
            },
            "min_score": min_score,
            "size": k,
            "_source": True
        }

        try:
            search_body_json = json.dumps(search_body, indent=2)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")
        except Exception as e:
            self.logger.error(f"Error converting search body to JSON: {e}")
            search_body_json = str(search_body)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")

        try:
            response = self.es_client.search(
                index=self.index_name,
                body=search_body
            )
            
            hits = [{
                "doc_id": hit["_source"]["doc_id"],
                "chunk_id": hit["_source"]["chunk_id"],
                "content": hit["_source"]["content"],
                "contextualized_content": hit["_source"].get("contextualized_content"),
                "score": hit["_score"],
                "metadata": hit["_source"].get("metadata", {})
            } for hit in response["hits"]["hits"]]
            
            self.logger.debug(
                f"Search for '{query}' returned {len(hits)} results "
                f"(max_score: {response['hits'].get('max_score', 0)})"
            )
            return hits
            
        except Exception as e:
            self.logger.error(f"Search error: {str(e)}")
            raise

    def search_content(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["content^1"],
            operator="or",
            minimum_should_match="30%"
        )

    def search_contextualized(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["contextualized_content^1.5"],
            operator="or",
            minimum_should_match="30%"
        )


# File: core/openai_client.py
#------------------------------------------------------------------------------
import logging
import os
from typing import List, Dict, Any
from openai import OpenAI
# Initialize logger
logger = logging.getLogger(__name__)

class OpenAIClient:
    """
    Wrapper for OpenAI API interactions using the updated SDK.
    """
    def __init__(self, api_key: str):
        """
        Initializes the OpenAI client with the provided API key.
        
        Args:
            api_key (str): OpenAI API key.
        """
        if not api_key:
            logger.error("OpenAI API key is not provided.")
            raise ValueError("OpenAI API key must be provided.")
        self.client = OpenAI(api_key=api_key)
        logger.debug("OpenAI client initialized successfully.")

    def create_chat_completion(self, model: str, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> Dict[str, Any]:
        """
        Creates a chat completion using OpenAI's API.
        
        Args:
            model (str): Model name to use.
            messages (List[Dict[str, str]]): List of message dictionaries.
            max_tokens (int): Maximum number of tokens in the response.
            temperature (float): Sampling temperature.
        
        Returns:
            Dict[str, Any]: API response as a dictionary.
        """
        if not model or not messages:
            logger.error("Model and messages must be provided for chat completion.")
            raise ValueError("Model and messages must be provided for chat completion.")
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            logger.debug(f"Chat completion created successfully for model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating chat completion: {e}")
            raise

    def create_embeddings(self, model: str, input: List[str]) -> Dict[str, Any]:
        """
        Creates embeddings for the given input texts using OpenAI's API.
        
        Args:
            model (str): Embedding model to use.
            input (List[str]): List of input texts.
        
        Returns:
            Dict[str, Any]: API response containing embeddings.
        """
        if not model or not input:
            logger.error("Model and input must be provided for creating embeddings.")
            raise ValueError("Model and input must be provided for creating embeddings.")
        
        try:
            response = self.client.embeddings.create(
                model=model,
                input=input
            )
            logger.debug(f"Embeddings created successfully using model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            raise



################################################################################
# Module: data
################################################################################


# File: data/__init__.py
#------------------------------------------------------------------------------
"""
Data handling modules for loading and processing data.
"""

from .data_loader import load_codebase_chunks, load_queries
from .copycode import CodeFileHandler

__all__ = ['load_codebase_chunks', 'load_queries', 'CodeFileHandler']

# File: data/copycode.py
#------------------------------------------------------------------------------
import logging
import os
import asyncio
import aiofiles
from typing import List, Set
import time
from pathlib import Path

# Initialize logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CodeFileHandler:
    def __init__(self, project_root: str, extensions: Set[str] = None):
        """
        Initialize the CodeFileHandler.
        
        Args:
            project_root (str): Root directory of the project
            extensions (Set[str]): Set of file extensions to include
        """
        self.project_root = project_root
        self.src_dir = os.path.join(project_root, '')
        self.extensions = extensions or {'.py', '.yaml', '.yml'}
        self.ignored_dirs = {'.venv', 'venv', 'env', '.env', 'myenv', '__pycache__', '.git', 'node_modules'}
        self.output_dir = os.path.join(project_root, 'output')

    async def get_code_files(self) -> List[str]:
        """
        Get all code files from the src directory.
        
        Returns:
            List[str]: List of file paths
        """
        logger.info(f"Scanning for code files in: {self.src_dir}")
        start_time = time.time()
        code_files = []
        
        try:
            for root, dirs, files in os.walk(self.src_dir):
                # Remove ignored directories
                dirs[:] = [d for d in dirs if d not in self.ignored_dirs]
                
                # Process files
                for file in files:
                    if file.endswith(tuple(self.extensions)):
                        file_path = os.path.join(root, file)
                        relative_path = os.path.relpath(file_path, self.src_dir)
                        code_files.append((file_path, relative_path))
                        logger.debug(f"Found code file: {relative_path}")
        
        except Exception as e:
            logger.error(f"Error scanning code files: {e}", exc_info=True)
            return []

        end_time = time.time()
        logger.info(f"Found {len(code_files)} code files in {end_time - start_time:.2f} seconds")
        return sorted(code_files, key=lambda x: x[1])  # Sort by relative path

    async def copy_code_to_file(self, code_files: List[tuple], output_file: str):
        """
        Copy code files to a single output file with proper organization.
        
        Args:
            code_files (List[tuple]): List of (file_path, relative_path) tuples
            output_file (str): Output file path
        """
        logger.info(f"Writing consolidated code to: {output_file}")
        start_time = time.time()
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        try:
            async with aiofiles.open(output_file, 'w', encoding='utf-8') as outfile:
                # Write header
                await outfile.write("# Consolidated Source Code\n")
                await outfile.write("# " + "=" * 78 + "\n\n")

                current_module = None
                for file_path, relative_path in code_files:
                    # Get module name (first directory in relative path)
                    module = relative_path.split(os.sep)[0] if os.sep in relative_path else 'root'
                    
                    # Write module header if changed
                    if module != current_module:
                        await outfile.write(f"\n\n{'#' * 80}\n")
                        await outfile.write(f"# Module: {module}\n")
                        await outfile.write(f"{'#' * 80}\n\n")
                        current_module = module

                    # Write file header
                    await outfile.write(f"\n# File: {relative_path}\n")
                    await outfile.write("#" + "-" * 78 + "\n")

                    try:
                        async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                            content = await infile.read()
                            await outfile.write(content)
                            await outfile.write("\n")
                    except Exception as e:
                        error_msg = f"# Error reading file {relative_path}: {str(e)}\n"
                        await outfile.write(error_msg)
                        logger.error(error_msg)

        except Exception as e:
            logger.error(f"Error writing to output file: {e}", exc_info=True)
            return

        end_time = time.time()
        logger.info(f"Code consolidation completed in {end_time - start_time:.2f} seconds")

async def main_async():
    """
    Main async function to handle code consolidation.
    """
    try:
        # Get project root (parent of src directory)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        logger.info(f"Project root: {project_root}")

        # Initialize handler
        handler = CodeFileHandler(
            project_root=project_root,
            extensions={'.py', '.yaml', '.yml'}
        )

        # Create output directory if it doesn't exist
        output_dir = os.path.join(project_root, 'output')
        os.makedirs(output_dir, exist_ok=True)

        # Get code files
        code_files = await handler.get_code_files()
        
        if not code_files:
            logger.error("No code files found to process")
            return

        # Generate output file path
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f'consolidated_code_{timestamp}.txt')

        # Copy code files
        await handler.copy_code_to_file(code_files, output_file)
        logger.info(f"Code consolidated successfully to: {output_file}")

    except Exception as e:
        logger.error(f"Error in main_async: {e}", exc_info=True)

def main():
    """
    Main entry point of the script.
    """
    try:
        asyncio.run(main_async())
        logger.info("Code consolidation completed successfully")
    except KeyboardInterrupt:
        logger.warning("Process interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)

if __name__ == "__main__":
    main()

# File: data/data_loader.py
#------------------------------------------------------------------------------
import json
import logging
from typing import List, Dict, Any

from src.utils.logger import setup_logging
logger = logging.getLogger(__name__)

class JSONLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Dict[str, Any]]:
        data = []
        try:
            if self.file_path.endswith('.jsonl'):
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    for line_number, line in enumerate(f, start=1):
                        if line.strip():
                            try:
                                obj = json.loads(line)
                                data.append(obj)
                            except json.JSONDecodeError as e:
                                logger.error(f"JSON parsing error in file '{self.file_path}' at line {line_number}: {e}")
                logger.info(f"Loaded JSONL file '{self.file_path}' with {len(data)} entries successfully.")
            else:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logger.info(f"Loaded JSON file '{self.file_path}' successfully with {len(data)} entries.")
            return data
        except FileNotFoundError:
            logger.error(f"Error loading JSON file '{self.file_path}': File does not exist.")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"Error loading JSON file '{self.file_path}': {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error loading JSON file '{self.file_path}': {e}")
            return []

def load_codebase_chunks(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading codebase chunks from '{file_path}'.")
    loader = JSONLoader(file_path)
    return loader.load()

def load_queries(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading queries from '{file_path}'.")
    loader = JSONLoader(file_path)
    return loader.load()



################################################################################
# Module: root
################################################################################


# File: decorators.py
#------------------------------------------------------------------------------
import logging
import functools

logger = logging.getLogger(__name__)

def handle_exceptions(func):
    """
    Decorator to handle exceptions in functions.
    
    Args:
        func (callable): The function to wrap with exception handling.
    
    Returns:
        callable: The wrapped function with exception handling.
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}", exc_info=True)
            return {"error": "An error occurred. Please try again later."}
    return wrapper



################################################################################
# Module: docs
################################################################################


# File: docs/mkdocs.yml
#------------------------------------------------------------------------------
site_name: DSPy Documentation
site_description: The framework for programmingrather than promptinglanguage models.
site_url: https://dspy.ai

repo_url: https://github.com/stanfordnlp/dspy
repo_name: stanfordnlp/dspy

edit_uri: blob/main/docs/docs/
docs_dir: "docs/"
nav:
  - Home: index.md
  - Learn DSPy:
      - Learning DSPy: learn/index.md
      - DSPy Programming:
          - Programming Overview: learn/programming/overview.md
          - Language Models: learn/programming/language_models.md
          - Signatures: learn/programming/signatures.md
          - Modules: learn/programming/modules.md
      - DSPy Evaluation:
          - Evaluation Overview: learn/evaluation/overview.md
          - Data Handling: learn/evaluation/data.md
          - Metrics: learn/evaluation/metrics.md
      - DSPy Optimization:
          - Optimization Overview: learn/optimization/overview.md
          - Optimizers: learn/optimization/optimizers.md
      - Other References:
          - Retrieval Clients:
              - Azure: deep-dive/retrieval_models_clients/Azure.md
              - ChromadbRM: deep-dive/retrieval_models_clients/ChromadbRM.md
              - ClarifaiRM: deep-dive/retrieval_models_clients/ClarifaiRM.md
              - ColBERTv2: deep-dive/retrieval_models_clients/ColBERTv2.md
              - Custom RM Client: deep-dive/retrieval_models_clients/custom-rm-client.md
              - DatabricksRM: deep-dive/retrieval_models_clients/DatabricksRM.md
              - FaissRM: deep-dive/retrieval_models_clients/FaissRM.md
              - LancedbRM: deep-dive/retrieval_models_clients/LancedbRM.md
              - MilvusRM: deep-dive/retrieval_models_clients/MilvusRM.md
              - MyScaleRM: deep-dive/retrieval_models_clients/MyScaleRM.md
              - Neo4jRM: deep-dive/retrieval_models_clients/Neo4jRM.md
              - QdrantRM: deep-dive/retrieval_models_clients/QdrantRM.md
              - RAGatouilleRM: deep-dive/retrieval_models_clients/RAGatouilleRM.md
              - SnowflakeRM: deep-dive/retrieval_models_clients/SnowflakeRM.md
              - WatsonDiscovery: deep-dive/retrieval_models_clients/WatsonDiscovery.md
              - WeaviateRM: deep-dive/retrieval_models_clients/WeaviateRM.md
              - YouRM: deep-dive/retrieval_models_clients/YouRM.md
  - Tutorials:
      - Tutorials Overview: tutorials/index.md
      - Retrieval-Augmented Generation: tutorials/rag/index.ipynb
      - Entity Extraction: tutorials/entity_extraction/index.ipynb
      - Deployment: tutorials/deployment/index.md
  - Community:
      - Community Resources: community/community-resources.md
      - Use Cases: community/use-cases.md
      - Roadmap: roadmap.md
      - Contributing: community/how-to-contribute.md
  - FAQ:
      - FAQ: faqs.md
      - Cheatsheet: cheatsheet.md

theme:
  name: material
  custom_dir: overrides
  features:
    - navigation.tabs
    - navigation.path
    - navigation.indexes
    - toc.follow
    - navigation.top
    - search.suggest
    - search.highlight
    - content.tabs.link
    - content.code.annotation
    - content.code.copy
    - navigation.footer
    - content.action.edit
  language: en
  palette:
    - scheme: default
      toggle:
        icon: material/weather-night
        name: Switch to dark mode
      primary: white
      accent: black
    - scheme: slate
      toggle:
        icon: material/weather-sunny
        name: Switch to light mode
      primary: black
      accent: lime
  icon:
    repo: fontawesome/brands/git-alt
    edit: material/pencil
    view: material/eye
  logo: static/img/dspy_logo.png
  favicon: static/img/logo.png

extra_css:
  - stylesheets/extra.css

plugins:
  - social
  - search
  - mkdocstrings
  # - blog
  - mkdocs-jupyter:
      ignore_h1_titles: True
  - redirects:
      redirect_maps:
        # Redirect /intro/ to the main page
        "intro/index.md": "index.md"
        "intro.md": "index.md"

        "docs/quick-start/getting-started-01.md": "tutorials/rag/index.ipynb"
        "docs/quick-start/getting-started-02.md": "tutorials/rag/index.ipynb"
        "quick-start/getting-started-01.md": "tutorials/rag/index.ipynb"
        "quick-start/getting-started-02.md": "tutorials/rag/index.ipynb"

extra:
  social:
    - icon: fontawesome/brands/github
      link: https://github.com/stanfordnlp/dspy
    - icon: fontawesome/brands/discord
      link: https://discord.gg/XCGy2WDCQB

extra_javascript:
  - "js/runllm-widget.js"

markdown_extensions:
  - pymdownx.tabbed:
      alternate_style: true
  - pymdownx.highlight:
      anchor_linenums: true
  - pymdownx.inlinehilite
  - pymdownx.snippets
  - admonition
  - pymdownx.arithmatex:
      generic: true
  - footnotes
  - pymdownx.details
  - pymdownx.superfences
  - pymdownx.mark
  - attr_list
  - pymdownx.emoji:
      emoji_index: !!python/name:material.extensions.emoji.twemoji
      emoji_generator: !!python/name:materialx.emoji.to_svg

copyright: |
  &copy; 2024 <a href="https://github.com/stanfordnlp"  target="_blank" rel="noopener">Stanford NLP</a>



################################################################################
# Module: evaluation
################################################################################


# File: evaluation/__init__.py
#------------------------------------------------------------------------------
"""
Evaluation modules for assessing model performance.
"""

from .evaluation import PipelineEvaluator
from .evaluator import PipelineEvaluator as Evaluator

__all__ = ['PipelineEvaluator', 'Evaluator']

# File: evaluation/evaluation.py
#------------------------------------------------------------------------------
import logging
import time
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.analysis.metrics import comprehensive_metric, is_answer_fully_correct

setup_logging()
logger = logging.getLogger(__name__)

class PipelineEvaluator:
    def __init__(self, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, retrieval_function: Callable):
        self.db = db
        self.es_bm25 = es_bm25
        self.retrieval_function = retrieval_function

    def evaluate_pipeline(self, queries: List[Dict[str, Any]], k: int = 20) -> Dict[str, float]:
        total_score = 0
        total_queries = len(queries)
        queries_with_golden = 0
        queries_without_golden = 0

        semantic_success = 0
        bm25_contextual_success = 0
        total_semantic_hits = 0
        total_bm25_contextual_hits = 0

        logger.info(f"Starting evaluation of {total_queries} queries.")

        for query_item in tqdm(queries, desc="Evaluating retrieval"):
            query = query_item.get('query', '').strip()

            has_golden_data = all([
                'golden_doc_uuids' in query_item,
                'golden_chunk_uuids' in query_item,
                'golden_documents' in query_item
            ])

            if has_golden_data:
                queries_with_golden += 1
                golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
                golden_contents = []

                for doc_uuid, chunk_index in golden_chunk_uuids:
                    golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                    if not golden_doc:
                        logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                    if not golden_chunk:
                        logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_contents.append(golden_chunk.get('content', '').strip())

                if not golden_contents:
                    logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                    continue

                retrieved_docs = self.retrieval_function(query, self.db, self.es_bm25, k)

                chunks_found = 0
                semantic_hits = 0
                bm25_contextual_hits = 0

                for golden_content in golden_contents:
                    for doc in retrieved_docs[:k]:
                        retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                        contextualized_content = doc.get('chunk', {}).get('contextualized_content', '').strip()
                        if retrieved_content == golden_content:
                            chunks_found += 1
                            semantic_hits += 1
                            break
                        elif contextualized_content == golden_content:
                            chunks_found += 1
                            bm25_contextual_hits += 1
                            break

                query_score = chunks_found / len(golden_contents)
                total_score += query_score
                logger.debug(f"Query '{query}' score: {query_score}")

                semantic_success += semantic_hits
                bm25_contextual_success += bm25_contextual_hits
                total_semantic_hits += semantic_hits
                total_bm25_contextual_hits += bm25_contextual_hits
            else:
                queries_without_golden += 1
                logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
                continue

        average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
        pass_at_n = average_score * 100

        semantic_percentage = (semantic_success / total_semantic_hits) * 100 if total_semantic_hits > 0 else 0
        bm25_contextual_percentage = (bm25_contextual_success / total_bm25_contextual_hits) * 100 if total_bm25_contextual_hits > 0 else 0

        logger.info(f"Evaluation completed.")
        logger.info(f"Total Queries: {total_queries}")
        logger.info(f"Queries with Golden Data: {queries_with_golden}")
        logger.info(f"Queries without Golden Data: {queries_without_golden}")
        logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")
        logger.info(f"Semantic Hits: {semantic_percentage:.2f}%")
        logger.info(f"BM25 Contextual Hits: {bm25_contextual_percentage:.2f}%")

        return {
            "pass_at_n": pass_at_n,
            "average_score": average_score,
            "semantic_hit_percentage": semantic_percentage,
            "bm25_contextual_hit_percentage": bm25_contextual_percentage,
            "total_queries": total_queries,
            "queries_with_golden": queries_with_golden,
            "queries_without_golden": queries_without_golden
        }

    def evaluate_complete_pipeline(self, k_values: List[int], evaluation_set: List[Dict[str, Any]]):
        for k in k_values:
            logger.info(f"Starting evaluation for Pass@{k}")
            results = self.evaluate_pipeline(evaluation_set, k)
            logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
            logger.info(f"Average Score: {results['average_score']:.4f}")
            logger.info(f"Semantic Hit Percentage: {results['semantic_hit_percentage']:.2f}%")
            logger.info(f"BM25 Contextual Hit Percentage: {results['bm25_contextual_hit_percentage']:.2f}%")
            logger.info(f"Total Queries: {results['total_queries']}")
            logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
            logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")


# File: evaluation/evaluator.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25

logger = logging.getLogger(__name__)

class PipelineEvaluator:
    def __init__(self, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, retrieval_function: Callable):
        self.db = db
        self.es_bm25 = es_bm25
        self.retrieval_function = retrieval_function

    def evaluate_pipeline(self, queries: List[Dict[str, Any]], k: int = 20) -> Dict[str, float]:
        total_score = 0
        total_queries = len(queries)
        queries_with_golden = 0
        queries_without_golden = 0

        logger.info(f"Starting evaluation of {total_queries} queries.")

        for query_item in tqdm(queries, desc="Evaluating retrieval"):
            query = query_item.get('query', '').strip()

            has_golden_data = all([
                'golden_doc_uuids' in query_item,
                'golden_chunk_uuids' in query_item,
                'golden_documents' in query_item
            ])

            if has_golden_data:
                queries_with_golden += 1
                golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
                golden_contents = []

                for doc_uuid, chunk_index in golden_chunk_uuids:
                    golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                    if not golden_doc:
                        logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                    if not golden_chunk:
                        logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_contents.append(golden_chunk.get('content', '').strip())

                if not golden_contents:
                    logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                    continue

                retrieved_docs = self.retrieval_function(query, self.db, self.es_bm25, k)

                chunks_found = 0
                for golden_content in golden_contents:
                    for doc in retrieved_docs[:k]:
                        retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                        if retrieved_content == golden_content:
                            chunks_found += 1
                            break

                query_score = chunks_found / len(golden_contents)
                total_score += query_score
                logger.debug(f"Query '{query}' score: {query_score}")
            else:
                queries_without_golden += 1
                logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
                continue

        average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
        pass_at_n = average_score * 100

        logger.info(f"Evaluation completed.")
        logger.info(f"Total Queries: {total_queries}")
        logger.info(f"Queries with Golden Data: {queries_with_golden}")
        logger.info(f"Queries without Golden Data: {queries_without_golden}")
        logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")

        return {
            "pass_at_n": pass_at_n,
            "average_score": average_score,
            "total_queries": total_queries,
            "queries_with_golden": queries_with_golden,
            "queries_without_golden": queries_without_golden
        }

    def evaluate_complete_pipeline(self, k_values: List[int], evaluation_set: List[Dict[str, Any]]):
        for k in k_values:
            logger.info(f"Starting evaluation for Pass@{k}")
            results = self.evaluate_pipeline(evaluation_set, k)
            logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
            logger.info(f"Average Score: {results['average_score']:.4f}")
            logger.info(f"Total Queries: {results['total_queries']}")
            logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
            logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")



################################################################################
# Module: root
################################################################################


# File: main.py
#------------------------------------------------------------------------------
import gc
import logging
import os
from typing import List, Dict, Any
import asyncio
import dspy
from dspy.teleprompt import BootstrapFewShotWithRandomSearch
from dspy.datasets import DataLoader
from dspy.primitives.assertions import assert_transform_module, backtrack_handler
import threading
import time

from src.utils.logger import setup_logging
from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.data.data_loader import load_codebase_chunks, load_queries
from src.processing.query_processor import process_queries, validate_queries
from src.evaluation.evaluation import PipelineEvaluator
from src.analysis.metrics import comprehensive_metric
from src.processing.answer_generator import generate_answer_dspy, QuestionAnswerSignature
from src.retrieval.reranking import retrieve_with_reranking
from src.analysis.select_quotation_module import EnhancedQuotationModule as EnhancedQuotationModuleStandard
# Removed Alternate Quotation Module import
from src.analysis.extract_keyword_module import KeywordExtractionModule  # Import KeywordExtractionModule
from src.analysis.coding_module import CodingAnalysisModule  # Import CodingAnalysisModule
from src.decorators import handle_exceptions

# Import the conversion functions from src.convert/
from src.convert.convertquotationforkeyword import convert_query_results as convert_quotation_to_keyword
from src.convert.convertkeywordforcoding import convert_query_results as convert_keyword_to_coding
from src.convert.convertcodingfortheme import process_input_file as convert_coding_to_theme

# Initialize logging
setup_logging()
logger = logging.getLogger(__name__)

# Configure DSPy settings
dspy.settings.configure(main_thread_only=True)

# Introduce a thread lock mechanism
thread_lock = threading.Lock()


class ThematicAnalysisPipeline:
    def __init__(self):
        self.contextual_db = ContextualVectorDB("contextual_db")
        self.es_bm25 = None
        self.optimized_coding_program = None  # Added for Coding Analysis
        logger.debug("ThematicAnalysisPipeline instance created with ContextualVectorDB initialized.")

    def create_elasticsearch_bm25_index(self, index_name: str) -> ElasticsearchBM25:
        """
        Create and index documents in Elasticsearch BM25.
        """
        logger.debug("Entering create_elasticsearch_bm25_index method.")
        try:
            es_bm25 = ElasticsearchBM25(index_name=index_name)
            logger.info(f"ElasticsearchBM25 instance created with index '{index_name}'.")
            start_time = time.time()
            success_count, failed_docs = es_bm25.index_documents(self.contextual_db.metadata)
            elapsed_time = time.time() - start_time
            logger.info(
                f"Elasticsearch BM25 index '{index_name}' created successfully with {success_count} documents indexed "
                f"in {elapsed_time:.2f} seconds."
            )
            if failed_docs:
                logger.warning(f"{len(failed_docs)} documents failed to index.")
                for doc in failed_docs:
                    logger.warning(f"Failed to index document ID: {doc.get('doc_id', 'Unknown')}")
        except Exception as e:
            logger.error(f"Error creating Elasticsearch BM25 index '{index_name}': {e}", exc_info=True)
            raise  # Re-raise the exception after logging
        finally:
            logger.debug("Exiting create_elasticsearch_bm25_index method.")
        return es_bm25

    async def initialize_quotation_optimizer(self, config):
        """
        Initialize the quotation selection optimizer.
        """
        logger.info("Initializing quotation selection optimizer.")
        start_time = time.time()
        try:
            dl = DataLoader()
            quotation_train_dataset = dl.from_csv(
                config['quotation_training_data'],
                fields=("input", "output"),
                input_keys=("input",)
            )
            logger.debug(f"Quotation training dataset loaded with {len(quotation_train_dataset)} samples.")

            self.quotation_qa_module = dspy.TypedChainOfThought(QuestionAnswerSignature)

            optimizer_config = {
                'max_bootstrapped_demos': 4,
                'max_labeled_demos': 4,
                'num_candidate_programs': 10,
                'num_threads': 1
            }

            self.quotation_teleprompter = BootstrapFewShotWithRandomSearch(
                metric=comprehensive_metric,
                **optimizer_config
            )

            self.optimized_quotation_program = self.quotation_teleprompter.compile(
                student=self.quotation_qa_module,
                teacher=self.quotation_qa_module,
                trainset=quotation_train_dataset
            )

            self.optimized_quotation_program.save(config['optimized_quotation_program'])
            elapsed_time = time.time() - start_time
            logger.info(f"Quotation optimizer initialized successfully in {elapsed_time:.2f} seconds.")
        except Exception as e:
            logger.error(f"Error initializing quotation optimizer: {e}", exc_info=True)
            raise  # Re-raise the exception after logging

    async def initialize_keyword_optimizer(self, config):
        """
        Initialize the keyword extraction optimizer.
        """
        logger.info("Initializing keyword extraction optimizer.")
        start_time = time.time()
        try:
            dl = DataLoader()
            keyword_train_dataset = dl.from_csv(
                config['keyword_training_data'],
                fields=("input", "output"),
                input_keys=("input",)
            )
            logger.debug(f"Keyword training dataset loaded with {len(keyword_train_dataset)} samples.")

            self.keyword_qa_module = dspy.TypedChainOfThought(QuestionAnswerSignature)

            optimizer_config = {
                'max_bootstrapped_demos': 4,
                'max_labeled_demos': 4,
                'num_candidate_programs': 10,
                'num_threads': 1
            }

            self.keyword_teleprompter = BootstrapFewShotWithRandomSearch(
                metric=comprehensive_metric,
                **optimizer_config
            )

            self.optimized_keyword_program = self.keyword_teleprompter.compile(
                student=self.keyword_qa_module,
                teacher=self.keyword_qa_module,
                trainset=keyword_train_dataset
            )

            self.optimized_keyword_program.save(config['optimized_keyword_program'])
            elapsed_time = time.time() - start_time
            logger.info(f"Keyword extraction optimizer initialized successfully in {elapsed_time:.2f} seconds.")
        except Exception as e:
            logger.error(f"Error initializing keyword extraction optimizer: {e}", exc_info=True)
            raise  # Re-raise the exception after logging

    async def initialize_coding_optimizer(self, config):
        """
        Initialize the coding analysis optimizer.
        """
        logger.info("Initializing coding analysis optimizer.")
        start_time = time.time()
        try:
            dl = DataLoader()
            coding_train_dataset = dl.from_csv(
                config['coding_training_data'],
                fields=("input", "output"),
                input_keys=("input",)
            )
            logger.debug(f"Coding training dataset loaded with {len(coding_train_dataset)} samples.")

            self.coding_qa_module = dspy.TypedChainOfThought(QuestionAnswerSignature)

            optimizer_config = {
                'max_bootstrapped_demos': 4,
                'max_labeled_demos': 4,
                'num_candidate_programs': 10,
                'num_threads': 1
            }

            self.coding_teleprompter = BootstrapFewShotWithRandomSearch(
                metric=comprehensive_metric,
                **optimizer_config
            )

            self.optimized_coding_program = self.coding_teleprompter.compile(
                student=self.coding_qa_module,
                teacher=self.coding_qa_module,
                trainset=coding_train_dataset
            )

            self.optimized_coding_program.save(config['optimized_coding_program'])
            elapsed_time = time.time() - start_time
            logger.info(f"Coding analysis optimizer initialized successfully in {elapsed_time:.2f} seconds.")
        except Exception as e:
            logger.error(f"Error initializing coding analysis optimizer: {e}", exc_info=True)
            raise  # Re-raise the exception after logging

    @handle_exceptions
    async def run_pipeline_with_config(self, config, module_class, optimizer_init_func):
        """
        Main function to load data, process queries, and generate outputs.
        :param config: Configuration dictionary for the pipeline.
        :param module_class: The DSPy module class to use (Quotation, Keyword Extraction, or Coding Analysis).
        :param optimizer_init_func: Function to initialize the optimizer (Quotation, Keyword Extraction, or Coding Analysis).
        """
        logger.debug("Entering run_pipeline_with_config method.")
        try:
            # Configure DSPy Language Model
            logger.info("Configuring DSPy Language Model.")
            lm = dspy.LM('openai/gpt-4o-mini', max_tokens=8192)
            dspy.configure(lm=lm)
            dspy.Cache = False
            logger.debug("DSPy Language Model configured.")

            # Define file paths from config
            codebase_chunks_file = config['codebase_chunks_file']
            queries_file_standard = config['queries_file_standard']
            evaluation_set_file = config['evaluation_set_file']
            output_filename_primary = config['output_filename_primary']
            training_data_file = config.get(f"{module_class.__name__.lower().replace('module', '')}_training_data", None)
            optimized_program_file = config.get(f"optimized_{module_class.__name__.lower().replace('module', '')}_program", None)

            dl = DataLoader()

            # Load the codebase chunks
            logger.info(f"Loading codebase chunks from '{codebase_chunks_file}'.")
            start_time = time.time()
            codebase_chunks = load_codebase_chunks(codebase_chunks_file)
            elapsed_time = time.time() - start_time
            logger.info(f"Loaded {len(codebase_chunks)} documents in {elapsed_time:.2f} seconds.")

            # Load and process the data
            try:
                logger.info("Loading data into ContextualVectorDB.")
                start_time = time.time()
                self.contextual_db.load_data(codebase_chunks, parallel_threads=4)
                elapsed_time = time.time() - start_time
                logger.info(f"Data loaded into ContextualVectorDB in {elapsed_time:.2f} seconds.")
                logger.info(f"Total embeddings: {len(self.contextual_db.embeddings)}.")
                logger.info(f"Total metadata entries: {len(self.contextual_db.metadata)}.")
            except Exception as e:
                logger.error(f"Error loading data into ContextualVectorDB: {e}", exc_info=True)
                return

            # Create the Elasticsearch BM25 index
            try:
                logger.info(f"Creating Elasticsearch BM25 index '{config['index_name']}'.")
                start_time = time.time()
                self.es_bm25 = self.create_elasticsearch_bm25_index(config['index_name'])
                elapsed_time = time.time() - start_time
                logger.info(f"Elasticsearch BM25 index '{config['index_name']}' created in {elapsed_time:.2f} seconds.")
            except Exception as e:
                logger.error(f"Error creating Elasticsearch BM25 index: {e}", exc_info=True)
                return

            # Load queries
            logger.info(f"Loading standard queries from '{queries_file_standard}'.")
            start_time = time.time()
            standard_queries = load_queries(queries_file_standard)
            elapsed_time = time.time() - start_time
            logger.info(f"Loaded {len(standard_queries)} standard queries in {elapsed_time:.2f} seconds.")

            if not standard_queries:
                logger.error("No standard queries found to process.")
                return

            # Validate queries
            logger.info("Validating standard queries.")
            start_time = time.time()
            validated_standard_queries = validate_queries(standard_queries, module_class())
            elapsed_time = time.time() - start_time
            logger.info(f"Validated {len(validated_standard_queries)} queries in {elapsed_time:.2f} seconds.")

            # Initialize optimizer (Quotation, Keyword Extraction, or Coding Analysis)
            logger.info(f"Initializing optimizer for {module_class.__name__}.")
            start_time = time.time()
            await optimizer_init_func(config)
            elapsed_time = time.time() - start_time
            logger.info(f"Optimizer for {module_class.__name__} initialized in {elapsed_time:.2f} seconds.")

            # Initialize the module with assertions
            try:
                logger.info(f"Initializing {module_class.__name__}.")
                module_instance = module_class()
                module_instance = assert_transform_module(
                    module_instance,
                    backtrack_handler
                )
                logger.info(f"{module_class.__name__} initialized successfully with assertions activated.")
            except Exception as e:
                logger.error(f"Error initializing {module_class.__name__}: {e}", exc_info=True)
                return

            # Define k value for queries
            k_standard = 20
            logger.debug(f"Set k_standard to {k_standard} for query processing.")

            # Determine the optimized program based on module type
            if 'quotation' in module_class.__name__.lower():
                optimized_program = self.optimized_quotation_program
                logger.debug("Using optimized quotation program.")
            elif 'keyword' in module_class.__name__.lower():
                optimized_program = self.optimized_keyword_program
                logger.debug("Using optimized keyword extraction program.")
            elif 'coding' in module_class.__name__.lower():
                optimized_program = self.optimized_coding_program
                logger.debug("Using optimized coding analysis program.")
            else:
                logger.error("No optimized program found for the given module type.")
                return

            # Process queries with the module
            logger.info(f"Processing queries with {module_class.__name__}.")
            start_time = time.time()
            await process_queries(
                validated_standard_queries,
                self.contextual_db,
                self.es_bm25,
                k=k_standard,
                output_file=config['output_filename_primary'],
                optimized_program=optimized_program,
                module=module_instance
            )
            elapsed_time = time.time() - start_time
            logger.info(f"Processed queries with {module_class.__name__} in {elapsed_time:.2f} seconds.")

            # Define k values for evaluation
            k_values = [5, 10, 20]
            logger.debug(f"Set k_values for evaluation: {k_values}.")

            # Initialize the evaluator
            logger.info("Starting evaluation of the retrieval pipeline.")
            try:
                evaluator = PipelineEvaluator(
                    db=self.contextual_db,
                    es_bm25=self.es_bm25,
                    retrieval_function=retrieve_with_reranking
                )
                logger.debug("PipelineEvaluator instance created.")

                # Perform evaluation
                logger.info(f"Loading evaluation set from '{config['evaluation_set_file']}'.")
                start_time = time.time()
                evaluation_set = load_queries(config['evaluation_set_file'])
                logger.info(f"Loaded {len(evaluation_set)} evaluation queries in {time.time() - start_time:.2f} seconds.")

                logger.info("Evaluating the complete retrieval pipeline.")
                start_time = time.time()
                evaluator.evaluate_complete_pipeline(
                    k_values=k_values,
                    evaluation_set=evaluation_set
                )
                elapsed_time = time.time() - start_time
                logger.info(f"Evaluation completed successfully in {elapsed_time:.2f} seconds.")
            except Exception as e:
                logger.error(f"Error during evaluation: {e}", exc_info=True)

            logger.info(f"Pipeline for {module_class.__name__} completed successfully.")
            logger.debug("Exiting run_pipeline_with_config method.")
        except Exception as e:
            logger.error(f"Unexpected error in run_pipeline_with_config: {e}", exc_info=True)
            raise  # Re-raise the exception after logging

    async def run_pipeline(self):
        logger.info("Starting Thematic Analysis Pipeline.")
        start_time = time.time()
        # Define configurations for each pipeline
        config_standard_quotation = {
            'index_name': 'contextual_bm25_index_standard_quotation',
            'codebase_chunks_file': 'data/codebase_chunks/codebase_chunks.json',
            'queries_file_standard': 'data/input/queries_quotation.json',
            'evaluation_set_file': 'data/evaluation/evaluation_set_quotation.jsonl',
            'output_filename_primary': 'data/output/query_results_quotation.json',
            'quotation_training_data': 'data/training/quotation_training_data.csv',
            'optimized_quotation_program': 'data/optimized/optimized_quotation_program.json'
        }
        config_keyword_extraction = {
            'index_name': 'contextual_bm25_index_keyword_extraction',
            'codebase_chunks_file': 'data/codebase_chunks/codebase_chunks.json',
            'queries_file_standard': 'data/input/queries_keyword.json',  # Will be updated after conversion
            'evaluation_set_file': 'data/evaluation/evaluation_set_keyword.jsonl',
            'output_filename_primary': 'data/output/query_results_keyword_extraction.json',
            'keyword_training_data': 'data/training/keyword_training_data.csv',
            'optimized_keyword_program': 'data/optimized/optimized_keyword_program.json'
        }
        config_coding_analysis = {  # Added configuration for Coding Analysis pipeline
            'index_name': 'contextual_bm25_index_coding_analysis',
            'codebase_chunks_file': 'data/codebase_chunks/codebase_chunks.json',
            'queries_file_standard': 'data/input/queries_coding.json',  # Will be updated after conversion
            'evaluation_set_file': 'data/evaluation/evaluation_set_coding.jsonl',
            'output_filename_primary': 'data/output/query_results_coding_analysis.json',
            'coding_training_data': 'data/training/coding_training_data.csv',
            'optimized_coding_program': 'data/optimized/optimized_coding_program.json'
        }

        # Run Standard Quotation Extraction Pipeline
        logger.info("Starting Standard Quotation Extraction Pipeline.")
        await self.run_pipeline_with_config(
            config_standard_quotation,
            EnhancedQuotationModuleStandard,
            self.initialize_quotation_optimizer
        )

        # Removed Alternate Quotation Extraction Pipeline

        # After running the quotation extraction pipeline, perform the conversion

        # Define a helper function to handle the conversion asynchronously
        async def perform_conversion(func, description="", **kwargs):
            try:
                logger.info(f"Converting data: {description}")
                start_time = time.time()
                # Run the conversion in a separate thread to avoid blocking the event loop
                await asyncio.to_thread(func, **kwargs)
                elapsed_time = time.time() - start_time
                logger.info(f"Conversion successful: {description} in {elapsed_time:.2f} seconds.")
            except Exception as e:
                logger.error(f"Conversion failed ({description}): {e}", exc_info=True)
                raise  # Re-raise the exception to halt the pipeline

        # Perform conversion for Standard Quotation Extraction
        await perform_conversion(
            convert_quotation_to_keyword,
            description="Quotation to Keyword Conversion (Standard)",
            input_file=config_standard_quotation['output_filename_primary'],
            output_dir='data',
            output_file='queries_keyword_standard.json'
        )

        # Run Keyword Extraction Pipeline for Standard Quotation
        logger.info("Starting Keyword Extraction Pipeline for Standard Quotation.")
        config_keyword_extraction_standard = config_keyword_extraction.copy()
        config_keyword_extraction_standard['queries_file_standard'] = 'data/input/queries_keyword_standard.json'
        await self.run_pipeline_with_config(
            config_keyword_extraction_standard,
            KeywordExtractionModule,
            self.initialize_keyword_optimizer
        )

        # After Keyword Extraction, perform conversion to Coding
        await perform_conversion(
            convert_keyword_to_coding,
            description="Keyword to Coding Conversion (Standard)",
            input_file=config_keyword_extraction_standard['output_filename_primary'],
            output_dir='data',
            output_file='queries_coding_standard.json'
        )

        # Run Coding Analysis Pipeline for Standard Keyword Extraction
        logger.info("Starting Coding Analysis Pipeline for Standard Keyword Extraction.")
        config_coding_analysis_standard = config_coding_analysis.copy()
        config_coding_analysis_standard['queries_file_standard'] = 'data/input/queries_coding_standard.json'
        await self.run_pipeline_with_config(
            config_coding_analysis_standard,
            CodingAnalysisModule,
            self.initialize_coding_optimizer
        )

        # After Coding Analysis, perform conversion to Theme (without ThemeAnalysisModule)
        await perform_conversion(
            convert_coding_to_theme,
            description="Coding to Theme Conversion (Standard)",
            input_file=config_coding_analysis_standard['output_filename_primary'],
            output_dir='data',
            output_file='queries_theme_standard.json'
        )

        # Since ThemeAnalysisModule is not available, we stop here

        elapsed_time = time.time() - start_time
        logger.info(f"All pipelines and conversion steps executed successfully up to Coding Analysis in {elapsed_time:.2f} seconds.")

    # Optionally, you can add more methods or helper functions here if needed.


if __name__ == "__main__":
    logger.info("Launching Thematic Analysis Pipeline.")
    pipeline = ThematicAnalysisPipeline()
    try:
        asyncio.run(pipeline.run_pipeline())
    except Exception as e:
        logger.critical(f"Pipeline execution failed: {e}", exc_info=True)
    finally:
        logger.info("Thematic Analysis Pipeline execution completed.")



################################################################################
# Module: processing
################################################################################


# File: processing/__init__.py
#------------------------------------------------------------------------------
"""
Processing modules for handling queries and generating answers.
"""

from .answer_generator import generate_answer_dspy, QuestionAnswerSignature
from .query_processor import validate_queries, process_queries

__all__ = [
    'generate_answer_dspy',
    'QuestionAnswerSignature',
    'validate_queries',
    'process_queries'
]

# File: processing/answer_generator.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
import dspy
import asyncio

from src.analysis.metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from src.utils.utils import check_answer_length

logger = logging.getLogger(__name__)

class QuestionAnswerSignature(dspy.Signature):
    input: str = dspy.InputField(
        desc=(
            "The combined input containing both the question and context. "
            "The format should be 'question: <question_text> context: <context_text>'."
        )
    )
    answer: str = dspy.OutputField(
        desc=(
            "The generated answer to the question. The answer should be concise, directly address "
            "the question, and be grounded in the provided context to ensure factual accuracy."
        )
    )

    def forward(self, input: str, max_tokens: int = 8192) -> Dict[str, str]:
        try:
            # Parse the input to extract question and context
            parts = input.split(' context: ', 1)
            question = parts[0].replace('question: ', '').strip()
            context = parts[1].strip() if len(parts) > 1 else ""

            logger.debug(f"Generating answer for question: '{question}' with context length: {len(context)} characters.")
            answer = self.language_model.generate(
                prompt=(
                    f"You are an expert in qualitative research and thematic analysis.\n\n"
                    f"**Guidelines**:\n"
                    f"- **Relevance:** Extract quotations that are closely related to the key themes.\n"
                    f"- **Diversity:** Ensure a range of perspectives and viewpoints.\n"
                    f"- **Clarity:** Choose clear and understandable quotations.\n"
                    f"- **Impact:** Select impactful quotations that highlight significant aspects of the data.\n"
                    f"- **Authenticity:** Maintain original expressions from participants.\n\n"
                    f"**Transcript Chunk**:\n{question}\n\n"
                    f"**Context:**\n{context}\n\n"
                    f"**Task:** Extract **3-5** relevant quotations from the transcript chunk based on the context provided. "
                    f"Provide each quotation in the following JSON format within a list:\n\n"
                    f"```json\n"
                    f"[\n"
                    f"    {{\"QUOTE\": \"This is the first quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the second quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the third quotation.\"}}\n"
                    f"]\n"
                    f"```"
                    f"Ensure that the response is a valid JSON array containing all relevant quotations. "
                    f"If no quotations are available, respond with an empty array `[]`."
                ),
                max_tokens=max_tokens,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=None
            ).strip()
            logger.info(f"Generated answer for question: '{question}'")
            logger.debug(f"Answer length: {len(answer)} characters.")
            return {"answer": answer}
        except Exception as e:
            logger.error(f"Error in QuestionAnswerSignature.forward: {e}", exc_info=True)
            return {"answer": "I'm sorry, I couldn't generate an answer at this time."}

try:
    qa_module = dspy.Program.load("optimized_program.json")
    logger.info("Optimized DSPy program loaded successfully.")
except Exception as e:
    try:
        qa_module = dspy.TypedChainOfThought(QuestionAnswerSignature)
        logger.info("Unoptimized DSPy module initialized successfully.")
    except Exception as inner_e:
        logger.error(f"Error initializing unoptimized DSPy module: {inner_e}", exc_info=True)
        raise

async def generate_answer(input: str, max_tokens: int = 8192) -> str:
    try:
        logger.debug(f"Generating answer for input with length: {len(input)} characters.")
        answer = await asyncio.to_thread(qa_module, input=input, max_tokens=max_tokens)
        return answer.get("answer", "I'm sorry, I couldn't generate an answer at this time.")
    except Exception as e:
        logger.error(f"Error in generate_answer: {e}", exc_info=True)
        return "I'm sorry, I couldn't generate an answer at this time."

async def evaluate_answer(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    try:
        logger.debug(f"Evaluating answer for input: '{example.get('input', '')}'")
        return await asyncio.to_thread(is_answer_fully_correct, example, pred)
    except Exception as e:
        logger.error(f"Error in evaluate_answer: {e}", exc_info=True)
        return False

async def generate_answer_dspy(query: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    logger.debug(f"Entering generate_answer_dspy with query='{query}' and {len(retrieved_chunks)} retrieved_chunks.")
    try:
        context = ""
        for i, chunk in enumerate(retrieved_chunks, 1):
            chunk_content = chunk['chunk'].get('original_content', '')
            chunk_context = chunk['chunk'].get('contextualized_content', '')
            context += f"Chunk {i}:\n"
            context += f"Content: {chunk_content}\n"
            context += f"Context: {chunk_context}\n\n"

        if not context.strip():
            logger.warning(f"No valid context found for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't find relevant information to answer your question.",
                "used_chunks": [],
                "num_chunks_used": 0
            }

        logger.debug(f"Formatted context with {len(retrieved_chunks)} sequential chunks:\n{context[:200]}...")
        used_chunks_info = [
            {
                "chunk_id": chunk['chunk'].get('chunk_id', ''),
                "doc_id": chunk['chunk'].get('doc_id', ''),
                "content_snippet": chunk['chunk'].get('original_content', '')[:100] + "..."
            }
            for chunk in retrieved_chunks
        ]
        logger.info(f"Total number of chunks used for context in query '{query}': {len(used_chunks_info)}")
        logger.info(f"Chunks used for context: {used_chunks_info}")
        input_data = f"question: {query} context: {context}"
        answer = await generate_answer(input_data)

        if not answer:
            logger.warning(f"No answer generated for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't generate an answer at this time.",
                "used_chunks": used_chunks_info,
                "num_chunks_used": len(used_chunks_info)
            }

        logger.debug(f"Generated answer for query '{query}': {answer}")
        logger.info(f"Number of chunks used for query '{query}': {len(used_chunks_info)}")
        example = {
            "context": context,
            "question": query
        }
        pred = {
            "answer": answer
        }
        suggestion = await evaluate_answer(example, pred)
        return {
            "answer": answer,
            "used_chunks": used_chunks_info,
            "num_chunks_used": len(used_chunks_info)
        }
    except Exception as e:
        logger.error(f"Error generating answer via DSPy for query '{query}': {e}", exc_info=True)
        return {
            "answer": "I'm sorry, I couldn't generate an answer at this time.",
            "used_chunks": [],
            "num_chunks_used": 0
        }

async def generate_answers_dspy(queries: List[str], retrieved_chunks_list: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    tasks = [
        generate_answer_dspy(query, retrieved_chunks)
        for query, retrieved_chunks in zip(queries, retrieved_chunks_list)
    ]
    return await asyncio.gather(*tasks)

def is_answer_factually_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    score = factuality_metric(example, pred)
    logger.debug(f"Factuality score: {score}")
    return score == 1


# File: processing/query_processor.py
#------------------------------------------------------------------------------

# src/processing/query_processor.py

import logging
from typing import List, Dict, Any
import json
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.retrieval.retrieval import multi_stage_retrieval
from src.utils.logger import setup_logging
from src.decorators import handle_exceptions
import dspy

from src.analysis.select_quotation_module import SelectQuotationModule, EnhancedQuotationModule
from src.analysis.extract_keyword_module import KeywordExtractionModule
from src.analysis.coding_module import CodingAnalysisModule  # Added import for CodingAnalysisModule

setup_logging()
logger = logging.getLogger(__name__)

def validate_queries(transcripts: List[Dict[str, Any]], module: dspy.Module) -> List[Dict[str, Any]]:
    """
    Validates the structure of input transcripts based on the module type.
    """
    valid_transcripts = []
    for idx, transcript in enumerate(transcripts):
        if isinstance(module, (SelectQuotationModule, EnhancedQuotationModule)):
            # Validation for quotation extraction modules
            if 'transcript_chunk' not in transcript or not isinstance(transcript['transcript_chunk'], str) or not transcript['transcript_chunk'].strip():
                logger.warning(f"Transcript at index {idx} is missing the 'transcript_chunk' field or it is empty/not a string. Skipping.")
                continue
        elif isinstance(module, KeywordExtractionModule):
            # Validation for keyword extraction module
            if 'quotation' not in transcript or not isinstance(transcript['quotation'], str) or not transcript['quotation'].strip():
                logger.warning(f"Transcript at index {idx} is missing the 'quotation' field or it is empty/not a string. Skipping.")
                continue
        elif isinstance(module, CodingAnalysisModule):
            # Validation for coding analysis module
            required_string_fields = ['quotation']
            required_list_fields = ['keywords']

            missing_fields = []
            # Check string fields
            for field in required_string_fields:
                if field not in transcript or not isinstance(transcript[field], str) or not transcript[field].strip():
                    missing_fields.append(field)
            # Check list fields
            for field in required_list_fields:
                if field not in transcript or not isinstance(transcript[field], list) or not all(isinstance(kw, str) and kw.strip() for kw in transcript[field]):
                    missing_fields.append(field)

            if missing_fields:
                logger.warning(f"Transcript at index {idx} is missing required fields {missing_fields} for coding analysis or they are empty/invalid. Skipping.")
                continue

        else:
            logger.warning(f"Unknown module type for transcript at index {idx}. Skipping.")
            continue
        valid_transcripts.append(transcript)

    logger.info(f"Validated {len(valid_transcripts)} transcripts out of {len(transcripts)} provided.")
    return valid_transcripts

def retrieve_documents(input_text: str, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int) -> List[Dict[str, Any]]:
    """
    Retrieves documents using multi-stage retrieval with contextual BM25.
    """
    logger.debug(f"Retrieving documents for input text: '{input_text[:100]}...' with top {k} results.")
    final_results = multi_stage_retrieval(input_text, db, es_bm25, k)
    logger.debug(f"Multi-stage retrieval returned {len(final_results)} results.")
    return final_results

@handle_exceptions
async def process_single_transcript_quotation(
    transcript_item: Dict[str, Any],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    module: dspy.Module
) -> Dict[str, Any]:
    """
    Processes a single transcript chunk for quotation extraction.
    """
    transcript_chunk = transcript_item.get('transcript_chunk', '').strip()
    if not transcript_chunk:
        logger.warning("Transcript chunk is empty. Skipping.")
        return {}

    logger.info(f"Processing transcript chunk for quotation: {transcript_chunk[:100]}...")

    # Retrieve and filter chunks
    retrieved_chunks = retrieve_documents(transcript_chunk, db, es_bm25, k)
    filtered_chunks = [chunk for chunk in retrieved_chunks if chunk['score'] >= 0.7]
    contextualized_contents = [chunk['chunk']['contextualized_content'] for chunk in filtered_chunks]

    research_objectives = transcript_item.get(
        'research_objectives',
        'Extract relevant quotations based on the provided objectives.'
    )
    theoretical_framework = transcript_item.get('theoretical_framework', {})

    # Process transcript using quotation module
    response = module.forward(
        research_objectives=research_objectives,
        transcript_chunk=transcript_chunk,
        contextualized_contents=contextualized_contents,
        theoretical_framework=theoretical_framework
    )
    
    # Prepare quotation-specific result dictionary
    result = {
        "transcript_info": response.get("transcript_info", {
            "transcript_chunk": transcript_chunk,
            "research_objectives": research_objectives,
            "theoretical_framework": theoretical_framework
        }),
        "retrieved_chunks": retrieved_chunks,
        "retrieved_chunks_count": len(retrieved_chunks),
        "filtered_chunks_count": len(filtered_chunks),
        "contextualized_contents": contextualized_contents,
        "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in filtered_chunks],
        "quotations": response.get("quotations", []),
        "analysis": response.get("analysis", {}),
        "answer": response.get("answer", {})
    }

    if not result["quotations"]:
        logger.warning(f"No quotations selected for transcript chunk: '{transcript_chunk[:100]}...'")
        result["answer"] = {"answer": "No relevant quotations were found to generate an answer."}

    logger.info(f"Selected {len(result['quotations'])} quotations for transcript chunk.")
    return result

@handle_exceptions
async def process_single_transcript_keyword(
    transcript_item: Dict[str, Any],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    module: dspy.Module
) -> Dict[str, Any]:
    """
    Processes a single transcript chunk for keyword extraction.
    """
    quotation = transcript_item.get('quotation', '').strip()
    if not quotation:
        logger.warning("Quotation is empty. Skipping.")
        return {}

    logger.info(f"Processing quotation for keywords: {quotation[:100]}...")

    # Retrieve and filter chunks
    retrieved_chunks = retrieve_documents(quotation, db, es_bm25, k)
    filtered_chunks = [chunk for chunk in retrieved_chunks if chunk['score'] >= 0.7]
    contextualized_contents = [chunk['chunk']['contextualized_content'] for chunk in filtered_chunks]

    research_objectives = transcript_item.get(
        'research_objectives',
        'Extract relevant keywords based on the provided objectives.'
    )
    theoretical_framework = transcript_item.get('theoretical_framework', {})

    # Process quotation using keyword module
    response = module.forward(
        research_objectives=research_objectives,
        quotation=quotation,
        contextualized_contents=contextualized_contents,
        theoretical_framework=theoretical_framework
    )
    
    # Prepare keyword-specific result dictionary
    result = {
        "quotation_info": response.get("quotation_info", {
            "quotation": quotation,
            "research_objectives": research_objectives,
            "theoretical_framework": theoretical_framework
        }),
        "retrieved_chunks": retrieved_chunks,
        "retrieved_chunks_count": len(retrieved_chunks),
        "filtered_chunks_count": len(filtered_chunks),
        "contextualized_contents": contextualized_contents,
        "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in filtered_chunks],
        "keywords": response.get("keywords", []),
        "analysis": response.get("analysis", {})
    }

    if not result["keywords"]:
        logger.warning(f"No keywords extracted for quotation: '{quotation[:100]}...'")
        result["analysis"]["error"] = "No relevant keywords were found to analyze."

    logger.info(f"Extracted {len(result['keywords'])} keywords for quotation.")
    return result

@handle_exceptions
async def process_single_transcript_coding(
    transcript_item: Dict[str, Any],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    module: dspy.Module
) -> Dict[str, Any]:
    """
    Processes a single transcript item for coding analysis.
    """
    quotation = transcript_item.get('quotation', '').strip()
    keywords = transcript_item.get('keywords', [])
    if not quotation:
        logger.warning("Quotation is missing or empty. Skipping.")
        return {}
    if not keywords:
        logger.warning("Keywords are missing or empty. Skipping.")
        return {}

    # Ensure keywords are a list of non-empty strings
    if not all(isinstance(kw, str) and kw.strip() for kw in keywords):
        logger.warning("One or more keywords are not valid strings. Skipping.")
        return {}

    logger.info(f"Processing transcript for coding analysis: Quotation='{quotation[:100]}...', Keywords={keywords}")

    # Retrieve and filter chunks
    retrieved_chunks = retrieve_documents(quotation, db, es_bm25, k)
    filtered_chunks = [chunk for chunk in retrieved_chunks if chunk['score'] >= 0.7]
    contextualized_contents = [chunk['chunk']['contextualized_content'] for chunk in filtered_chunks]

    research_objectives = transcript_item.get(
        'research_objectives',
        'Perform comprehensive coding analysis based on the provided quotation and keywords.'
    )
    theoretical_framework = transcript_item.get('theoretical_framework', {})

    # Process transcript using coding analysis module
    response = module.forward(
        research_objectives=research_objectives,
        quotation=quotation,
        keywords=keywords,
        contextualized_contents=contextualized_contents,
        theoretical_framework=theoretical_framework
    )

    # Prepare coding analysis-specific result dictionary
    result = {
        "coding_info": response.get("coding_info", {
            "quotation": quotation,
            "keywords": keywords,
            "research_objectives": research_objectives,
            "theoretical_framework": theoretical_framework
        }),
        "retrieved_chunks": retrieved_chunks,
        "retrieved_chunks_count": len(retrieved_chunks),
        "filtered_chunks_count": len(filtered_chunks),
        "contextualized_contents": contextualized_contents,
        "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in filtered_chunks],
        "codes": response.get("codes", []),  # Changed 'developed_codes' to 'codes'
        "analysis": response.get("analysis", {}),
    }

    if not result["codes"]:
        logger.warning(f"No codes developed for quotation: '{quotation[:100]}...'")
        result["analysis"]["error"] = "No relevant codes were developed for analysis."

    logger.info(f"Developed {len(result['codes'])} codes for coding analysis.")
    return result


def save_results(results: List[Dict[str, Any]], output_file: str):
    """
    Saves the processed transcript results to a specified output file.
    """
    try:
        with open(output_file, 'w', encoding='utf-8') as outfile:
            json.dump(results, outfile, indent=4)
        logger.info(f"All transcript results have been saved to '{output_file}'")
    except Exception as e:
        logger.error(f"Error saving results to '{output_file}': {e}", exc_info=True)

@handle_exceptions
async def process_queries(
    transcripts: List[Dict[str, Any]],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    output_file: str,
    optimized_program: dspy.Program,
    module: dspy.Module
):
    """
    Processes a list of transcript items based on the module type.
    """
    logger.info(f"Starting to process transcripts for output file '{output_file}'.")

    all_results = []
    try:
        # Determine the processing function based on module type
        if isinstance(module, (SelectQuotationModule, EnhancedQuotationModule)):
            process_func = process_single_transcript_quotation
        elif isinstance(module, KeywordExtractionModule):
            process_func = process_single_transcript_keyword
        elif isinstance(module, CodingAnalysisModule):
            process_func = process_single_transcript_coding
        else:
            logger.error("Unsupported module type provided.")
            return

        for idx, transcript_item in enumerate(tqdm(transcripts, desc="Processing transcripts")):
            try:
                result = await process_func(
                    transcript_item,
                    db,
                    es_bm25,
                    k,
                    module
                )
                if result:
                    all_results.append(result)
            except Exception as e:
                logger.error(f"Error processing transcript at index {idx}: {e}", exc_info=True)

        # Save results to the specified output file
        save_results(all_results, output_file)

    except KeyboardInterrupt:
        logger.warning("Process interrupted by user. Saving partial results.")
        save_results(all_results, output_file)
        raise

    except Exception as e:
        logger.error(f"Error processing transcripts: {e}", exc_info=True)
        raise



################################################################################
# Module: retrieval
################################################################################


# File: retrieval/__init__.py
#------------------------------------------------------------------------------
"""
Retrieval functionality for searching and ranking content.
"""

from .query_generator import QueryGeneratorSignature
from .retrieval import hybrid_retrieval, multi_stage_retrieval
from .reranking import retrieve_with_reranking
from .reranker import SentenceTransformerReRanker

__all__ = [
    'QueryGeneratorSignature',
    'hybrid_retrieval',
    'multi_stage_retrieval',
    'retrieve_with_reranking',
    'SentenceTransformerReRanker'
]

# File: retrieval/query_generator.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Example/src/query_generator.py
# query_generator.py
import dspy
import logging
from typing import Dict

from src.utils.logger import setup_logging

logger = logging.getLogger(__name__)

class QueryGeneratorSignature(dspy.Signature):
    question: str = dspy.InputField(desc="The original user question.")
    context: str = dspy.InputField(desc="The accumulated context from previous retrievals.")
    new_query: str = dspy.OutputField(desc="The generated query for the next retrieval hop.")

    def forward(self, question: str, context: str) -> Dict[str, str]:
        try:
            if not question or not context:
                raise ValueError("Both 'question' and 'context' must be provided and non-empty.")
            
            prompt = (
                f"Given the question: '{question}'\n"
                f"and the context retrieved so far:\n{context}\n"
                "Generate a search query that will help find additional information needed to answer the question."
            )
            new_query = self.language_model.generate(
                prompt=prompt,
                max_tokens=50,
                temperature=1.0,
                top_p=0.9,
                n=1,
                stop=["\n"]
            ).strip()
            logger.info(f"Generated new query: '{new_query}'")
            return {"new_query": new_query}
        except ValueError as ve:
            logger.error(f"ValueError in QueryGeneratorSignature.forward: {ve}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question
        except Exception as e:
            logger.error(f"Error in QueryGeneratorSignature.forward: {e}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question


# File: retrieval/reranker.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util
import torch

from src.utils.logger import setup_logging
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class SentenceTransformerReRanker:
    """
    Re-ranker using Sentence Transformers for semantic similarity.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        """
        Initializes the Sentence Transformer re-ranker.
        
        Args:
            model_name (str): Pre-trained Sentence Transformer model name.
            device (str, optional): Device to run the model on ('cuda', 'mps', or 'cpu'). Defaults to automatic selection.
        """
        self.model = SentenceTransformer(model_name)
        
        # Set device
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'.")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Re-ranks documents based on semantic similarity to the query.
        
        Args:
            query (str): The search query.
            documents (List[str]): List of document contents to re-rank.
            top_k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
        
        Returns:
            List[Dict[str, Any]]: List of re-ranked documents with similarity scores.
        """
        if not query or not documents:
            logger.warning("Query and documents must be provided for re-ranking.")
            return []

        try:
            logger.debug("Encoding query and documents.")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities.")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)  # Adjust k to the number of available documents
            if actual_k == 0:
                logger.warning("No documents available for re-ranking.")
                return []
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}.")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            logger.info(f"Re-ranked {actual_k} documents based on semantic similarity.")
            return re_ranked_docs
        except Exception as e:
            logger.error(f"Error during re-ranking with Sentence Transformers: {e}", exc_info=True)
            return []

def rerank_documents_sentence_transformer(query: str, retrieved_docs: List[Dict[str, Any]], k: int = 20) -> List[Dict[str, Any]]:
    """
    Re-ranks the retrieved documents using Sentence Transformers.
    
    Args:
        query (str): The search query.
        retrieved_docs (List[Dict[str, Any]]): List of retrieved documents.
        k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.info(f"Starting re-ranking of {len(retrieved_docs)} documents for query: '{query}' using Sentence Transformers.")
    if not query or not retrieved_docs:
        logger.warning(f"Query and retrieved documents must be provided for re-ranking.")
        return []
    try:
        # Initialize the re-ranker
        reranker = SentenceTransformerReRanker()
        
        # Extract document contents
        documents = [doc['chunk']['original_content'] for doc in retrieved_docs]
        
        # Perform re-ranking
        re_ranked = reranker.rerank(query, documents, top_k=k)
        
        # Attach scores back to the documents
        re_ranked_docs = []
        for r, original_doc in zip(re_ranked, retrieved_docs[:len(re_ranked)]):  # Ensure matching length
            re_ranked_docs.append({
                "chunk": original_doc['chunk'],
                "score": r['score']
            })
        
        logger.info(f"Re-ranking completed using Sentence Transformers. Top {k} documents selected.")
        return re_ranked_docs
    except Exception as e:
        logger.error(f"Error during document re-ranking with Sentence Transformers: {e}", exc_info=True)
        return retrieved_docs[:k]  # Fallback to original ranking if re-ranking fails


# File: retrieval/reranking.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Example/src/reranking.py
import logging
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util
import torch
import time

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.retrieval.retrieval import hybrid_retrieval
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class SentenceTransformerReRanker:
    """
    Re-ranker using Sentence Transformers for semantic similarity.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        """
        Initializes the Sentence Transformer re-ranker.
        
        Args:
            model_name (str): Pre-trained Sentence Transformer model name.
            device (str, optional): Device to run the model on ('cuda', 'mps', or 'cpu'). Defaults to automatic selection.
        """
        self.model = SentenceTransformer(model_name)
        
        # Set device
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'.")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Re-ranks documents based on semantic similarity to the query.
        
        Args:
            query (str): The search query.
            documents (List[str]): List of document contents to re-rank.
            top_k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
        
        Returns:
            List[Dict[str, Any]]: List of re-ranked documents with similarity scores.
        """
        if not query or not documents:
            logger.warning("Query and documents must be provided for re-ranking.")
            return []

        try:
            logger.debug("Encoding query and documents.")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities.")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)  # Adjust k to the number of available documents
            if actual_k == 0:
                logger.warning("No documents available for re-ranking.")
                return []
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}.")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            logger.info(f"Re-ranked {actual_k} documents based on semantic similarity.")
            return re_ranked_docs
        except Exception as e:
            logger.error(f"Error during re-ranking with Sentence Transformers: {e}", exc_info=True)
            return []

def rerank_results(query: str, retrieved_docs: List[Dict[str, Any]], k: int = 20) -> List[Dict[str, Any]]:
    """
    Re-ranks the retrieved documents using Sentence Transformers.
    
    Args:
        query (str): The search query.
        retrieved_docs (List[Dict[str, Any]]): List of retrieved documents.
        k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.info(f"Starting re-ranking of {len(retrieved_docs)} documents for query: '{query}' using Sentence Transformers.")
    if not query or not retrieved_docs:
        logger.warning(f"Query and retrieved documents must be provided for re-ranking.")
        return []
    try:
        # Initialize the re-ranker
        reranker = SentenceTransformerReRanker()
        
        # Extract document contents
        documents = [doc['chunk']['original_content'] for doc in retrieved_docs]
        
        # Perform re-ranking
        re_ranked = reranker.rerank(query, documents, top_k=k)
        
        # Attach scores back to the documents
        re_ranked_docs = []
        for r, original_doc in zip(re_ranked, retrieved_docs[:len(re_ranked)]):  # Ensure matching length
            re_ranked_docs.append({
                "chunk": original_doc['chunk'],
                "score": r['score']
            })
        
        logger.info(f"Re-ranking completed using Sentence Transformers. Top {k} documents selected.")
        return re_ranked_docs
    except Exception as e:
        logger.error(f"Error during document re-ranking with Sentence Transformers: {e}", exc_info=True)
        return retrieved_docs[:k]  # Fallback to original ranking if re-ranking fails

def retrieve_with_reranking(query: str, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int) -> List[Dict[str, Any]]:
    """
    Retrieves documents using hybrid retrieval and re-ranks them using Sentence Transformers.
    
    Args:
        query (str): The search query.
        db (ContextualVectorDB): Contextual vector database instance.
        es_bm25 (ElasticsearchBM25): Elasticsearch BM25 instance.
        k (int): Number of top documents to retrieve.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.debug(f"Entering retrieve_with_reranking method with query='{query}' and k={k}.")
    start_time = time.time()

    try:
        logger.debug(f"Performing hybrid retrieval for query: '{query}'")
        initial_results = hybrid_retrieval(query, db, es_bm25, k=k*10)
        logger.debug(f"Initial hybrid retrieval returned {len(initial_results)} results.")

        # **Add logging for all initial chunks retrieved**
        logger.info(f"Total chunks retrieved during hybrid retrieval for query '{query}': {len(initial_results)}")
        logger.info(f"Chunk IDs retrieved during hybrid retrieval: {[res['chunk']['chunk_id'] for res in initial_results]}")

        if not initial_results:
            logger.warning(f"No initial results retrieved for query '{query}'. Skipping reranking.")
            return []

        # Re-rank the retrieved documents
        final_results = rerank_results(query, initial_results, top_k=5)  # Set top_k to 5

    except Exception as e:
        logger.error(f"Error during retrieval or re-ranking for query '{query}': {e}", exc_info=True)
        return []

    end_time = time.time()
    logger.debug(f"Exiting retrieve_with_reranking method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results


# File: retrieval/retrieval.py
#------------------------------------------------------------------------------
# File: retrieval.py
import logging
import time
from typing import List, Dict, Any
import dspy

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.retrieval.query_generator import QueryGeneratorSignature
from src.utils.utils import compute_similarity
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)


def hybrid_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    semantic_weight: float = 0.2,
    bm25_content_weight: float = 0.2,
    bm25_contextual_weight: float = 0.6,
    min_chunks: int = 1
) -> List[Dict[str, Any]]:
    """
    Performs hybrid retrieval by combining FAISS semantic search and dual BM25 contextual search using Reciprocal Rank Fusion.
    """
    logger.debug(
        f"Entering hybrid_retrieval with query='{query}', k={k}, "
        f"semantic_weight={semantic_weight}, bm25_content_weight={bm25_content_weight}, "
        f"bm25_contextual_weight={bm25_contextual_weight}, min_chunks={min_chunks}."
    )
    start_time = time.time()
    num_chunks_to_recall = k * 10  # Retrieve more to improve chances

    # Initialize reciprocal rank fusion score dictionary
    chunk_id_to_score = {}

    while True:
        # Semantic search
        logger.debug(f"Performing semantic search using FAISS for query: '{query}'")
        semantic_results = db.search(query, k=num_chunks_to_recall)
        ranked_semantic = [result['chunk_id'] for result in semantic_results]
        semantic_scores = {result['chunk_id']: result['score'] for result in semantic_results}
        logger.debug(f"Semantic search retrieved {len(ranked_semantic)} chunk IDs.")

        # BM25 search on 'content'
        logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        bm25_content_results = es_bm25.search_content(query, k=num_chunks_to_recall)
        ranked_bm25_content = [result['chunk_id'] for result in bm25_content_results]
        bm25_content_scores = {result['chunk_id']: result['score'] for result in bm25_content_results}
        logger.debug(f"BM25 'content' search retrieved {len(ranked_bm25_content)} chunk IDs.")

        # BM25 search on 'contextualized_content'
        logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        bm25_contextual_results = es_bm25.search_contextualized(query, k=num_chunks_to_recall)
        ranked_bm25_contextual = [result['chunk_id'] for result in bm25_contextual_results]
        bm25_contextual_scores = {result['chunk_id']: result['score'] for result in bm25_contextual_results}
        logger.debug(f"BM25 'contextualized_content' search retrieved {len(ranked_bm25_contextual)} chunk IDs.")

        # Combine all unique chunk IDs
        chunk_ids = list(set(ranked_semantic + ranked_bm25_content + ranked_bm25_contextual))
        logger.debug(f"Total unique chunk IDs after combining: {len(chunk_ids)}")

        # Calculate Reciprocal Rank Fusion scores
        for chunk_id in chunk_ids:
            score = 0
            if chunk_id in ranked_semantic:
                index = ranked_semantic.index(chunk_id)
                score += semantic_weight * (1 / (index + 1))
                logger.debug(
                    f"Added semantic RRF score for chunk_id {chunk_id}: "
                    f"{semantic_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_content:
                index = ranked_bm25_content.index(chunk_id)
                score += bm25_content_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_content_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_contextual:
                index = ranked_bm25_contextual.index(chunk_id)
                score += bm25_contextual_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'contextualized_content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_contextual_weight * (1 / (index + 1))}"
                )
            chunk_id_to_score[chunk_id] = score

        # Sort chunk IDs by their RRF scores in descending order
        sorted_chunk_ids = sorted(
            chunk_id_to_score.keys(),
            key=lambda x: chunk_id_to_score[x],
            reverse=True
        )
        logger.debug(f"Sorted chunk IDs based on RRF scores.")

        # Select top k chunks, ensuring at least min_chunks are returned
        final_results = []
        filtered_count = 0
        for chunk_id in sorted_chunk_ids[:k]:
            chunk_metadata = next(
                (chunk for chunk in db.metadata if chunk['chunk_id'] == chunk_id),
                None
            )
            if not chunk_metadata:
                filtered_count += 1
                logger.warning(f"Chunk metadata not found for chunk_id {chunk_id}")
                continue
            final_results.append({
                'chunk': chunk_metadata,
                'score': chunk_id_to_score[chunk_id]
            })

        logger.info(f"Filtered {filtered_count} chunks due to missing metadata.")
        logger.info(
            f"Total chunks retrieved after filtering: {len(final_results)} "
            f"(required min_chunks={min_chunks})"
        )

        if len(final_results) >= min_chunks or k >= num_chunks_to_recall:
            break
        else:
            k += 5  # Increment k to retrieve more chunks
            logger.info(
                f"Number of retrieved chunks ({len(final_results)}) is less than min_chunks ({min_chunks}). "
                f"Increasing k to {k} and retrying retrieval."
            )

    logger.debug(f"Hybrid retrieval returning {len(final_results)} chunks.")
    logger.info(
        f"Chunks used for hybrid retrieval for query '{query}': "
        f"[{', '.join([res['chunk']['chunk_id'] for res in final_results])}]"
    )
    end_time = time.time()
    logger.debug(f"Exiting hybrid_retrieval method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results


def multi_stage_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    max_hops: int = 3,
    max_results: int = 5,
    similarity_threshold: float = 0.9
) -> List[Dict[str, Any]]:
    accumulated_context = ""
    all_retrieved_chunks = {}
    current_query = query
    previous_query = ""
    query_generator = dspy.TypedChainOfThought(QueryGeneratorSignature)

    for hop in range(max_hops):
        logger.info(f"Starting hop {hop+1} with query: '{current_query}'")

        retrieved_chunks = hybrid_retrieval(current_query, db, es_bm25, k)

        # Update accumulated retrieved chunks
        for chunk in retrieved_chunks:
            chunk_id = chunk['chunk']['chunk_id']
            if chunk_id not in all_retrieved_chunks:
                all_retrieved_chunks[chunk_id] = chunk

        # Check if we have reached the desired number of results
        if len(all_retrieved_chunks) >= max_results:
            logger.info(f"Retrieved sufficient chunks ({len(all_retrieved_chunks)}). Terminating.")
            break

        # Accumulate new context from retrieved chunks
        new_context = "\n\n".join([
            chunk['chunk'].get('contextualized_content', '') or chunk['chunk'].get('original_content', '')
            for chunk in retrieved_chunks
        ])
        accumulated_context += "\n\n" + new_context

        # Generate a new query based on the accumulated context
        response = query_generator(question=query, context=accumulated_context)
        new_query = response.get('new_query', '').strip()
        if not new_query:
            logger.info("No new query generated. Terminating multi-stage retrieval.")
            break

        # Compute similarity between the new query and the previous query
        similarity = compute_similarity(current_query, new_query)
        logger.debug(f"Similarity between queries: {similarity:.4f}")
        if similarity >= similarity_threshold:
            logger.info("New query is too similar to the current query. Terminating multi-stage retrieval.")
            break

        # Update queries for the next iteration
        previous_query = current_query
        current_query = new_query

    # Sort and return the top results
    final_results = sorted(
        all_retrieved_chunks.values(),
        key=lambda x: x['score'],
        reverse=True
    )[:max_results]

    logger.info(f"Multi-stage retrieval completed with {len(final_results)} chunks.")
    return final_results



################################################################################
# Module: utils
################################################################################


# File: utils/__init__.py
#------------------------------------------------------------------------------
"""
Utility functions and helpers.
"""

from decorators import handle_exceptions
from .logger import setup_logging
from .utils import check_answer_length, compute_similarity
from .validation_functions import validate_relevance, validate_quality, validate_context_clarity

__all__ = [
    'handle_exceptions',
    'setup_logging',
    'check_answer_length',
    'compute_similarity',
    'validate_relevance',
    'validate_quality',
    'validate_context_clarity'
]

# File: utils/history_utils.py
#------------------------------------------------------------------------------
# Add this to src/utils/history_utils.py

import os
import json
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class HistoryManager:
    def __init__(self, base_dir='interaction_histories'):
        self.base_dir = base_dir
        self._ensure_directory()

    def _ensure_directory(self):
        """Ensures the history directory exists."""
        try:
            os.makedirs(self.base_dir, exist_ok=True)
            logger.info(f"Ensured history directory exists at: {self.base_dir}")
        except Exception as e:
            logger.error(f"Error creating history directory: {e}")

    def save_history(self, history, prefix='quotation'):
        """Saves a history entry with timestamp."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}_interaction_{timestamp}.json"
            filepath = os.path.join(self.base_dir, filename)

            history_data = {
                'timestamp': timestamp,
                'history': history
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2)
            
            logger.info(f"Saved interaction history to: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"Error saving history: {e}")
            return None

    def get_latest_history(self, prefix='quotation'):
        """Gets the most recent history file."""
        try:
            files = [f for f in os.listdir(self.base_dir) if f.startswith(prefix)]
            if not files:
                return None
            latest_file = max(files, key=lambda x: os.path.getctime(os.path.join(self.base_dir, x)))
            with open(os.path.join(self.base_dir, latest_file), 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error getting latest history: {e}")
            return None

# File: utils/logger.py
#------------------------------------------------------------------------------
# src/utils/logger.py:

import logging
import logging.config
import os
import yaml
from typing import Optional
from functools import wraps
import time

def setup_logging(
    default_path: str = 'config/logging_config.yaml',
    default_level: int = logging.INFO,
    env_key: str = 'LOG_CFG'
) -> None:
    """
    Setup logging configuration with enhanced error handling and directory creation.
    
    Args:
        default_path: Path to the logging configuration file
        default_level: Default logging level if config file is not found
        env_key: Environment variable that can be used to override the config path
    """
    try:
        path = os.getenv(env_key, default_path)
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            # Create log directories for all handlers
            handlers = config.get('handlers', {})
            for handler in handlers.values():
                if 'filename' in handler:
                    log_dir = os.path.dirname(handler['filename'])
                    if log_dir:
                        os.makedirs(log_dir, exist_ok=True)
            
            logging.config.dictConfig(config)
            logging.info(f"Logging configuration loaded from {path}")
        else:
            logging.basicConfig(level=default_level)
            logging.warning(f"Logging config file not found at {path}. Using basic config.")
    except Exception as e:
        logging.basicConfig(level=default_level)
        logging.error(f"Error in logging configuration: {str(e)}")

def get_logger(name: str) -> logging.Logger:
    """
    Get a logger with the specified name and adds extra handlers if needed.
    
    Args:
        name: Name for the logger
        
    Returns:
        logging.Logger: Configured logger instance
    """
    logger = logging.getLogger(name)
    
    # Add a null handler if no handlers exist
    if not logger.handlers:
        logger.addHandler(logging.NullHandler())
    
    return logger

def log_execution_time(logger: Optional[logging.Logger] = None):
    """
    Decorator to log function execution time.
    
    Args:
        logger: Logger instance to use. If None, creates a new logger.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            nonlocal logger
            if logger is None:
                logger = get_logger(func.__module__)
            
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                logger.debug(
                    f"Function '{func.__name__}' executed in {execution_time:.2f} seconds"
                )
                return result
            except Exception as e:
                execution_time = time.time() - start_time
                logger.error(
                    f"Function '{func.__name__}' failed after {execution_time:.2f} seconds. "
                    f"Error: {str(e)}"
                )
                raise
        return wrapper
    return decorator


# File: utils/utils.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/utils.py

import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

def check_answer_length(answer: str, max_length: int = 500) -> bool:
    """
    Checks if the answer length is within the specified limit.

    Args:
        answer (str): The generated answer to evaluate.
        max_length (int, optional): The maximum allowed length for the answer. Defaults to 500.

    Returns:
        bool: True if the answer length is within the limit, False otherwise.
    """
    return len(answer) <= max_length

def compute_similarity(query1: str, query2: str) -> float:
    """
    Computes cosine similarity between two queries using TF-IDF vectorization.

    Args:
        query1 (str): The first query string.
        query2 (str): The second query string.

    Returns:
        float: Cosine similarity score between query1 and query2.
    """
    try:
        vectorizer = TfidfVectorizer().fit_transform([query1, query2])
        vectors = vectorizer.toarray()
        similarity = cosine_similarity([vectors[0]], [vectors[1]])[0][0]
        return similarity
    except Exception as e:
        logger.error(f"Error computing similarity: {e}", exc_info=True)
        return 0.0


# File: utils/validation_functions.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/validation_functions.py

import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def validate_relevance(quotations: List[str], research_objectives: str) -> bool:
    """
    Validates that each quotation is relevant to the research objectives.
    """
    try:
        for quote in quotations:
            # Simple keyword matching; can be enhanced with NLP techniques
            if not any(obj.lower() in quote.lower() for obj in research_objectives.split()):
                logger.debug(f"Quotation '{quote}' is not relevant to the research objectives.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_relevance: {e}", exc_info=True)
        return False

def validate_quality(quotations: List[Dict[str, Any]]) -> bool:
    """
    Validates the quality and representation of each quotation.
    """
    try:
        for quote in quotations:
            if len(quote["quotation"].strip()) < 10:  # Example quality check
                logger.debug(f"Quotation '{quote['QUOTE']}' is too short to be considered high quality.")
                return False
            # Additional quality checks can be added here
        return True
    except Exception as e:
        logger.error(f"Error in validate_quality: {e}", exc_info=True)
        return False

def validate_context_clarity(quotations: List[str], context: str) -> bool:
    """
    Validates that each quotation is clear and has sufficient context.
    """
    try:
        for quote in quotations:
            if quote.lower() not in context.lower():
                logger.debug(f"Quotation '{quote}' lacks sufficient context.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_context_clarity: {e}", exc_info=True)
        return False

