# Consolidated Source Code
# ==============================================================================



################################################################################
# Module: root
################################################################################


# File: __init__.py
#------------------------------------------------------------------------------
"""
Thematic Analysis Package
A package for analyzing textual data using thematic analysis techniques.
"""

__version__ = '0.1.0'


################################################################################
# Module: analysis
################################################################################


# File: analysis/__init__.py
#------------------------------------------------------------------------------
"""
Analysis modules for thematic analysis.
Contains metrics and quotation selection functionality.
"""

from .metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from .select_quotation import SelectQuotationSignature
from .select_quotation_module import SelectQuotationModule
import src.decorators as decorators

__all__ = [
    'comprehensive_metric',
    'is_answer_fully_correct',
    'factuality_metric',
    'SelectQuotationSignature',
    'SelectQuotationModule'
]



# File: analysis/extract_keywords.py
#------------------------------------------------------------------------------
import logging
import json
from typing import List, Dict, Any
import dspy
from pathlib import Path
import os

logger = logging.getLogger(__name__)

class KeywordExtractionSignature(dspy.Signature):
    """
    Extract keywords from quotations with simplified input/output format.
    """
    research_objectives: str = dspy.InputField(
        desc="The research objectives guiding the extraction of keywords."
    )
    quotations: List[Dict[str, str]] = dspy.InputField(
        desc="List containing quotations with 'quotation' field."
    )
    keywords: List[str] = dspy.OutputField(
        desc="List of extracted keywords for the provided quote."
    )

    def forward(self, research_objectives: str, quotations: List[Dict[str, str]]) -> Dict[str, List[str]]:
        try:
            if not quotations:
                logger.warning("No quotations provided for keyword extraction.")
                return {"keywords": []}
            
            quotes = [quote.get("quotation", "") for quotation in quotations]
            valid_quotes = [quote for quote in quotes if quote]

            if not valid_quotes:
                logger.warning("No valid quotes provided for keyword extraction.")
                return {"keywords": []}

            keywords_mapping = []

            for quote in valid_quotes:
                logger.debug("Starting keyword extraction process.")
                prompt = (
                    f"You are an expert in qualitative research and thematic analysis.\n\n"
                    f"Research Objectives: {research_objectives}\n\n"
                    f"Analyze the following quote:\n\"{quote}\"\n\n"
                    f"Extract key terms, concepts, and themes from this quotation. "
                    f"Return ONLY a list of single words or short phrases (2-3 words maximum) "
                    f"that represent the main concepts, without any additional metadata.\n"
                    f"Focus on important themes mentioned in the quote."
                )

                response = self.language_model.generate(
                    prompt=prompt,
                    max_tokens=150,
                    temperature=0.5,
                    top_p=0.9,
                    n=1,
                    stop=None
                ).strip()

                # Process the response to extract clean keywords
                keywords = [
                    keyword.strip().lower()
                    for keyword in response.split('\n')
                    if keyword.strip() and not keyword.startswith(('-', '*', 'â€¢', '1.', '2.'))
                ]

                # Remove duplicates while preserving order
                seen = set()
                unique_keywords = [x for x in keywords if not (x in seen or seen.add(x))]

                keywords_mapping.append({"quote": quote, "keywords": unique_keywords})

            logger.info(f"Extracted keywords for {len(valid_quotes)} quotes.")
            return {"keywords": keywords_mapping}
            
        except Exception as e:
            logger.error(f"Error in keyword extraction: {e}", exc_info=True)
            return {"keywords": []}

def load_quotations(input_file: str) -> List[Dict[str, str]]:
    """Load quotations from input JSON file."""
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get("quotations", [])
    except Exception as e:
        logger.error(f"Error loading quotations from {input_file}: {e}")
        return []

def save_keywords(keywords_mapping: List[Dict[str, Any]], output_file: str):
    """Save extracted keywords mapping to output JSON file."""
    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(keywords_mapping, f, indent=4)
        logger.info(f"Keywords mapping saved to {output_file}")
    except Exception as e:
        logger.error(f"Error saving keywords to {output_file}: {e}")


# File: analysis/extract_keywords_module.py
#------------------------------------------------------------------------------
# File: analysis/extract_keywords_module.py
#------------------------------------------------------------------------------
import logging
from typing import Dict, Any
import dspy
import json

from .extract_keywords import KeywordExtractionSignature, save_keywords

logger = logging.getLogger(__name__)

class KeywordExtractionModule(dspy.Module):
    """
    DSPy module to extract keywords from quotations.
    """
    def __init__(self, input_file: str = "query_results.json", output_file: str = "data/keywords.json"):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(KeywordExtractionSignature)
        self.input_file = input_file
        self.output_file = output_file

    def process_file(self, input_file: str, research_objectives: str) -> Dict[str, Any]:
        """Process a single input file and extract keywords for each quote."""
        try:
            logger.debug(f"Processing file: {input_file}")
            
            # Load quotations from input file
            with open(input_file, 'r', encoding='utf-8') as file:
                data = json.load(file)
            
            quotes = []
            for item in data:
                # Extract quotes only from quotations
                for quotation in item.get("quotations", []):
                    quotes.append({"quote": quotation.get("quote", "")})

            if not quotes:
                logger.warning(f"No quotations found in {input_file}")
                return {"keywords": []}

            keywords_mapping = []
            for idx, quote_dict in enumerate(quotes):
                quote = quote_dict.get("quote", "")
                if not quote:
                    logger.warning(f"Quote at index {idx} is empty.")
                    keywords_mapping.append({"quote": quote, "keywords": []})
                    continue

                logger.debug(f"Extracting keywords for quote {idx+1}: {quote}")
                # Extract keywords for the current quote
                response = self.chain(
                    research_objectives=research_objectives,
                    quotations=[quote_dict]
                )
                keywords = response.get("keywords", [])

                keywords_mapping.append({"quote": quote, "keywords": keywords})

            # Save the mapping of quotes to keywords
            save_keywords(keywords_mapping, self.output_file)
            
            return {"keywords": keywords_mapping, "output_file": self.output_file}
            
        except Exception as e:
            logger.error(f"Error processing file {input_file}: {e}", exc_info=True)
            return {"keywords": []}


# File: analysis/metrics.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any, Callable
import dspy

from src.utils.utils import check_answer_length
from src.utils.logger import setup_logging
# Initialize logger
logger = logging.getLogger(__name__)

class BaseAssessment(dspy.Signature):
    """
    Base class for all assessment signatures.
    """
    context: str = dspy.InputField(
        desc=(
            "The contextual information provided to generate the answer. This includes all relevant "
            "documents, data chunks, or information sources that the answer is based upon."
        )
    )
    question: str = dspy.InputField(
        desc=(
            "The original question that was posed. This is used to understand the intent and scope "
            "of the answer in relation to the provided context."
        )
    )
    answer: str = dspy.InputField(
        desc=(
            "The answer generated by the system that needs to be evaluated for factual correctness "
            "based on the provided context."
        )
    )

    def generate_prompt(self, context: str, question: str, answer: str, task: str) -> str:
        return (
            f"Context: {context}\n"
            f"Question: {question}\n"
            f"Answer: {answer}\n\n"
            f"{task}"
        )

class Assess(BaseAssessment):
    """
    Assess the factual correctness of an answer based on the provided context.

    This signature evaluates whether the generated answer accurately reflects the information
    present in the given context. It leverages a language model to perform a nuanced analysis
    beyond simple keyword matching, ensuring a thorough assessment of factual accuracy.
    """
    factually_correct: str = dspy.OutputField(
        desc=(
            "Indicator of whether the answer is factually correct based on the context. "
            "Should be 'Yes' if the answer accurately reflects the information in the context, "
            "and 'No' otherwise."
        )
    )

    def forward(self, context: str, question: str, answer: str) -> Dict[str, str]:
        try:
            logger.debug(f"Assessing factual correctness for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Based on the context provided, evaluate whether the answer is factually correct.\n"
                "Respond with 'Yes' if the answer accurately reflects the information in the context.\n"
                "Respond with 'No' if the answer contains factual inaccuracies or is not supported by the context.\n"
                "Please respond with 'Yes' or 'No' only."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=3,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Factuality assessment response: '{response}'")
            if response.lower() in ['yes', 'no']:
                result = response.capitalize()
            else:
                logger.warning(f"Unexpected response from factuality assessment: '{response}'. Defaulting to 'No'.")
                result = 'No'
            logger.info(f"Factuality assessment result: {result} for question: '{question}'")
            return {"factually_correct": result}
        except Exception as e:
            logger.error(f"Error in Assess.forward: {e}", exc_info=True)
            return {"factually_correct": "No"}


class AssessRelevance(BaseAssessment):
    """
    Assess the relevance of an answer to the given question and context.
    
    This signature evaluates whether the generated answer directly and comprehensively 
    addresses the user's query, considering the provided context.
    """
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug(f"Assessing relevance for question: '{question}'")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the relevance of the answer to the question based on the context.\n"
                "Provide a relevance score between 1 (not relevant) and 5 (highly relevant)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Relevance assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected relevance score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid relevance score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Relevance assessment result: {result} for question: '{question}'")
            return {"relevance_score": result}
        except Exception as e:
            logger.error(f"Error in AssessRelevance.forward: {e}", exc_info=True)
            return {"relevance_score": 1}


class AssessCoherence(BaseAssessment):
    """
    Assess the coherence of an answer.
    
    This signature evaluates whether the generated answer is well-structured and logically consistent.
    """
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing coherence of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the coherence of the above answer.\n"
                "Provide a coherence score between 1 (not coherent) and 5 (highly coherent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Coherence assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected coherence score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid coherence score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Coherence assessment result: {result}")
            return {"coherence_score": result}
        except Exception as e:
            logger.error(f"Error in AssessCoherence.forward: {e}", exc_info=True)
            return {"coherence_score": 1}


class AssessConciseness(BaseAssessment):
    """
    Assess the conciseness of an answer.
    
    This signature evaluates whether the generated answer is succinct and free from unnecessary verbosity.
    """
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing conciseness of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the conciseness of the above answer.\n"
                "Provide a conciseness score between 1 (not concise) and 5 (highly concise)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Conciseness assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected conciseness score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid conciseness score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Conciseness assessment result: {result}")
            return {"conciseness_score": result}
        except Exception as e:
            logger.error(f"Error in AssessConciseness.forward: {e}", exc_info=True)
            return {"conciseness_score": 1}


class AssessFluency(BaseAssessment):
    """
    Assess the fluency of an answer.
    
    This signature evaluates whether the generated answer exhibits natural language flow and is free from grammatical errors.
    """
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, int]:
        try:
            logger.debug("Assessing fluency of the answer.")
            prompt = self.generate_prompt(
                context, question, answer,
                "Evaluate the fluency of the above answer.\n"
                "Provide a fluency score between 1 (not fluent) and 5 (highly fluent)."
            )
            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=1,
                temperature=0.0,
                top_p=1.0,
                n=1,
                stop=["\n"]
            ).strip()
            logger.debug(f"Fluency assessment response: '{response}'")
            try:
                score = int(response)
                if 1 <= score <= 5:
                    result = score
                else:
                    logger.warning(f"Unexpected fluency score: '{response}'. Defaulting to 1.")
                    result = 1
            except ValueError:
                logger.warning(f"Invalid fluency score format: '{response}'. Defaulting to 1.")
                result = 1
            logger.info(f"Fluency assessment result: {result}")
            return {"fluency_score": result}
        except Exception as e:
            logger.error(f"Error in AssessFluency.forward: {e}", exc_info=True)
            return {"fluency_score": 1}


class ComprehensiveAssessment(BaseAssessment):
    """
    Comprehensive assessment of generated answers across multiple dimensions.
    """
    factually_correct: str = dspy.OutputField(desc="Whether the answer is factually correct ('Yes'/'No').")
    relevance_score: int = dspy.OutputField(desc="A score indicating relevance (1-5).")
    coherence_score: int = dspy.OutputField(desc="A score indicating coherence (1-5).")
    conciseness_score: int = dspy.OutputField(desc="A score indicating conciseness (1-5).")
    fluency_score: int = dspy.OutputField(desc="A score indicating fluency (1-5).")

    def forward(self, context: str, question: str, answer: str) -> Dict[str, Any]:
        try:
            logger.debug(f"Performing comprehensive assessment for question: '{question}'")
            
            # Assess factual correctness
            factuality = Assess()(context=context, question=question, answer=answer)['factually_correct']
            
            # Assess relevance
            relevance = AssessRelevance()(context=context, question=question, answer=answer)['relevance_score']
            
            # Assess coherence
            coherence = AssessCoherence()(context=context, question=question, answer=answer)['coherence_score']
            
            # Assess conciseness
            conciseness = AssessConciseness()(context=context, question=question, answer=answer)['conciseness_score']
            
            # Assess fluency
            fluency = AssessFluency()(context=context, question=question, answer=answer)['fluency_score']
            
            # Aggregate scores with adjusted weights
            composite_score = {
                "factually_correct": factuality,
                "relevance_score": relevance,
                "coherence_score": coherence,
                "conciseness_score": conciseness,
                "fluency_score": fluency
            }
            logger.info(f"Comprehensive assessment result: {composite_score}")
            return composite_score
        except Exception as e:
            logger.error(f"Error in ComprehensiveAssessment.forward: {e}", exc_info=True)
            return {
                "factually_correct": "No",
                "relevance_score": 1,
                "coherence_score": 1,
                "conciseness_score": 1,
                "fluency_score": 1
            }


def comprehensive_metric(example: Dict[str, Any], pred: Dict[str, Any], trace: Any = None) -> float:
    """
    Comprehensive metric to evaluate the answer across multiple dimensions.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
        trace (Any, optional): Trace information for optimization (unused here).
    
    Returns:
        float: A combined score representing the quality of the answer.
    """
    try:
        logger.debug(f"Evaluating comprehensive metrics for question: '{example.get('question', '')}'")
        assessment = comprehensive_assessment_module(
            context=example.get('context', ''),
            question=example.get('question', ''),
            answer=pred.get('answer', '')
        )
        logger.debug(f"Comprehensive assessment: {assessment}")
        
        # Convert 'factually_correct' to binary
        factually_correct = 1 if assessment.get("factually_correct") == "Yes" else 0
        
        # Normalize scores between 0 and 1
        relevance = assessment.get("relevance_score", 1) / 5
        coherence = assessment.get("coherence_score", 1) / 5
        conciseness = assessment.get("conciseness_score", 1) / 5
        fluency = assessment.get("fluency_score", 1) / 5
        
        # Define adjusted weights for each metric
        weights = {
            "factually_correct": 0.5,
            "relevance": 0.2,
            "coherence": 0.1,
            "conciseness": 0.1,
            "fluency": 0.1
        }
        
        # Calculate the composite score
        composite_score = (
            weights["factually_correct"] * factually_correct +
            weights["relevance"] * relevance +
            weights["coherence"] * coherence +
            weights["conciseness"] * conciseness +
            weights["fluency"] * fluency
        )
        
        logger.info(f"Comprehensive metric score: {composite_score}")
        return composite_score
    except Exception as e:
        logger.error(f"Error in comprehensive_metric: {e}", exc_info=True)
        return 0.0


def is_answer_fully_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    """
    Determines if the answer meets all quality metrics.
    
    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.
    
    Returns:
        bool: True if all metrics meet the desired thresholds, False otherwise.
    """
    scores = comprehensive_metric(example, pred)
    # Define threshold (e.g., composite score should be at least 0.8)
    is_factual = scores >= 0.8
    logger.debug(f"Is answer fully correct (scores >= 0.8): {is_factual}")
    return is_factual


def factuality_metric(example: Dict[str, Any], pred: Dict[str, Any]) -> int:
    """
    Metric to evaluate factual correctness of the answer.

    Args:
        example (Dict[str, Any]): The example containing 'context' and 'question'.
        pred (Dict[str, Any]): The prediction containing the generated 'answer'.

    Returns:
        int: 1 if factually correct, 0 otherwise.
    """
    try:
        assess = Assess()
        result = assess(context=example.get('context', ''), question=example.get('question', ''), answer=pred.get('answer', ''))
        factually_correct = result.get('factually_correct', 'No')
        logger.debug(f"Factuality metric result: {factually_correct}")
        return 1 if factually_correct == 'Yes' else 0
    except Exception as e:
        logger.error(f"Error in factuality_metric: {e}", exc_info=True)
        return 0 


# Initialize the assessment modules
try:
    # Use the unoptimized module directly without caching
    comprehensive_assessment_module = dspy.TypedChainOfThought(ComprehensiveAssessment)
    logger.info("Comprehensive Assessment DSPy module initialized successfully.")
except Exception as e:
    logger.error(f"Error initializing Comprehensive Assessment DSPy module: {e}", exc_info=True)
    raise


# File: analysis/select_quotation.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
import dspy
import json

logger = logging.getLogger(__name__)

class SelectQuotationSignature(dspy.Signature):
    """
    Select relevant quotations from transcript chunks based on research objectives.
    """
    research_objectives: str = dspy.InputField(
        desc="The research objectives guiding the selection of quotations."
    )
    transcript_chunks: List[str] = dspy.InputField(
        desc="Chunks of transcript from which quotations are to be selected."
    )
    quotations: List[Dict[str, Any]] = dspy.OutputField(
        desc="List of selected quotations with their types and functions."
    )
    purpose: str = dspy.OutputField(
        desc="Purpose of selecting the quotations."
    )

    def forward(self, research_objectives: str, transcript_chunks: List[str]) -> Dict[str, Any]:
        try:
            logger.debug("Starting quotation selection process.")
            prompt = (
                f"You are an expert in qualitative research and thematic analysis.\n\n"
                f"**Research Objectives**:\n{research_objectives}\n\n"
                f"**Transcript Chunks**:\n" +
                "\n".join([f"Chunk {i+1}: {chunk}" for i, chunk in enumerate(transcript_chunks)]) +
                "\n\n"
                f"**Task:** Extract **3-5** relevant quotations from the transcript chunks that align with the research objectives provided. "
                f"Provide each quotation in the following JSON format within a list:\n\n"
                f"```json\n"
                f"[\n"
                f"    {{\n"
                f"        \"quotation\": \"This is the first quotation.\",\n"
                f"        \"type\": \"Descriptive\",\n"
                f"        \"function\": \"Evidence\"\n"
                f"    }}\n"
                f"]\n"
                f"```\n"
                f"Ensure that the response is a valid JSON array containing the quotations and the overall purpose. "
                f"If no quotations are available, respond with an empty list and an empty string."
            )

            response = self.language_model.generate(
                prompt=prompt,
                max_tokens=500,
                temperature=0.7,
                top_p=0.9,
                n=1,
                stop=None
            ).strip()

            logger.info("Quotations selected successfully.")
            parsed_response = self._parse_json_response(response)
            quotations = self._standardize_quotations(parsed_response.get("quotations", []))
            purpose = parsed_response.get("purpose", "")

            # Validate quotations
            self._validate_quotations(quotations, research_objectives, transcript_chunks)

            return {
                "quotations": quotations,
                "purpose": purpose
            }
        except Exception as e:
            logger.error(f"Error in SelectQuotationSignature.forward: {e}", exc_info=True)
            return {
                "quotations": [],
                "purpose": ""
            }

    def _standardize_quotations(self, quotations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Standardize quotation format to ensure consistency."""
        standardized = []
        for q in quotations:
            standardized_quote = {}
            # Ensure "quotation" is the standard key
            if "quote" in q:
                standardized_quote["quotation"] = q.pop("quote")
            elif "quotation" in q:
                standardized_quote["quotation"] = q["quotation"]
            else:
                logger.warning("Quotation missing required text field")
                continue

            # Copy other fields
            standardized_quote["type"] = q.get("type", "")
            standardized_quote["function"] = q.get("function", "")
            
            standardized.append(standardized_quote)
        return standardized

    def _validate_quotations(self, quotations: List[Dict[str, Any]], research_objectives: str, context: List[str]) -> None:
        """Validate quotations meet quality standards."""
        if not quotations:
            return

        context_text = " ".join(context)
        
        for q in quotations:
            # Validate quotation exists in context
            if q["quotation"] not in context_text:
                raise ValueError(f"Quotation not found in original context: {q['quotation'][:50]}...")
            
            # Validate required fields
            required_fields = ["quotation", "type", "function"]
            missing_fields = [field for field in required_fields if not q.get(field)]
            if missing_fields:
                raise ValueError(f"Quotation missing required fields: {missing_fields}")
            
            # Validate quotation length
            if len(q["quotation"]) < 10:
                raise ValueError(f"Quotation too short: {q['quotation']}")

    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        try:
            # Attempt to parse the JSON array of quotations
            start_index = response.find('[')
            end_index = response.rfind(']') + 1
            if start_index == -1 or end_index == 0:
                logger.warning("JSON array of quotations not found in response.")
                return {"quotations": [], "purpose": ""}
            quotations_json = response[start_index:end_index]
            quotations = json.loads(quotations_json)
            return {
                "quotations": quotations,
                "purpose": ""
            }
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON response. Returning empty fields.")
            return {
                "quotations": [],
                "purpose": ""
            }

# File: analysis/select_quotation_module.py
#------------------------------------------------------------------------------
import logging
from typing import Dict, Any, List
import dspy

from .select_quotation import SelectQuotationSignature

logger = logging.getLogger(__name__)

class SelectQuotationModule(dspy.Module):
    """
    DSPy module to select quotations based on research objectives and transcript chunks.
    """
    def __init__(self):
        super().__init__()
        self.chain = dspy.TypedChainOfThought(SelectQuotationSignature)

    def forward(self, research_objectives: str, transcript_chunks: List[str]) -> Dict[str, Any]:
        try:
            logger.debug("Running SelectQuotationModule.")
            
            # Validate inputs
            if not research_objectives or not transcript_chunks:
                logger.warning("Missing required inputs for quotation selection.")
                return {
                    "quotations": [],
                    "purpose": ""
                }

            # Process through chain
            response = self.chain(
                research_objectives=research_objectives,
                transcript_chunks=transcript_chunks
            )

            # Extract and validate results
            quotations = response.get("quotations", [])
            purpose = response.get("purpose", "")

            # Log results
            logger.info(f"Selected {len(quotations)} quotations.")
            if quotations:
                logger.debug("First quotation sample: " + str(quotations[0]))

            return {
                "quotations": quotations,
                "purpose": purpose
            }

        except Exception as e:
            logger.error(f"Error in SelectQuotationModule.forward: {e}", exc_info=True)
            return {
                "quotations": [],
                "purpose": ""
            }


################################################################################
# Module: core
################################################################################


# File: core/__init__.py
#------------------------------------------------------------------------------
"""
Core functionality for the thematic analysis package.
Contains database and client implementations.
"""

from .contextual_vector_db import ContextualVectorDB
from .elasticsearch_bm25 import ElasticsearchBM25
from .openai_client import OpenAIClient

__all__ = ['ContextualVectorDB', 'ElasticsearchBM25', 'OpenAIClient']

# File: core/contextual_vector_db.py
#------------------------------------------------------------------------------
import os
import pickle
import numpy as np
import threading
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from dotenv import load_dotenv
import logging
import faiss
import dspy

from src.core.openai_client import OpenAIClient
from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)


class SituateContext(dspy.Module):
    def __init__(self):
        super().__init__()
        self.chain = dspy.ChainOfThought(SituateContextSignature)

    def forward(self, doc: str, chunk: str):
        prompt = f"""
                <document>
                {doc}
                </document>
            

                CHUNK_CONTEXT_PROMPT = 
                Here is the chunk we want to situate within the whole document
                <chunk>
                {chunk}
                </chunk>

                Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
                Answer only with the succinct context and nothing else.
    """
        
        return self.chain(doc=doc, chunk=chunk, prompt=prompt)


class SituateContextSignature(dspy.Signature):
    doc = dspy.InputField(desc="Full document content")
    chunk = dspy.InputField(desc="Specific chunk content")
    reasoning = dspy.OutputField(desc="Chain of thought reasoning")
    contextualized_content = dspy.OutputField(desc="Contextualized content for the chunk")
        

class ContextualVectorDB:
    def __init__(self, name: str, openai_api_key: str = None):
        if openai_api_key is None:
            openai_api_key = os.getenv("OPENAI_API_KEY")
        
        self.name = name
        self.embeddings = []
        self.metadata = []
        self.db_path = f"./data/{name}/contextual_vector_db.pkl"
        self.faiss_index_path = f"./data/{name}/faiss_index.bin"

        self.client = OpenAIClient(api_key=openai_api_key)
        logger.debug(f"Initialized OpenAIClient for ContextualVectorDB '{self.name}'")

    def situate_context(self, doc: str, chunk: str) -> Tuple[str, Any]:
        logger.debug(f"Entering situate_context with doc length={len(doc)} and chunk length={len(chunk)}.")
        try:
            if not hasattr(self, 'situate_context_module'):
                self.situate_context_module = SituateContext()
                logger.debug("Initialized SituateContext module")

            response = self.situate_context_module(doc=doc, chunk=chunk)
            contextualized_content = response.contextualized_content
            logger.debug("Generated contextualized_content using DSPy.")
            usage_metrics = {}
            return contextualized_content, usage_metrics
        except Exception as e:
            logger.error(f"Error during DSPy situate_context: {e}", exc_info=True)
            return "", None

    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 1):
        logger.debug("Entering load_data method.")
        if self.embeddings and self.metadata and os.path.exists(self.faiss_index_path):
            logger.info("Vector database is already loaded. Skipping data loading.")
            return
        if os.path.exists(self.db_path) and os.path.exists(self.faiss_index_path):
            logger.info("Loading vector database and FAISS index from disk.")
            self.load_db()
            self.load_faiss_index()
            return

        texts_to_embed, metadata = self._process_dataset(dataset, parallel_threads)
        
        self._embed_and_store(texts_to_embed, metadata)
        self.save_db()
        self._build_faiss_index()

        logger.info(f"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}")


    def _process_dataset(self, dataset: List[Dict[str, Any]], parallel_threads: int) -> Tuple[List[str], List[Dict[str, Any]]]:
        texts_to_embed = []
        metadata = []
        total_chunks = sum(len(doc.get('chunks', [])) for doc in dataset)
        logger.info(f"Total chunks to process: {total_chunks}")

        logger.info(f"Processing {total_chunks} chunks with {parallel_threads} threads.")
        try:
            with ThreadPoolExecutor(max_workers=parallel_threads) as executor:
                futures = []
                for doc in dataset:
                    for chunk in doc.get('chunks', []):
                        futures.append(executor.submit(self._generate_contextualized_content, doc, chunk))
                
                for future in tqdm(as_completed(futures), total=total_chunks, desc="Processing chunks"):
                    result = future.result()
                    if result:
                        texts_to_embed.append(result['text_to_embed'])
                        metadata.append(result['metadata'])
        except Exception as e:
            logger.error(f"Error during processing chunks: {e}", exc_info=True)
            return [], []

        return texts_to_embed, metadata

    def _generate_contextualized_content(self, doc: Dict[str, Any], chunk: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(doc, dict):
            logger.error(f"Document is not a dictionary: {doc}")
            return None

        if isinstance(chunk, dict):
            chunk_id = chunk.get('chunk_id')
            if not chunk_id:
                # Assign a unique chunk_id combining doc_id and chunk's index
                chunk_index = chunk.get('index', 0)
                chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_{chunk_index}"
                chunk['chunk_id'] = chunk_id
            content = chunk.get('content', '')
            original_index = chunk.get('original_index', chunk.get('index', 0))
        elif isinstance(chunk, str):
            # Handle case where chunk is a string
            content = chunk
            chunk_id = f"{doc.get('doc_id', 'unknown_doc_id')}_0"
            original_index = 0
            logger.warning(f"Chunk is a string. Expected a dict. Assigning default values.")
        else:
            logger.error(f"Unsupported chunk type: {type(chunk)}. Skipping chunk.")
            return None

        logger.debug(f"Processing chunk_id='{chunk_id}' in doc_id='{doc.get('doc_id', 'unknown_doc_id')}'")
        contextualized_text, usage = self.situate_context(doc.get('content', ''), content)
        return {
            'text_to_embed': f"{content}\n\n{contextualized_text}",
            'metadata': {
                'doc_id': doc.get('doc_id', ''),
                'original_uuid': doc.get('original_uuid', ''),
                'chunk_id': chunk_id,
                'original_index': original_index,
                'original_content': content,
                'contextualized_content': contextualized_text
            }
        }

    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):
        logger.debug("Entering _embed_and_store method.")
        batch_size = 128
        embeddings = []
        logger.info("Starting embedding generation.")
        try:
            with tqdm(total=len(texts), desc="Embedding chunks") as pbar:
                for i in range(0, len(texts), batch_size):
                    batch = texts[i : i + batch_size]
                    try:
                        response = self.client.create_embeddings(
                            model="text-embedding-3-small",
                            input=batch
                        )
                        embeddings_batch = [item['embedding'] for item in response['data']]
                        embeddings.extend(embeddings_batch)
                        pbar.update(len(batch))
                        logger.debug(f"Processed batch {i // batch_size + 1}: {len(batch)} embeddings.")
                    except Exception as e:
                        logger.error(f"Error during OpenAI embeddings for batch starting at index {i}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Unexpected error during embedding generation: {e}", exc_info=True)
        
        self.embeddings = embeddings
        self.metadata = data
        logger.info("Embedding generation completed.")

    def _build_faiss_index(self):
        self.create_faiss_index()
        self.save_faiss_index()

    def create_faiss_index(self):
        logger.debug("Entering create_faiss_index method.")
        try:
            embedding_dim = len(self.embeddings[0])
            logger.info(f"Embedding dimension: {embedding_dim}")
            embeddings_np = np.array(self.embeddings).astype('float32')
            faiss.normalize_L2(embeddings_np)
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.index.add(embeddings_np)
            logger.info(f"FAISS index created with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error creating FAISS index: {e}", exc_info=True)
            raise

    def save_faiss_index(self):
        logger.debug("Entering save_faiss_index method.")
        os.makedirs(os.path.dirname(self.faiss_index_path), exist_ok=True)
        try:
            faiss.write_index(self.index, self.faiss_index_path)
            logger.info(f"FAISS index saved to '{self.faiss_index_path}'")
        except Exception as e:
            logger.error(f"Error saving FAISS index to '{self.faiss_index_path}': {e}", exc_info=True)

    def load_faiss_index(self):
        logger.debug("Entering load_faiss_index method.")
        if not os.path.exists(self.faiss_index_path):
            logger.error(f"FAISS index file not found at '{self.faiss_index_path}'.")
            raise ValueError("FAISS index file not found.")
        try:
            self.index = faiss.read_index(self.faiss_index_path)
            logger.info(f"FAISS index loaded from '{self.faiss_index_path}' with {self.index.ntotal} vectors.")
        except Exception as e:
            logger.error(f"Error loading FAISS index from '{self.faiss_index_path}': {e}", exc_info=True)
            raise

    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        logger.debug(f"Entering search method with query='{query}' and k={k}.")
        if not self.embeddings or not self.metadata:
            logger.error("Embeddings or metadata are not loaded. Cannot perform search.")
            return []
        if not hasattr(self, 'index'):
            logger.error("FAISS index is not loaded.")
            return []
        
        try:
            response = self.client.create_embeddings(
                model="text-embedding-3-small",
                input=[query]
            )
            query_embedding = response['data'][0]['embedding']
            logger.debug(f"Generated embedding for query: '{query}'")
        except Exception as e:
            logger.error(f"Error generating embedding for query '{query}': {e}", exc_info=True)
            return []

        query_embedding_np = np.array([query_embedding]).astype('float32')
        faiss.normalize_L2(query_embedding_np)

        logger.debug("Performing FAISS search.")
        try:
            distances, indices = self.index.search(query_embedding_np, k)
            indices = indices.flatten()
            distances = distances.flatten()
        except Exception as e:
            logger.error(f"Error during FAISS search: {e}", exc_info=True)
            return []

        top_results = []
        for idx, score in zip(indices, distances):
            if idx < len(self.metadata):
                meta = self.metadata[idx]
                result = {
                    "doc_id": meta['doc_id'],
                    "chunk_id": meta['chunk_id'],
                    "original_index": meta.get('original_index', 0),
                    "content": meta['original_content'],
                    "contextualized_content": meta.get('contextualized_content'),
                    "score": float(score),
                    "metadata": meta
                }
                top_results.append(result)
            else:
                logger.warning(f"Index {idx} out of bounds for metadata.")
        logger.debug(f"FAISS search returned {len(top_results)} results for query: '{query}'")
        logger.info(f"Chunks retrieved for query '{query}': {[res['chunk_id'] for res in top_results]}")

        return top_results

    def save_db(self):
        logger.debug("Entering save_db method.")
        data = {
            "metadata": self.metadata,
        }
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        try:
            with open(self.db_path, "wb") as file:
                pickle.dump(data, file)
            logger.info(f"Vector database metadata saved to '{self.db_path}'")
        except Exception as e:
            logger.error(f"Error saving vector database metadata to '{self.db_path}': {e}", exc_info=True)

    def load_db(self):
        logger.debug("Entering load_db method.")
        if not os.path.exists(self.db_path):
            logger.error(f"Vector database file not found at '{self.db_path}'. Use load_data to create a new database.")
            raise ValueError("Vector database file not found.")
        try:
            with open(self.db_path, "rb") as file:
                data = pickle.load(file)
            self.metadata = data.get("metadata", [])
            logger.info(f"Vector database metadata loaded from '{self.db_path}' with {len(self.metadata)} entries.")
            logger.info(f"Chunks loaded: {[meta['chunk_id'] for meta in self.metadata]}")
        except Exception as e:
            logger.error(f"Error loading vector database metadata from '{self.db_path}': {e}", exc_info=True)
            raise


# File: core/elasticsearch_bm25.py
#------------------------------------------------------------------------------
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk, scan
from elasticsearch.exceptions import NotFoundError, RequestError
import json
import os

from src.utils.logger import setup_logging
setup_logging()
logger = logging.getLogger(__name__)

es_host = "http://localhost:9200"
index_name = "contextual_bm25_index"
es_client = Elasticsearch(es_host)

if es_client.indices.exists(index=index_name):
    es_client.indices.delete(index=index_name)
    print(f"Index '{index_name}' deleted.")

class ElasticsearchBM25:
    
    def __init__(
        self, 
        index_name: str = "contextual_bm25_index",
        es_host: str = "http://localhost:9200",
        logger: Optional[logging.Logger] = None
    ):
        self.logger = logger or logging.getLogger(__name__)
        self.index_name = index_name
        self.es_client = Elasticsearch(es_host)
        
        if not self.es_client.ping():
            raise ConnectionError(f"Failed to connect to Elasticsearch at {es_host}")
            
        self._create_index()
        
    def _create_index(self) -> None:
        index_settings = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "english"
                        },
                        "custom_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "porter_stem"]
                        }
                    }
                },
                "similarity": {
                    "custom_bm25": {
                        "type": "BM25",
                        "k1": 1.2,
                        "b": 0.75,
                    }
                },
                "index": {
                    "refresh_interval": "1s",
                    "number_of_shards": 1,
                    "number_of_replicas": 0
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "contextualized_content": {
                        "type": "text",
                        "analyzer": "custom_analyzer",
                        "similarity": "custom_bm25"
                    },
                    "doc_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "original_index": {"type": "integer"},
                    "metadata": {"type": "object", "enabled": True}
                }
            }
        }
        
        index_data_dir = f"./data/{self.index_name}"
        os.makedirs(index_data_dir, exist_ok=True)
        self.logger.debug(f"Ensured existence of index data directory: {index_data_dir}")
        
        try:
            if self.es_client.indices.exists(index=self.index_name):
                self.es_client.indices.close(index=self.index_name)
                self.es_client.indices.put_settings(
                    index=self.index_name,
                    body=index_settings["settings"]
                )
                self.es_client.indices.open(index=self.index_name)
            else:
                self.es_client.indices.create(
                    index=self.index_name,
                    body=index_settings
                )
            self.logger.info(f"Successfully configured index: {self.index_name}")
        except Exception as e:
            self.logger.error(f"Failed to create/update index: {str(e)}")
            raise

    def index_documents(
        self,
        documents: List[Dict[str, Any]],
        batch_size: int = 500
    ) -> Tuple[int, List[Dict[str, Any]]]:
        if not documents:
            self.logger.warning("No documents provided for indexing")
            return 0, []

        failed_docs = []
        success_count = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            actions = [{
                "_index": self.index_name,
                "_source": {
                    "content": doc.get("original_content", ""),
                    "contextualized_content": doc.get("contextualized_content", ""),
                    "doc_id": doc.get("doc_id", ""),
                    "chunk_id": doc.get("chunk_id", ""),
                    "original_index": doc.get("original_index", 0),
                    "metadata": doc.get("metadata", {})
                }
            } for doc in batch]
            
            try:
                success, failed = bulk(
                    self.es_client,
                    actions,
                    raise_on_error=False,
                    raise_on_exception=False
                )
                success_count += success
                if failed:
                    for fail in failed:
                        failed_doc = batch[fail.get('index', 0)]
                        failed_docs.append(failed_doc)
                    self.logger.warning(f"Failed to index {len(failed)} documents in batch")
                    
            except Exception as e:
                self.logger.error(f"Batch indexing error: {str(e)}")
                failed_docs.extend(batch)
                
        self.es_client.indices.refresh(index=self.index_name)
        self.logger.info(f"Indexed {success_count}/{len(documents)} documents successfully")
        return success_count, failed_docs

    def search(
        self,
        query: str,
        k: int = 20,
        min_score: float = 0.1,
        fields: List[str] = None,
        operator: str = "or",
        minimum_should_match: str = "30%"
    ) -> List[Dict[str, Any]]:
        if not fields:
            fields = ["content^1", "contextualized_content^1.5"]
            
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "operator": operator,
                    "minimum_should_match": minimum_should_match,
                    "type": "best_fields",
                    "tie_breaker": 0.3
                }
            },
            "min_score": min_score,
            "size": k,
            "_source": True
        }

        try:
            search_body_json = json.dumps(search_body, indent=2)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")
        except Exception as e:
            self.logger.error(f"Error converting search body to JSON: {e}")
            search_body_json = str(search_body)
            self.logger.debug(f"Elasticsearch search body: {search_body_json}")

        try:
            response = self.es_client.search(
                index=self.index_name,
                body=search_body
            )
            
            hits = [{
                "doc_id": hit["_source"]["doc_id"],
                "chunk_id": hit["_source"]["chunk_id"],
                "content": hit["_source"]["content"],
                "contextualized_content": hit["_source"].get("contextualized_content"),
                "score": hit["_score"],
                "metadata": hit["_source"].get("metadata", {})
            } for hit in response["hits"]["hits"]]
            
            self.logger.debug(
                f"Search for '{query}' returned {len(hits)} results "
                f"(max_score: {response['hits'].get('max_score', 0)})"
            )
            return hits
            
        except Exception as e:
            self.logger.error(f"Search error: {str(e)}")
            raise

    def search_content(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["content^1"],
            operator="or",
            minimum_should_match="30%"
        )

    def search_contextualized(self, query: str, k: int = 20) -> List[Dict[str, Any]]:
        self.logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        return self.search(
            query=query,
            k=k,
            fields=["contextualized_content^1.5"],
            operator="or",
            minimum_should_match="30%"
        )


# File: core/openai_client.py
#------------------------------------------------------------------------------
import logging
import os
from typing import List, Dict, Any
from openai import OpenAI
# Initialize logger
logger = logging.getLogger(__name__)

class OpenAIClient:
    """
    Wrapper for OpenAI API interactions using the updated SDK.
    """
    def __init__(self, api_key: str):
        """
        Initializes the OpenAI client with the provided API key.
        
        Args:
            api_key (str): OpenAI API key.
        """
        if not api_key:
            logger.error("OpenAI API key is not provided.")
            raise ValueError("OpenAI API key must be provided.")
        self.client = OpenAI(api_key=api_key)
        logger.debug("OpenAI client initialized successfully.")

    def create_chat_completion(self, model: str, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> Dict[str, Any]:
        """
        Creates a chat completion using OpenAI's API.
        
        Args:
            model (str): Model name to use.
            messages (List[Dict[str, str]]): List of message dictionaries.
            max_tokens (int): Maximum number of tokens in the response.
            temperature (float): Sampling temperature.
        
        Returns:
            Dict[str, Any]: API response as a dictionary.
        """
        if not model or not messages:
            logger.error("Model and messages must be provided for chat completion.")
            raise ValueError("Model and messages must be provided for chat completion.")
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            logger.debug(f"Chat completion created successfully for model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating chat completion: {e}")
            raise

    def create_embeddings(self, model: str, input: List[str]) -> Dict[str, Any]:
        """
        Creates embeddings for the given input texts using OpenAI's API.
        
        Args:
            model (str): Embedding model to use.
            input (List[str]): List of input texts.
        
        Returns:
            Dict[str, Any]: API response containing embeddings.
        """
        if not model or not input:
            logger.error("Model and input must be provided for creating embeddings.")
            raise ValueError("Model and input must be provided for creating embeddings.")
        
        try:
            response = self.client.embeddings.create(
                model=model,
                input=input
            )
            logger.debug(f"Embeddings created successfully using model '{model}'.")
            return response.model_dump()
        except Exception as e:
            logger.error(f"Error creating embeddings: {e}")
            raise



################################################################################
# Module: data
################################################################################


# File: data/__init__.py
#------------------------------------------------------------------------------
"""
Data handling modules for loading and processing data.
"""

from .data_loader import load_codebase_chunks, load_queries
from .copycode import CodeFileHandler

__all__ = ['load_codebase_chunks', 'load_queries', 'CodeFileHandler', 'main_async', 'main']

# File: data/copycode.py
#------------------------------------------------------------------------------
import logging
import os
import asyncio
import aiofiles
from typing import List, Set
import time
from pathlib import Path

# Initialize logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CodeFileHandler:
    def __init__(self, project_root: str, extensions: Set[str] = None):
        """
        Initialize the CodeFileHandler.
        
        Args:
            project_root (str): Root directory of the project
            extensions (Set[str]): Set of file extensions to include
        """
        self.project_root = project_root
        self.src_dir = os.path.join(project_root, '')
        self.extensions = extensions or {'.py', '.yaml', '.yml'}
        self.ignored_dirs = {'.venv', 'venv', 'env', '.env', 'myenv', '__pycache__', '.git', 'node_modules'}
        self.output_dir = os.path.join(project_root, 'output')

    async def get_code_files(self) -> List[str]:
        """
        Get all code files from the src directory.
        
        Returns:
            List[str]: List of file paths
        """
        logger.info(f"Scanning for code files in: {self.src_dir}")
        start_time = time.time()
        code_files = []
        
        try:
            for root, dirs, files in os.walk(self.src_dir):
                # Remove ignored directories
                dirs[:] = [d for d in dirs if d not in self.ignored_dirs]
                
                # Process files
                for file in files:
                    if file.endswith(tuple(self.extensions)):
                        file_path = os.path.join(root, file)
                        relative_path = os.path.relpath(file_path, self.src_dir)
                        code_files.append((file_path, relative_path))
                        logger.debug(f"Found code file: {relative_path}")
        
        except Exception as e:
            logger.error(f"Error scanning code files: {e}", exc_info=True)
            return []

        end_time = time.time()
        logger.info(f"Found {len(code_files)} code files in {end_time - start_time:.2f} seconds")
        return sorted(code_files, key=lambda x: x[1])  # Sort by relative path

    async def copy_code_to_file(self, code_files: List[tuple], output_file: str):
        """
        Copy code files to a single output file with proper organization.
        
        Args:
            code_files (List[tuple]): List of (file_path, relative_path) tuples
            output_file (str): Output file path
        """
        logger.info(f"Writing consolidated code to: {output_file}")
        start_time = time.time()
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        try:
            async with aiofiles.open(output_file, 'w', encoding='utf-8') as outfile:
                # Write header
                await outfile.write("# Consolidated Source Code\n")
                await outfile.write("# " + "=" * 78 + "\n\n")

                current_module = None
                for file_path, relative_path in code_files:
                    # Get module name (first directory in relative path)
                    module = relative_path.split(os.sep)[0] if os.sep in relative_path else 'root'
                    
                    # Write module header if changed
                    if module != current_module:
                        await outfile.write(f"\n\n{'#' * 80}\n")
                        await outfile.write(f"# Module: {module}\n")
                        await outfile.write(f"{'#' * 80}\n\n")
                        current_module = module

                    # Write file header
                    await outfile.write(f"\n# File: {relative_path}\n")
                    await outfile.write("#" + "-" * 78 + "\n")

                    try:
                        async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                            content = await infile.read()
                            await outfile.write(content)
                            await outfile.write("\n")
                    except Exception as e:
                        error_msg = f"# Error reading file {relative_path}: {str(e)}\n"
                        await outfile.write(error_msg)
                        logger.error(error_msg)

        except Exception as e:
            logger.error(f"Error writing to output file: {e}", exc_info=True)
            return

        end_time = time.time()
        logger.info(f"Code consolidation completed in {end_time - start_time:.2f} seconds")

async def main_async():
    """
    Main async function to handle code consolidation.
    """
    try:
        # Get project root (parent of src directory)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        logger.info(f"Project root: {project_root}")

        # Initialize handler
        handler = CodeFileHandler(
            project_root=project_root,
            extensions={'.py', '.yaml', '.yml'}
        )

        # Create output directory if it doesn't exist
        output_dir = os.path.join(project_root, 'output')
        os.makedirs(output_dir, exist_ok=True)

        # Get code files
        code_files = await handler.get_code_files()
        
        if not code_files:
            logger.error("No code files found to process")
            return

        # Generate output file path
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f'consolidated_code_{timestamp}.txt')

        # Copy code files
        await handler.copy_code_to_file(code_files, output_file)
        logger.info(f"Code consolidated successfully to: {output_file}")

    except Exception as e:
        logger.error(f"Error in main_async: {e}", exc_info=True)

def main():
    """
    Main entry point of the script.
    """
    try:
        asyncio.run(main_async())
        logger.info("Code consolidation completed successfully")
    except KeyboardInterrupt:
        logger.warning("Process interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)

if __name__ == "__main__":
    main()

# File: data/data_loader.py
#------------------------------------------------------------------------------
import json
import logging
from typing import List, Dict, Any

from src.utils.logger import setup_logging
logger = logging.getLogger(__name__)

class JSONLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Dict[str, Any]]:
        data = []
        try:
            if self.file_path.endswith('.jsonl'):
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    for line_number, line in enumerate(f, start=1):
                        if line.strip():
                            try:
                                obj = json.loads(line)
                                data.append(obj)
                            except json.JSONDecodeError as e:
                                logger.error(f"JSON parsing error in file '{self.file_path}' at line {line_number}: {e}")
                logger.info(f"Loaded JSONL file '{self.file_path}' with {len(data)} entries successfully.")
            else:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logger.info(f"Loaded JSON file '{self.file_path}' successfully with {len(data)} entries.")
            return data
        except FileNotFoundError:
            logger.error(f"Error loading JSON file '{self.file_path}': File does not exist.")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"Error loading JSON file '{self.file_path}': {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error loading JSON file '{self.file_path}': {e}")
            return []

def load_codebase_chunks(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading codebase chunks from '{file_path}'.")
    loader = JSONLoader(file_path)
    return loader.load()

def load_queries(file_path: str) -> List[Dict[str, Any]]:
    logger.debug(f"Loading queries from '{file_path}'.")
    loader = JSONLoader(file_path)
    return loader.load()



################################################################################
# Module: root
################################################################################


# File: decorators.py
#------------------------------------------------------------------------------
import logging
import functools

logger = logging.getLogger(__name__)

def handle_exceptions(func):
    """
    Decorator to handle exceptions in functions.
    
    Args:
        func (callable): The function to wrap with exception handling.
    
    Returns:
        callable: The wrapped function with exception handling.
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}", exc_info=True)
            return {"error": "An error occurred. Please try again later."}
    return wrapper



################################################################################
# Module: evaluation
################################################################################


# File: evaluation/__init__.py
#------------------------------------------------------------------------------
"""
Evaluation modules for assessing model performance.
"""

from .evaluation import PipelineEvaluator
from .evaluator import PipelineEvaluator as Evaluator

__all__ = ['PipelineEvaluator', 'Evaluator']

# File: evaluation/evaluation.py
#------------------------------------------------------------------------------
import logging
import time
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.analysis.metrics import comprehensive_metric, is_answer_fully_correct

setup_logging()
logger = logging.getLogger(__name__)

class PipelineEvaluator:
    def __init__(self, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, retrieval_function: Callable):
        self.db = db
        self.es_bm25 = es_bm25
        self.retrieval_function = retrieval_function

    def evaluate_pipeline(self, queries: List[Dict[str, Any]], k: int = 20) -> Dict[str, float]:
        total_score = 0
        total_queries = len(queries)
        queries_with_golden = 0
        queries_without_golden = 0

        semantic_success = 0
        bm25_contextual_success = 0
        total_semantic_hits = 0
        total_bm25_contextual_hits = 0

        logger.info(f"Starting evaluation of {total_queries} queries.")

        for query_item in tqdm(queries, desc="Evaluating retrieval"):
            query = query_item.get('query', '').strip()

            has_golden_data = all([
                'golden_doc_uuids' in query_item,
                'golden_chunk_uuids' in query_item,
                'golden_documents' in query_item
            ])

            if has_golden_data:
                queries_with_golden += 1
                golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
                golden_contents = []

                for doc_uuid, chunk_index in golden_chunk_uuids:
                    golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                    if not golden_doc:
                        logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                    if not golden_chunk:
                        logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_contents.append(golden_chunk.get('content', '').strip())

                if not golden_contents:
                    logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                    continue

                retrieved_docs = self.retrieval_function(query, self.db, self.es_bm25, k)

                chunks_found = 0
                semantic_hits = 0
                bm25_contextual_hits = 0

                for golden_content in golden_contents:
                    for doc in retrieved_docs[:k]:
                        retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                        contextualized_content = doc.get('chunk', {}).get('contextualized_content', '').strip()
                        if retrieved_content == golden_content:
                            chunks_found += 1
                            semantic_hits += 1
                            break
                        elif contextualized_content == golden_content:
                            chunks_found += 1
                            bm25_contextual_hits += 1
                            break

                query_score = chunks_found / len(golden_contents)
                total_score += query_score
                logger.debug(f"Query '{query}' score: {query_score}")

                semantic_success += semantic_hits
                bm25_contextual_success += bm25_contextual_hits
                total_semantic_hits += semantic_hits
                total_bm25_contextual_hits += bm25_contextual_hits
            else:
                queries_without_golden += 1
                logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
                continue

        average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
        pass_at_n = average_score * 100

        semantic_percentage = (semantic_success / total_semantic_hits) * 100 if total_semantic_hits > 0 else 0
        bm25_contextual_percentage = (bm25_contextual_success / total_bm25_contextual_hits) * 100 if total_bm25_contextual_hits > 0 else 0

        logger.info(f"Evaluation completed.")
        logger.info(f"Total Queries: {total_queries}")
        logger.info(f"Queries with Golden Data: {queries_with_golden}")
        logger.info(f"Queries without Golden Data: {queries_without_golden}")
        logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")
        logger.info(f"Semantic Hits: {semantic_percentage:.2f}%")
        logger.info(f"BM25 Contextual Hits: {bm25_contextual_percentage:.2f}%")

        return {
            "pass_at_n": pass_at_n,
            "average_score": average_score,
            "semantic_hit_percentage": semantic_percentage,
            "bm25_contextual_hit_percentage": bm25_contextual_percentage,
            "total_queries": total_queries,
            "queries_with_golden": queries_with_golden,
            "queries_without_golden": queries_without_golden
        }

    def evaluate_complete_pipeline(self, k_values: List[int], evaluation_set: List[Dict[str, Any]]):
        for k in k_values:
            logger.info(f"Starting evaluation for Pass@{k}")
            results = self.evaluate_pipeline(evaluation_set, k)
            logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
            logger.info(f"Average Score: {results['average_score']:.4f}")
            logger.info(f"Semantic Hit Percentage: {results['semantic_hit_percentage']:.2f}%")
            logger.info(f"BM25 Contextual Hit Percentage: {results['bm25_contextual_hit_percentage']:.2f}%")
            logger.info(f"Total Queries: {results['total_queries']}")
            logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
            logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")


# File: evaluation/evaluator.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25

logger = logging.getLogger(__name__)

class PipelineEvaluator:
    def __init__(self, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, retrieval_function: Callable):
        self.db = db
        self.es_bm25 = es_bm25
        self.retrieval_function = retrieval_function

    def evaluate_pipeline(self, queries: List[Dict[str, Any]], k: int = 20) -> Dict[str, float]:
        total_score = 0
        total_queries = len(queries)
        queries_with_golden = 0
        queries_without_golden = 0

        logger.info(f"Starting evaluation of {total_queries} queries.")

        for query_item in tqdm(queries, desc="Evaluating retrieval"):
            query = query_item.get('query', '').strip()

            has_golden_data = all([
                'golden_doc_uuids' in query_item,
                'golden_chunk_uuids' in query_item,
                'golden_documents' in query_item
            ])

            if has_golden_data:
                queries_with_golden += 1
                golden_chunk_uuids = query_item.get('golden_chunk_uuids', [])
                golden_contents = []

                for doc_uuid, chunk_index in golden_chunk_uuids:
                    golden_doc = next((doc for doc in query_item.get('golden_documents', []) if doc.get('uuid') == doc_uuid), None)
                    if not golden_doc:
                        logger.debug(f"No document found with UUID '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_chunk = next((chunk for chunk in golden_doc.get('chunks', []) if chunk.get('index') == chunk_index), None)
                    if not golden_chunk:
                        logger.debug(f"No chunk found with index '{chunk_index}' in document '{doc_uuid}' for query '{query}'.")
                        continue
                    golden_contents.append(golden_chunk.get('content', '').strip())

                if not golden_contents:
                    logger.warning(f"No golden contents found for query '{query}'. Skipping evaluation for this query.")
                    continue

                retrieved_docs = self.retrieval_function(query, self.db, self.es_bm25, k)

                chunks_found = 0
                for golden_content in golden_contents:
                    for doc in retrieved_docs[:k]:
                        retrieved_content = doc.get('chunk', {}).get('original_content', '').strip()
                        if retrieved_content == golden_content:
                            chunks_found += 1
                            break

                query_score = chunks_found / len(golden_contents)
                total_score += query_score
                logger.debug(f"Query '{query}' score: {query_score}")
            else:
                queries_without_golden += 1
                logger.debug(f"Query '{query}' does not contain golden data. Skipping evaluation metrics for this query.")
                continue

        average_score = (total_score / queries_with_golden) if queries_with_golden > 0 else 0
        pass_at_n = average_score * 100

        logger.info(f"Evaluation completed.")
        logger.info(f"Total Queries: {total_queries}")
        logger.info(f"Queries with Golden Data: {queries_with_golden}")
        logger.info(f"Queries without Golden Data: {queries_without_golden}")
        logger.info(f"Pass@{k}: {pass_at_n:.2f}%, Average Score: {average_score:.4f}")

        return {
            "pass_at_n": pass_at_n,
            "average_score": average_score,
            "total_queries": total_queries,
            "queries_with_golden": queries_with_golden,
            "queries_without_golden": queries_without_golden
        }

    def evaluate_complete_pipeline(self, k_values: List[int], evaluation_set: List[Dict[str, Any]]):
        for k in k_values:
            logger.info(f"Starting evaluation for Pass@{k}")
            results = self.evaluate_pipeline(evaluation_set, k)
            logger.info(f"Pass@{k}: {results['pass_at_n']:.2f}%")
            logger.info(f"Average Score: {results['average_score']:.4f}")
            logger.info(f"Total Queries: {results['total_queries']}")
            logger.info(f"Queries with Golden Data: {results.get('queries_with_golden', 0)}")
            logger.info(f"Queries without Golden Data: {results.get('queries_without_golden', 0)}\n")



################################################################################
# Module: root
################################################################################


# File: main.py
#------------------------------------------------------------------------------
# main.py

import gc
import logging
import os
from typing import List, Dict, Any
import asyncio
import dspy
from dspy.teleprompt import BootstrapFewShotWithRandomSearch
from dspy.datasets import DataLoader

from src.utils.logger import setup_logging
from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.data.data_loader import load_codebase_chunks, load_queries
from src.processing.query_processor import validate_queries, process_queries
from src.evaluation.evaluation import PipelineEvaluator
from src.analysis.metrics import comprehensive_metric
from src.processing.answer_generator import generate_answer_dspy, QuestionAnswerSignature
from src.retrieval.reranking import retrieve_with_reranking
from src.analysis.select_quotation_module import SelectQuotationModule
from src.analysis.extract_keywords_module import KeywordExtractionModule
from src.decorators import handle_exceptions

# Initialize logging
setup_logging()
logger = logging.getLogger(__name__)

class ThematicAnalysisPipeline:
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the Thematic Analysis Pipeline with given configuration.
        
        Args:
            config (Dict[str, Any]): Configuration dictionary containing file paths and settings
        """
        self.config = config
        self.contextual_db = None
        self.es_bm25 = None
        self.qa_module = None
        self.teleprompter = None
        self.optimized_program = None
        self.quotation_module = None
        self.keyword_module = None
        self.lm = None
        self.lm_history = []

    def log_lm_call(self, prompt):
        """Log each call to the LLM."""
        self.lm_history.append(prompt)
        logger.info(f"LLM Call: {prompt}")

    @handle_exceptions
    def initialize_components(self):
        """Initialize all pipeline components."""
        # Configure DSPy Language Model
        self.lm = dspy.LM('openai/gpt-4o-mini', max_tokens=8192)
        dspy.configure(lm=self.lm)

        # Initialize modules
        self.quotation_module = SelectQuotationModule()
        self.keyword_module = KeywordExtractionModule(
            input_file=self.config['quotation_file'],
            output_file=self.config['keywords_output_file']
        )

        # Initialize QA module
        try:
            self.qa_module = dspy.Program.load("optimized_program.json")
        except Exception:
            self.qa_module = dspy.TypedChainOfThought(QuestionAnswerSignature)

        # Example of logging an LLM call
        prompt = "Initialize components"
        self.log_lm_call(prompt)
        self.lm.inspect_history()

    @handle_exceptions
    def initialize_databases(self, codebase_chunks):
        """Initialize and populate databases."""
        # Initialize ContextualVectorDB
        self.contextual_db = ContextualVectorDB("contextual_db")
        self.contextual_db.load_data(codebase_chunks, parallel_threads=5)

        # Initialize ElasticsearchBM25
        self.es_bm25 = ElasticsearchBM25()
        success_count, _ = self.es_bm25.index_documents(self.contextual_db.metadata)
        logger.info(f"Indexed {success_count} documents in Elasticsearch")

        # Log and inspect LLM call
        prompt = "Initialize databases"
        self.log_lm_call(prompt)
        self.lm.inspect_history()

    @handle_exceptions
    def optimize_qa_module(self, train_dataset):
        """Optimize the QA module using DSPy's BootstrapFewShotWithRandomSearch."""
        optimizer_config = {
            'max_bootstrapped_demos': 4,
            'max_labeled_demos': 4,
            'num_candidate_programs': 10,
            'num_threads': 4
        }
        
        self.teleprompter = BootstrapFewShotWithRandomSearch(
            metric=comprehensive_metric,
            **optimizer_config
        )

        self.optimized_program = self.teleprompter.compile(
            student=self.qa_module,
            teacher=self.qa_module,
            trainset=train_dataset
        )
        
        self.optimized_program.save("optimized_program.json")

        # Log and inspect LLM call
        prompt = "Optimize QA module"
        self.log_lm_call(prompt)
        self.lm.inspect_history()

    @handle_exceptions
    async def process_data(self):
        """Process data through the pipeline including quotation selection and keyword extraction."""
        # Process queries
        await process_queries(
            self.validated_queries,
            self.contextual_db,
            self.es_bm25,
            k=20,
            output_file=self.config['output_filename']
        )

        # Extract keywords from quotations
        keywords_result = self.keyword_module.process_file(
            input_file=self.config['quotation_file'],
            research_objectives="Analyze and extract relevant themes and concepts"
        )
        
        if keywords_result.get("keywords"):
            logger.info(f"Keywords extracted and saved to {keywords_result.get('output_file')}")

        # Log and inspect LLM call
        prompt = "Process data"
        self.log_lm_call(prompt)
        self.lm.inspect_history()

    @handle_exceptions
    def evaluate_pipeline(self, evaluation_set):
        """Evaluate the pipeline performance."""
        evaluator = PipelineEvaluator(
            db=self.contextual_db,
            es_bm25=self.es_bm25,
            retrieval_function=retrieve_with_reranking
        )
        
        evaluator.evaluate_complete_pipeline(
            k_values=[5, 10, 20],
            evaluation_set=evaluation_set
        )

        # Log and inspect LLM call
        prompt = "Evaluate pipeline"
        self.log_lm_call(prompt)
        self.lm.inspect_history()

    @handle_exceptions
    async def run_pipeline(self):
        """Main pipeline execution method."""
        try:
            # Initialize components
            self.initialize_components()
            
            # Load and prepare data
            dl = DataLoader()
            train_dataset = dl.from_csv(
                "data/new_training_data.csv",
                fields=("input", "output"),
                input_keys=("input",)
            )
            
            # Load and validate input data
            codebase_chunks = load_codebase_chunks(self.config['codebase_chunks_file'])
            queries = load_queries(self.config['queries_file'])
            self.validated_queries = validate_queries(queries)
            evaluation_set = load_queries(self.config['evaluation_set_file'])

            # Initialize databases
            self.initialize_databases(codebase_chunks)

            # Optimize QA module
            self.optimize_qa_module(train_dataset)

            # Process data
            await self.process_data()

            # Evaluate pipeline
            self.evaluate_pipeline(evaluation_set)

        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}", exc_info=True)
            raise
        finally:
            # Cleanup
            gc.collect()

def main():
    """Entry point of the application."""
    config = {
        'codebase_chunks_file': 'data/codebase_chunks.json',
        'queries_file': 'data/queries.json',
        'evaluation_set_file': 'data/evaluation_set.jsonl',
        'output_filename': 'query_results.json',
        'quotation_file': 'query_results.json',
        'keywords_output_file': 'data/keywords.json'
    }

    pipeline = ThematicAnalysisPipeline(config)
    asyncio.run(pipeline.run_pipeline())

if __name__ == "__main__":
    main()


################################################################################
# Module: processing
################################################################################


# File: processing/__init__.py
#------------------------------------------------------------------------------
"""
Processing modules for handling queries and generating answers.
"""

from .answer_generator import generate_answer_dspy, QuestionAnswerSignature
from .query_processor import validate_queries, process_queries

__all__ = [
    'generate_answer_dspy',
    'QuestionAnswerSignature',
    'validate_queries',
    'process_queries'
]

# File: processing/answer_generator.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
import dspy
import asyncio

from src.analysis.metrics import comprehensive_metric, is_answer_fully_correct, factuality_metric
from src.utils.utils import check_answer_length

logger = logging.getLogger(__name__)

class QuestionAnswerSignature(dspy.Signature):
    input: str = dspy.InputField(
        desc=(
            "The combined input containing both the question and context. "
            "The format should be 'question: <question_text> context: <context_text>'."
        )
    )
    answer: str = dspy.OutputField(
        desc=(
            "The generated answer to the question. The answer should be concise, directly address "
            "the question, and be grounded in the provided context to ensure factual accuracy."
        )
    )

    def forward(self, input: str, max_tokens: int = 8192) -> Dict[str, str]:
        try:
            # Parse the input to extract question and context
            parts = input.split(' context: ', 1)
            question = parts[0].replace('question: ', '').strip()
            context = parts[1].strip() if len(parts) > 1 else ""

            logger.debug(f"Generating answer for question: '{question}' with context length: {len(context)} characters.")
            answer = self.language_model.generate(
                prompt=(
                    f"You are an expert in qualitative research and thematic analysis.\n\n"
                    f"**Guidelines**:\n"
                    f"- **Relevance:** Extract quotations that are closely related to the key themes.\n"
                    f"- **Diversity:** Ensure a range of perspectives and viewpoints.\n"
                    f"- **Clarity:** Choose clear and understandable quotations.\n"
                    f"- **Impact:** Select impactful quotations that highlight significant aspects of the data.\n"
                    f"- **Authenticity:** Maintain original expressions from participants.\n\n"
                    f"**Transcript Chunk**:\n{question}\n\n"
                    f"**Context:**\n{context}\n\n"
                    f"**Task:** Extract **3-5** relevant quotations from the transcript chunk based on the context provided. "
                    f"Provide each quotation in the following JSON format within a list:\n\n"
                    f"```json\n"
                    f"[\n"
                    f"    {{\"QUOTE\": \"This is the first quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the second quotation.\"}},\n"
                    f"    {{\"QUOTE\": \"This is the third quotation.\"}}\n"
                    f"]\n"
                    f"```"
                    f"Ensure that the response is a valid JSON array containing all relevant quotations. "
                    f"If no quotations are available, respond with an empty array `[]`."
                ),
                max_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                n=1,
                stop=None
            ).strip()
            logger.info(f"Generated answer for question: '{question}'")
            logger.debug(f"Answer length: {len(answer)} characters.")
            return {"answer": answer}
        except Exception as e:
            logger.error(f"Error in QuestionAnswerSignature.forward: {e}", exc_info=True)
            return {"answer": "I'm sorry, I couldn't generate an answer at this time."}

try:
    qa_module = dspy.Program.load("optimized_program.json")
    logger.info("Optimized DSPy program loaded successfully.")
except Exception as e:
    try:
        qa_module = dspy.TypedChainOfThought(QuestionAnswerSignature)
        logger.info("Unoptimized DSPy module initialized successfully.")
    except Exception as inner_e:
        logger.error(f"Error initializing unoptimized DSPy module: {inner_e}", exc_info=True)
        raise

async def generate_answer(input: str, max_tokens: int = 8192) -> str:
    try:
        logger.debug(f"Generating answer for input with length: {len(input)} characters.")
        answer = await asyncio.to_thread(qa_module, input=input, max_tokens=max_tokens)
        return answer.get("answer", "I'm sorry, I couldn't generate an answer at this time.")
    except Exception as e:
        logger.error(f"Error in generate_answer: {e}", exc_info=True)
        return "I'm sorry, I couldn't generate an answer at this time."

async def evaluate_answer(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    try:
        logger.debug(f"Evaluating answer for input: '{example.get('input', '')}'")
        return await asyncio.to_thread(is_answer_fully_correct, example, pred)
    except Exception as e:
        logger.error(f"Error in evaluate_answer: {e}", exc_info=True)
        return False

async def generate_answer_dspy(query: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    logger.debug(f"Entering generate_answer_dspy with query='{query}' and {len(retrieved_chunks)} retrieved_chunks.")
    try:
        context = ""
        for i, chunk in enumerate(retrieved_chunks, 1):
            chunk_content = chunk['chunk'].get('original_content', '')
            chunk_context = chunk['chunk'].get('contextualized_content', '')
            context += f"Chunk {i}:\n"
            context += f"Content: {chunk_content}\n"
            context += f"Context: {chunk_context}\n\n"

        if not context.strip():
            logger.warning(f"No valid context found for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't find relevant information to answer your question.",
                "used_chunks": [],
                "num_chunks_used": 0
            }

        logger.debug(f"Formatted context with {len(retrieved_chunks)} sequential chunks:\n{context[:200]}...")
        used_chunks_info = [
            {
                "chunk_id": chunk['chunk'].get('chunk_id', ''),
                "doc_id": chunk['chunk'].get('doc_id', ''),
                "content_snippet": chunk['chunk'].get('original_content', '')[:100] + "..."
            }
            for chunk in retrieved_chunks
        ]
        logger.info(f"Total number of chunks used for context in query '{query}': {len(used_chunks_info)}")
        logger.info(f"Chunks used for context: {used_chunks_info}")
        input_data = f"question: {query} context: {context}"
        answer = await generate_answer(input_data)

        if not answer:
            logger.warning(f"No answer generated for query '{query}'.")
            return {
                "answer": "I'm sorry, I couldn't generate an answer at this time.",
                "used_chunks": used_chunks_info,
                "num_chunks_used": len(used_chunks_info)
            }

        logger.debug(f"Generated answer for query '{query}': {answer}")
        logger.info(f"Number of chunks used for query '{query}': {len(used_chunks_info)}")
        example = {
            "context": context,
            "question": query
        }
        pred = {
            "answer": answer
        }
        suggestion = await evaluate_answer(example, pred)
        return {
            "answer": answer,
            "used_chunks": used_chunks_info,
            "num_chunks_used": len(used_chunks_info)
        }
    except Exception as e:
        logger.error(f"Error generating answer via DSPy for query '{query}': {e}", exc_info=True)
        return {
            "answer": "I'm sorry, I couldn't generate an answer at this time.",
            "used_chunks": [],
            "num_chunks_used": 0
        }

async def generate_answers_dspy(queries: List[str], retrieved_chunks_list: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    tasks = [
        generate_answer_dspy(query, retrieved_chunks)
        for query, retrieved_chunks in zip(queries, retrieved_chunks_list)
    ]
    return await asyncio.gather(*tasks)

def is_answer_factually_correct(example: Dict[str, Any], pred: Dict[str, Any]) -> bool:
    score = factuality_metric(example, pred)
    logger.debug(f"Factuality score: {score}")
    return score == 1


# File: processing/query_processor.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
import json
from tqdm import tqdm

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.retrieval.retrieval import multi_stage_retrieval
from src.analysis.select_quotation_module import SelectQuotationModule
from src.processing.answer_generator import generate_answer_dspy
from src.utils.logger import setup_logging

setup_logging()
logger = logging.getLogger(__name__)

def validate_queries(queries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Validates the structure of input queries.
    """
    valid_queries = []
    for idx, query in enumerate(queries):
        if 'query' not in query or not query['query'].strip():
            logger.warning(f"Query at index {idx} is missing the 'query' field or is empty. Skipping.")
            continue

        if 'research_objectives' not in query:
            logger.warning(f"Query at index {idx} missing research_objectives. Using default.")
            query['research_objectives'] = "Extract relevant insights and themes from the provided text."

        valid_queries.append(query)

    logger.info(f"Validated {len(valid_queries)} queries out of {len(queries)} provided.")
    return valid_queries

def retrieve_documents(query: str, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int) -> List[Dict[str, Any]]:
    """
    Retrieves documents using multi-stage retrieval with contextual BM25.
    """
    logger.debug(f"Retrieving documents for query: '{query}' with top {k} results.")
    final_results = multi_stage_retrieval(query, db, es_bm25, k)
    logger.debug(f"Retrieved {len(final_results)} results.")
    return final_results

async def process_single_query(
    query_item: Dict[str, Any],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    quotation_module: SelectQuotationModule
) -> Dict[str, Any]:
    """
    Processes a single query to retrieve documents, select quotations, and generate an answer.
    """
    try:
        query_text = query_item.get('query', '').strip()
        if not query_text:
            logger.warning("Empty query received. Skipping.")
            return {}

        logger.info(f"Processing query: {query_text}")

        # Retrieve relevant chunks
        retrieved_chunks = retrieve_documents(query_text, db, es_bm25, k)
        logger.info(f"Retrieved {len(retrieved_chunks)} chunks.")

        # Extract transcript chunks
        transcript_chunks = [chunk['chunk']['original_content'] for chunk in retrieved_chunks]

        # Get research objectives
        research_objectives = query_item.get('research_objectives', 
            'Extract relevant insights and themes from the provided text.')

        # Select quotations
        quotations_response = quotation_module.forward(
            research_objectives=research_objectives,
            transcript_chunks=transcript_chunks
        )
        
        # Generate answer
        qa_response = await generate_answer_dspy(query_text, retrieved_chunks)

        # Construct result
        result = {
            "query": query_text,
            "research_objectives": research_objectives,
            "retrieved_chunks": retrieved_chunks,
            "retrieved_chunks_count": len(retrieved_chunks),
            "used_chunk_ids": [chunk['chunk']['chunk_id'] for chunk in retrieved_chunks],
            "quotations": quotations_response.get("quotations", []),
            "purpose": quotations_response.get("purpose", ""),
            "answer": {
                "answer": qa_response.get("answer", "")
            }
        }

        logger.info(f"Processed query with {len(result['quotations'])} quotations selected.")
        return result

    except Exception as e:
        logger.error(f"Error processing query: {e}", exc_info=True)
        return {}

def save_results(results: List[Dict[str, Any]], output_file: str) -> None:
    """
    Saves the results of the processed queries to a specified output file.
    """
    try:
        with open(output_file, 'w', encoding='utf-8') as outfile:
            json.dump(results, outfile, indent=4, ensure_ascii=False)
        logger.info(f"Results saved to '{output_file}'")
    except Exception as e:
        logger.error(f"Error saving results: {e}", exc_info=True)
        raise

async def process_queries(
    queries: List[Dict[str, Any]],
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    output_file: str
) -> None:
    """
    Processes a list of queries to retrieve documents, select quotations, and generate answers.
    """
    logger.info("Starting query processing.")
    
    all_results = []
    quotation_module = SelectQuotationModule()

    try:
        for idx, query_item in enumerate(tqdm(queries, desc="Processing queries")):
            try:
                result = await process_single_query(
                    query_item=query_item,
                    db=db,
                    es_bm25=es_bm25,
                    k=k,
                    quotation_module=quotation_module
                )
                if result:
                    all_results.append(result)
            except Exception as e:
                logger.error(f"Error processing query {idx}: {e}", exc_info=True)
                continue

        save_results(all_results, output_file)
        logger.info("Query processing completed successfully.")

    except KeyboardInterrupt:
        logger.warning("Process interrupted. Saving partial results.")
        save_results(all_results, output_file)
        raise
    except Exception as e:
        logger.error(f"Fatal error in query processing: {e}", exc_info=True)
        raise


################################################################################
# Module: retrieval
################################################################################


# File: retrieval/__init__.py
#------------------------------------------------------------------------------
"""
Retrieval functionality for searching and ranking content.
"""

from .query_generator import QueryGeneratorSignature
from .retrieval import hybrid_retrieval, multi_stage_retrieval
from .reranking import retrieve_with_reranking
from .reranker import SentenceTransformerReRanker

__all__ = [
    'QueryGeneratorSignature',
    'hybrid_retrieval',
    'multi_stage_retrieval',
    'retrieve_with_reranking',
    'SentenceTransformerReRanker'
]

# File: retrieval/query_generator.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Example/src/query_generator.py
# query_generator.py
import dspy
import logging
from typing import Dict

from src.utils.logger import setup_logging

logger = logging.getLogger(__name__)

class QueryGeneratorSignature(dspy.Signature):
    question: str = dspy.InputField(desc="The original user question.")
    context: str = dspy.InputField(desc="The accumulated context from previous retrievals.")
    new_query: str = dspy.OutputField(desc="The generated query for the next retrieval hop.")

    def forward(self, question: str, context: str) -> Dict[str, str]:
        try:
            if not question or not context:
                raise ValueError("Both 'question' and 'context' must be provided and non-empty.")
            
            prompt = (
                f"Given the question: '{question}'\n"
                f"and the context retrieved so far:\n{context}\n"
                "Generate a search query that will help find additional information needed to answer the question."
            )
            new_query = self.language_model.generate(
                prompt=prompt,
                max_tokens=50,
                temperature=0.7,
                top_p=0.9,
                n=1,
                stop=["\n"]
            ).strip()
            logger.info(f"Generated new query: '{new_query}'")
            return {"new_query": new_query}
        except ValueError as ve:
            logger.error(f"ValueError in QueryGeneratorSignature.forward: {ve}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question
        except Exception as e:
            logger.error(f"Error in QueryGeneratorSignature.forward: {e}", exc_info=True)
            return {"new_query": question}  # Fallback to the original question


# File: retrieval/reranker.py
#------------------------------------------------------------------------------
import logging
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util
import torch

from src.utils.logger import setup_logging
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class SentenceTransformerReRanker:
    """
    Re-ranker using Sentence Transformers for semantic similarity.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        """
        Initializes the Sentence Transformer re-ranker.
        
        Args:
            model_name (str): Pre-trained Sentence Transformer model name.
            device (str, optional): Device to run the model on ('cuda', 'mps', or 'cpu'). Defaults to automatic selection.
        """
        self.model = SentenceTransformer(model_name)
        
        # Set device
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'.")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Re-ranks documents based on semantic similarity to the query.
        
        Args:
            query (str): The search query.
            documents (List[str]): List of document contents to re-rank.
            top_k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
        
        Returns:
            List[Dict[str, Any]]: List of re-ranked documents with similarity scores.
        """
        if not query or not documents:
            logger.warning("Query and documents must be provided for re-ranking.")
            return []

        try:
            logger.debug("Encoding query and documents.")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities.")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)  # Adjust k to the number of available documents
            if actual_k == 0:
                logger.warning("No documents available for re-ranking.")
                return []
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}.")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            logger.info(f"Re-ranked {actual_k} documents based on semantic similarity.")
            return re_ranked_docs
        except Exception as e:
            logger.error(f"Error during re-ranking with Sentence Transformers: {e}", exc_info=True)
            return []

def rerank_documents_sentence_transformer(query: str, retrieved_docs: List[Dict[str, Any]], k: int = 20) -> List[Dict[str, Any]]:
    """
    Re-ranks the retrieved documents using Sentence Transformers.
    
    Args:
        query (str): The search query.
        retrieved_docs (List[Dict[str, Any]]): List of retrieved documents.
        k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.info(f"Starting re-ranking of {len(retrieved_docs)} documents for query: '{query}' using Sentence Transformers.")
    if not query or not retrieved_docs:
        logger.warning(f"Query and retrieved documents must be provided for re-ranking.")
        return []
    try:
        # Initialize the re-ranker
        reranker = SentenceTransformerReRanker()
        
        # Extract document contents
        documents = [doc['chunk']['original_content'] for doc in retrieved_docs]
        
        # Perform re-ranking
        re_ranked = reranker.rerank(query, documents, top_k=k)
        
        # Attach scores back to the documents
        re_ranked_docs = []
        for r, original_doc in zip(re_ranked, retrieved_docs[:len(re_ranked)]):  # Ensure matching length
            re_ranked_docs.append({
                "chunk": original_doc['chunk'],
                "score": r['score']
            })
        
        logger.info(f"Re-ranking completed using Sentence Transformers. Top {k} documents selected.")
        return re_ranked_docs
    except Exception as e:
        logger.error(f"Error during document re-ranking with Sentence Transformers: {e}", exc_info=True)
        return retrieved_docs[:k]  # Fallback to original ranking if re-ranking fails


# File: retrieval/reranking.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Example/src/reranking.py
import logging
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util
import torch
import time

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.retrieval.retrieval import hybrid_retrieval
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)

class SentenceTransformerReRanker:
    """
    Re-ranker using Sentence Transformers for semantic similarity.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):
        """
        Initializes the Sentence Transformer re-ranker.
        
        Args:
            model_name (str): Pre-trained Sentence Transformer model name.
            device (str, optional): Device to run the model on ('cuda', 'mps', or 'cpu'). Defaults to automatic selection.
        """
        self.model = SentenceTransformer(model_name)
        
        # Set device
        if device:
            self.device = torch.device(device)
        else:
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        self.model.to(self.device)
        logger.info(f"SentenceTransformerReRanker initialized with model '{model_name}' on device '{self.device}'.")

    def rerank(self, query: str, documents: List[str], top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Re-ranks documents based on semantic similarity to the query.
        
        Args:
            query (str): The search query.
            documents (List[str]): List of document contents to re-rank.
            top_k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
        
        Returns:
            List[Dict[str, Any]]: List of re-ranked documents with similarity scores.
        """
        if not query or not documents:
            logger.warning("Query and documents must be provided for re-ranking.")
            return []

        try:
            logger.debug("Encoding query and documents.")
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            logger.debug("Computing cosine similarities.")
            cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]
            
            num_docs = len(documents)
            actual_k = min(top_k, num_docs)  # Adjust k to the number of available documents
            if actual_k == 0:
                logger.warning("No documents available for re-ranking.")
                return []
            
            logger.debug(f"Selecting top {actual_k} documents out of {num_docs}.")
            top_results = torch.topk(cosine_scores, k=actual_k)
            
            re_ranked_docs = []
            for score, idx in zip(top_results.values, top_results.indices):
                re_ranked_docs.append({
                    "document": documents[idx],
                    "score": score.item()
                })
            
            logger.info(f"Re-ranked {actual_k} documents based on semantic similarity.")
            return re_ranked_docs
        except Exception as e:
            logger.error(f"Error during re-ranking with Sentence Transformers: {e}", exc_info=True)
            return []

def rerank_results(query: str, retrieved_docs: List[Dict[str, Any]], k: int = 20) -> List[Dict[str, Any]]:
    """
    Re-ranks the retrieved documents using Sentence Transformers.
    
    Args:
        query (str): The search query.
        retrieved_docs (List[Dict[str, Any]]): List of retrieved documents.
        k (int, optional): Number of top documents to return after re-ranking. Defaults to 20.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.info(f"Starting re-ranking of {len(retrieved_docs)} documents for query: '{query}' using Sentence Transformers.")
    if not query or not retrieved_docs:
        logger.warning(f"Query and retrieved documents must be provided for re-ranking.")
        return []
    try:
        # Initialize the re-ranker
        reranker = SentenceTransformerReRanker()
        
        # Extract document contents
        documents = [doc['chunk']['original_content'] for doc in retrieved_docs]
        
        # Perform re-ranking
        re_ranked = reranker.rerank(query, documents, top_k=k)
        
        # Attach scores back to the documents
        re_ranked_docs = []
        for r, original_doc in zip(re_ranked, retrieved_docs[:len(re_ranked)]):  # Ensure matching length
            re_ranked_docs.append({
                "chunk": original_doc['chunk'],
                "score": r['score']
            })
        
        logger.info(f"Re-ranking completed using Sentence Transformers. Top {k} documents selected.")
        return re_ranked_docs
    except Exception as e:
        logger.error(f"Error during document re-ranking with Sentence Transformers: {e}", exc_info=True)
        return retrieved_docs[:k]  # Fallback to original ranking if re-ranking fails

def retrieve_with_reranking(query: str, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int) -> List[Dict[str, Any]]:
    """
    Retrieves documents using hybrid retrieval and re-ranks them using Sentence Transformers.
    
    Args:
        query (str): The search query.
        db (ContextualVectorDB): Contextual vector database instance.
        es_bm25 (ElasticsearchBM25): Elasticsearch BM25 instance.
        k (int): Number of top documents to retrieve.
    
    Returns:
        List[Dict[str, Any]]: List of re-ranked documents.
    """
    logger.debug(f"Entering retrieve_with_reranking method with query='{query}' and k={k}.")
    start_time = time.time()

    try:
        logger.debug(f"Performing hybrid retrieval for query: '{query}'")
        initial_results = hybrid_retrieval(query, db, es_bm25, k=k*10)
        logger.debug(f"Initial hybrid retrieval returned {len(initial_results)} results.")

        # **Add logging for all initial chunks retrieved**
        logger.info(f"Total chunks retrieved during hybrid retrieval for query '{query}': {len(initial_results)}")
        logger.info(f"Chunk IDs retrieved during hybrid retrieval: {[res['chunk']['chunk_id'] for res in initial_results]}")

        if not initial_results:
            logger.warning(f"No initial results retrieved for query '{query}'. Skipping reranking.")
            return []

        # Re-rank the retrieved documents
        final_results = rerank_results(query, initial_results, top_k=5)  # Set top_k to 5

    except Exception as e:
        logger.error(f"Error during retrieval or re-ranking for query '{query}': {e}", exc_info=True)
        return []

    end_time = time.time()
    logger.debug(f"Exiting retrieve_with_reranking method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results


# File: retrieval/retrieval.py
#------------------------------------------------------------------------------
# File: retrieval.py
import logging
import time
from typing import List, Dict, Any
import dspy

from src.core.contextual_vector_db import ContextualVectorDB
from src.core.elasticsearch_bm25 import ElasticsearchBM25
from src.utils.logger import setup_logging
from src.retrieval.query_generator import QueryGeneratorSignature
from src.utils.utils import compute_similarity
# Initialize logger
setup_logging()
logger = logging.getLogger(__name__)


def hybrid_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    semantic_weight: float = 0.2,
    bm25_content_weight: float = 0.2,
    bm25_contextual_weight: float = 0.6,
    min_chunks: int = 1
) -> List[Dict[str, Any]]:
    """
    Performs hybrid retrieval by combining FAISS semantic search and dual BM25 contextual search using Reciprocal Rank Fusion.
    """
    logger.debug(
        f"Entering hybrid_retrieval with query='{query}', k={k}, "
        f"semantic_weight={semantic_weight}, bm25_content_weight={bm25_content_weight}, "
        f"bm25_contextual_weight={bm25_contextual_weight}, min_chunks={min_chunks}."
    )
    start_time = time.time()
    num_chunks_to_recall = k * 10  # Retrieve more to improve chances

    # Initialize reciprocal rank fusion score dictionary
    chunk_id_to_score = {}

    while True:
        # Semantic search
        logger.debug(f"Performing semantic search using FAISS for query: '{query}'")
        semantic_results = db.search(query, k=num_chunks_to_recall)
        ranked_semantic = [result['chunk_id'] for result in semantic_results]
        semantic_scores = {result['chunk_id']: result['score'] for result in semantic_results}
        logger.debug(f"Semantic search retrieved {len(ranked_semantic)} chunk IDs.")

        # BM25 search on 'content'
        logger.debug(f"Performing BM25 search on 'content' for query: '{query}'")
        bm25_content_results = es_bm25.search_content(query, k=num_chunks_to_recall)
        ranked_bm25_content = [result['chunk_id'] for result in bm25_content_results]
        bm25_content_scores = {result['chunk_id']: result['score'] for result in bm25_content_results}
        logger.debug(f"BM25 'content' search retrieved {len(ranked_bm25_content)} chunk IDs.")

        # BM25 search on 'contextualized_content'
        logger.debug(f"Performing BM25 search on 'contextualized_content' for query: '{query}'")
        bm25_contextual_results = es_bm25.search_contextualized(query, k=num_chunks_to_recall)
        ranked_bm25_contextual = [result['chunk_id'] for result in bm25_contextual_results]
        bm25_contextual_scores = {result['chunk_id']: result['score'] for result in bm25_contextual_results}
        logger.debug(f"BM25 'contextualized_content' search retrieved {len(ranked_bm25_contextual)} chunk IDs.")

        # Combine all unique chunk IDs
        chunk_ids = list(set(ranked_semantic + ranked_bm25_content + ranked_bm25_contextual))
        logger.debug(f"Total unique chunk IDs after combining: {len(chunk_ids)}")

        # Calculate Reciprocal Rank Fusion scores
        for chunk_id in chunk_ids:
            score = 0
            if chunk_id in ranked_semantic:
                index = ranked_semantic.index(chunk_id)
                score += semantic_weight * (1 / (index + 1))
                logger.debug(
                    f"Added semantic RRF score for chunk_id {chunk_id}: "
                    f"{semantic_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_content:
                index = ranked_bm25_content.index(chunk_id)
                score += bm25_content_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_content_weight * (1 / (index + 1))}"
                )
            if chunk_id in ranked_bm25_contextual:
                index = ranked_bm25_contextual.index(chunk_id)
                score += bm25_contextual_weight * (1 / (index + 1))
                logger.debug(
                    f"Added BM25 'contextualized_content' RRF score for chunk_id {chunk_id}: "
                    f"{bm25_contextual_weight * (1 / (index + 1))}"
                )
            chunk_id_to_score[chunk_id] = score

        # Sort chunk IDs by their RRF scores in descending order
        sorted_chunk_ids = sorted(
            chunk_id_to_score.keys(),
            key=lambda x: chunk_id_to_score[x],
            reverse=True
        )
        logger.debug(f"Sorted chunk IDs based on RRF scores.")

        # Select top k chunks, ensuring at least min_chunks are returned
        final_results = []
        filtered_count = 0
        for chunk_id in sorted_chunk_ids[:k]:
            chunk_metadata = next(
                (chunk for chunk in db.metadata if chunk['chunk_id'] == chunk_id),
                None
            )
            if not chunk_metadata:
                filtered_count += 1
                logger.warning(f"Chunk metadata not found for chunk_id {chunk_id}")
                continue
            final_results.append({
                'chunk': chunk_metadata,
                'score': chunk_id_to_score[chunk_id]
            })

        logger.info(f"Filtered {filtered_count} chunks due to missing metadata.")
        logger.info(
            f"Total chunks retrieved after filtering: {len(final_results)} "
            f"(required min_chunks={min_chunks})"
        )

        if len(final_results) >= min_chunks or k >= num_chunks_to_recall:
            break
        else:
            k += 5  # Increment k to retrieve more chunks
            logger.info(
                f"Number of retrieved chunks ({len(final_results)}) is less than min_chunks ({min_chunks}). "
                f"Increasing k to {k} and retrying retrieval."
            )

    logger.debug(f"Hybrid retrieval returning {len(final_results)} chunks.")
    logger.info(
        f"Chunks used for hybrid retrieval for query '{query}': "
        f"[{', '.join([res['chunk']['chunk_id'] for res in final_results])}]"
    )
    end_time = time.time()
    logger.debug(f"Exiting hybrid_retrieval method. Time taken: {end_time - start_time:.2f} seconds.")
    return final_results


def multi_stage_retrieval(
    query: str,
    db: ContextualVectorDB,
    es_bm25: ElasticsearchBM25,
    k: int,
    max_hops: int = 3,
    max_results: int = 5,
    similarity_threshold: float = 0.9
) -> List[Dict[str, Any]]:
    accumulated_context = ""
    all_retrieved_chunks = {}
    current_query = query
    previous_query = ""
    query_generator = dspy.TypedChainOfThought(QueryGeneratorSignature)

    for hop in range(max_hops):
        logger.info(f"Starting hop {hop+1} with query: '{current_query}'")

        retrieved_chunks = hybrid_retrieval(current_query, db, es_bm25, k)

        # Update accumulated retrieved chunks
        for chunk in retrieved_chunks:
            chunk_id = chunk['chunk']['chunk_id']
            if chunk_id not in all_retrieved_chunks:
                all_retrieved_chunks[chunk_id] = chunk

        # Check if we have reached the desired number of results
        if len(all_retrieved_chunks) >= max_results:
            logger.info(f"Retrieved sufficient chunks ({len(all_retrieved_chunks)}). Terminating.")
            break

        # Accumulate new context from retrieved chunks
        new_context = "\n\n".join([
            chunk['chunk'].get('contextualized_content', '') or chunk['chunk'].get('original_content', '')
            for chunk in retrieved_chunks
        ])
        accumulated_context += "\n\n" + new_context

        # Generate a new query based on the accumulated context
        response = query_generator(question=query, context=accumulated_context)
        new_query = response.get('new_query', '').strip()
        if not new_query:
            logger.info("No new query generated. Terminating multi-stage retrieval.")
            break

        # Compute similarity between the new query and the previous query
        similarity = compute_similarity(current_query, new_query)
        logger.debug(f"Similarity between queries: {similarity:.4f}")
        if similarity >= similarity_threshold:
            logger.info("New query is too similar to the current query. Terminating multi-stage retrieval.")
            break

        # Update queries for the next iteration
        previous_query = current_query
        current_query = new_query

    # Sort and return the top results
    final_results = sorted(
        all_retrieved_chunks.values(),
        key=lambda x: x['score'],
        reverse=True
    )[:max_results]

    logger.info(f"Multi-stage retrieval completed with {len(final_results)} chunks.")
    return final_results



################################################################################
# Module: utils
################################################################################


# File: utils/__init__.py
#------------------------------------------------------------------------------
"""
Utility functions and helpers.
"""

from src.decorators import handle_exceptions
from .logger import setup_logging
from .utils import check_answer_length, compute_similarity
from .validation_functions import validate_relevance, validate_quality, validate_context_clarity

__all__ = [
    'handle_exceptions',
    'setup_logging',
    'check_answer_length',
    'compute_similarity',
    'validate_relevance',
    'validate_quality',
    'validate_context_clarity',
    ''
]



# File: utils/logger.py
#------------------------------------------------------------------------------
import logging
import os
from datetime import datetime

def setup_logging():
    """Configure logging with timestamp and proper formatting."""
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"thematic_analysis_{timestamp}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )

# File: utils/utils.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/utils.py

import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

def check_answer_length(answer: str, max_length: int = 500) -> bool:
    """
    Checks if the answer length is within the specified limit.

    Args:
        answer (str): The generated answer to evaluate.
        max_length (int, optional): The maximum allowed length for the answer. Defaults to 500.

    Returns:
        bool: True if the answer length is within the limit, False otherwise.
    """
    return len(answer) <= max_length

def compute_similarity(query1: str, query2: str) -> float:
    """
    Computes cosine similarity between two queries using TF-IDF vectorization.

    Args:
        query1 (str): The first query string.
        query2 (str): The second query string.

    Returns:
        float: Cosine similarity score between query1 and query2.
    """
    try:
        vectorizer = TfidfVectorizer().fit_transform([query1, query2])
        vectors = vectorizer.toarray()
        similarity = cosine_similarity([vectors[0]], [vectors[1]])[0][0]
        return similarity
    except Exception as e:
        logger.error(f"Error computing similarity: {e}", exc_info=True)
        return 0.0


# File: utils/validation_functions.py
#------------------------------------------------------------------------------
# File: /Users/amankumarshrestha/Downloads/Thematic-AnalysisE/src/utils/validation_functions.py

import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

def validate_relevance(quotations: List[str], research_objectives: str) -> bool:
    """
    Validates that each quotation is relevant to the research objectives.
    """
    try:
        for quote in quotations:
            # Simple keyword matching; can be enhanced with NLP techniques
            if not any(obj.lower() in quote.lower() for obj in research_objectives.split()):
                logger.debug(f"Quotation '{quote}' is not relevant to the research objectives.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_relevance: {e}", exc_info=True)
        return False

def validate_quality(quotations: List[Dict[str, Any]]) -> bool:
    """
    Validates the quality and representation of each quotation.
    """
    try:
        for quote in quotations:
            if len(quote["QUOTE"].strip()) < 10:  # Example quality check
                logger.debug(f"Quotation '{quote['QUOTE']}' is too short to be considered high quality.")
                return False
            # Additional quality checks can be added here
        return True
    except Exception as e:
        logger.error(f"Error in validate_quality: {e}", exc_info=True)
        return False

def validate_context_clarity(quotations: List[str], context: str) -> bool:
    """
    Validates that each quotation is clear and has sufficient context.
    """
    try:
        for quote in quotations:
            if quote.lower() not in context.lower():
                logger.debug(f"Quotation '{quote}' lacks sufficient context.")
                return False
        return True
    except Exception as e:
        logger.error(f"Error in validate_context_clarity: {e}", exc_info=True)
        return False

